{
  
    
        "post0": {
            "title": "Resources list",
            "content": "Below is a non-exhaustive list of resources including blogs, courses, books, podcasts, and video lectures which I have found extremely useful in learning python, statstics, and machine-learning concepts. . Highly recommended machine-learning starting point: . I am self taught machine learning practitioner. I strongly believe in the 80-20 rule, 80% of the output/results from 20% inputs. In that spirit, following are top five sources to get upto speed on applying ML and learning its basics. For practical purposes it is not always necessary dive super deep, rather its helpful to get a tldr version of the concept, assumptions, and start applying them right away. . Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Github | . I started to read on ML and data analysis using this wonderful book by Aurélien Géron. This is one of the best, if not the best, introductory books for machine learning. It is concise and simple to read and has jupyter notebooks to apply the concepts taught in it. Initial chapters (Part 1) of the book offer a strong foundation for traditional ML algorithms. . Data science from scratch | . Besides just focusing on ML, having experience with data wrangling using PyData stack (NumPy, Pandas, and friends) is always a plus. In fact, most of the time the limitation in setting up any ML model is massaging data into machine readable format. . Practical Deep Learning for coders from Fast.AI (using PyTorch) | . Deep Learning is the most popular sub-branch of ML and something you should have a general understanding of. Jeremy Howard and team have setup this wonderful didactic coursework using PyTorch (personal preference) comprising of useful collection of walkthroughs and practical examples. . Introduction to Statistical Learning + Elements of Statistical Learning | . Fantastic high-level math focussed introduction to algorithms. . 100 page ML | . Approachable compendium of key ML concepts boiled down to key insights, offers a nice way to articulate concepts in a concise way. . Nice (free) online courses: . Machine Learning . MIT’s Intro to Deep Learning | Google’s ML crash course | Stanford’s CS - CNN course | NYU’s PyTorch Deep learning | . Data Science and Computation . MIT’s course on Computational Thinking | Harvard’s CS 109 | . Miscellaneous . CMU’s course on Computer Graphics | . Python in general . Learning Python . Automate Boring Stuff with Python | Scientific programing with Python | Visual Guide to Numpy | Python DataScience Handbook | Chris Albon’s notes | Numpy Visual Introduction | . Tutorials / Projects . Pynative | Python Workout | Reuven Lerner’s Python Interview Prep | Project Euler | Calm code tips on python code | . Writing better code . Corey Schafer’s Tips for writing better code | Refactory blog | RealPython blog | . Datasets . FiveThreeEight | Pudding data | Our world in data | India centric dataset | Misc collection | . Books . Statstics &amp; Exploratory Analysis . Think Stats by Allen Downley | Telling stories with data | . Data Science . Jakevdp Python Datascience Notebook | . Data Visualization . Fundamentals of Data Visualization | . Machine-Learning . ML Online notebook | ML interview book by Chip Huyen | Andrew White’s Deep Learning for Molecules and Materials Notebook | Online deep learning book by Ian Goodfellow | . Machine-learning focused key commentaries, perspectives, and reviews . Area reviews . A Survey of Deep Learning for Scientific Discovery | The Discipline of Machine Learning | . General tips . Scikit-learn documentation on common pitfalls | Machine Learning that Matters | Three pitfalls to avoid in machine learning | A Few Useful Things to Know about Machine Learning | . Commentaries . Statistical Modeling: The Two Cultures | The Hardware Lottery | Machine Learning that Matters | Why is AI harder than we think | . In Chemical Sciences: . A. Y. T. Wang et al., “Machine Learning for Materials Scientists: An Introductory Guide toward Best Practices,” Chem. Mater., vol. 32, no. 12, pp. 4954–4965, 2020 | . Molecular science: . F. Strieth-Kalthoff, F. Sandfort, M. H. S. Segler, and F. Glorius, Machine learning the ropes: principles, applications and directions in synthetic chemistry, Chem. Soc. Rev | . Graph networks . Graph networks: Relational inductive biases, deep learning, and graph networks | Medium blog on graph neural networks | GraphML substack | . Cheat Sheets . I’ve compiled some nice cheat-sheets discussing basics of ML, Data Science, Statistics concepts alongside some tips on NumPy, Pandas, and Scikit-learn. These compilations are particularly useful when brushing up details before a potential job interview. Link to dataset repository . Video series . Explanations . Neural networks series by 3Blue1Brown | Machine learning zero to hero | Ali Godsi’s video lecture series (highly recommend his lecture on Variational Auto Encoders) | Khan Academy’s Multivariate Calculus | Khan Academy Statsitics + Probability | . PyCon talks . Statistics for Hackers, Jake Vanderplas | Statistical Thinking for Data Science, Chris Fonnesbeck | . AI talks . Dangers of AI is weirder than you think | . Blogs . Data Science focused . Nate Silver’s 538 | Pudding’s data viz | Spurious Correlations | Understanding uncertainty | Math3ma Blog | Max Wolfe | Chris Albon | Caitlin Hudson | . Statistics Blogs . Statistics by Jim | Probably overthinking it by Allen Downley | . ML inclined . Chris Olah | Andrej Karpathy, wonderfully didactic posts | Jay Alammer’s NLP focussed | . ML code examples and tutorials . Keras Code Examples | Tensorflow Examples | PyTorch Examples | . General compilations . Distill Blog | KDNuggets | . Data-inspired Podcasts . Fanstastic resource, you can be a fly on the wall and listen to experts talk about a topic that interests you . AI in Business | McKinsey AI | AZ16 podcast | Data Skeptic | Lex Friedman / AI podcast | . YouTubers . List of YouTuber channels that never fail to inspire me . 1. Science and Technology . Vsauce | Sixty Symbols | Tom Scott | 3Blue1Brown | Vertisasium | Applied Science | MKBHD | . Statistics . StatQuest | Corey Schafer | . 2. Food . J Kenji Lopez-Alt | Binging with Babish | Adam Ragusea | Food Wishes | . 3.Videography and Design . Casey Neistat | Dan Mace | Peter McKinnon | Andrew Price - Blender | CGMatter | CG Figures - Scientific visualization in Blender | . 4. Journalism . Dhruv Rahtee | Johnny Harris | Faye D’Souza | . Diversity &amp; Inclusion . [Delotte’s Woman in AI]https://www2.deloitte.com/content/dam/Deloitte/us/Documents/deloitte-analytics/us-consulting-women-in-ai.pdf | Account from Dow employees going from R&amp;D to D&amp;I | Diversity in STEM: What it is and Why it Matters | How Diversity Makes Us Smarter | Increasing Gender Diversity in the STEM Research Workforce | Without Inclusion, Diversity Initiatives May Not Be Enough | Work organization and mental health problems in PhD students | . Misc . Paul Graham | Sam Altman | Beauty of Science | Farnam Street | Less Wrong | Maria Popova’s Brain Pickings | Wait But Why | Brad Feld’s Personal Blog | Melthing Asphalt | Robert Heaton | Astral Codex Ten | . Comics . Xkcd | Oatmeal | Catana Comics | .",
            "url": "http://pgg1610.github.io/blog_fastpages/exploratory-data-analysis/machine-learning/resources/2021/06/25/ML_resources.html",
            "relUrl": "/exploratory-data-analysis/machine-learning/resources/2021/06/25/ML_resources.html",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Pandas cookbook",
            "content": "Selected Pandas code snippets and recipes that I revisit now and again. The snippets are adopted from different python scripts written over time, ignore the variable names. . This post was inspired by the wonderful work of Chris Albon and his snippets of code blocks. . Last update: 13th June 2021 . Reading and Writing . Basic reading a blank csv file . # If you have no column names but know the number of columns pd.read_csv(file_name, header=None, names=[&#39;col1&#39;,&#39;col2&#39;]) . Saving a file to not have ‘Unamed’ column . df1.to_csv(os.path.join(output_dir, &#39;file_name_to_save_as.csv&#39;), sep=&#39;,&#39;,columns=df1.columns, index=False, header=False) # header = None for no column names . Information about the dataframe . pandas_dataframe.info() . Summary statistics (mean, quartile ranges) . pandas_dataframe.describe().round(2) . Replace . df = df.replace( [list_of_value_to_replace], value_to_replace_with) # Eg: df.replace( [98-99], np.nan) . Replace characters in the columns . # List of characters to remove chars_to_remove = [&#39;+&#39;,&#39;$&#39;,&#39;,&#39;] # List of column names to clean cols_to_clean = [&#39;Installs&#39;,&#39;Price&#39;] # Loop for each column in cols_to_clean for col in cols_to_clean: # Loop for each char in chars_to_remove for char in chars_to_remove: # Replace the character with an empty string apps[col] = apps[col].apply(lambda x: x.replace(char, &#39;&#39;)) # Convert col to float data type apps[col] = apps[col].astype(float) . Convert spaces titles in the row to one word separated by ‘-‘ . reduced_df[&#39;product_title&#39;] = reduced_df[&#39;product_title&#39;].apply( lambda x: x.lower().replace(&#39; &#39;, &#39;-&#39;) ) . Define a new column with temp entries . pandas_dataframe[&#39;columns_name&#39;] = 42 . Create columns in a loop . pandas_dataframe.columns = [&#39;feature_&#39; + str(i) for i in range(n_columns)] . Dropping miscellaneous columns and NaN entries . columns_to_drop = [&#39;CookTimeInMins&#39;, &#39;Servings&#39;, &#39;Course&#39;, &#39;Diet&#39;, &#39;Instructions&#39;, &#39;TranslatedInstructions&#39;, &#39;URL&#39;] food_df = food_df.drop(columns = columns_to_drop).dropna() . Quick plotting . Simple pearson correlation plot . # Generate Pearson Correlation Matrix for HOUSING corr_matrix=housing.corr() # Edit the visuals and precision corr_matrix.style.background_gradient(cmap=&#39;coolwarm&#39;).set_precision(2) # Look at Pearson values for one attribute corr_matrix[&#39;median_house_value&#39;].sort_values(ascending=True) . Plot multiple scatter plots . from pandas.plotting import scatter_matrix attributes = [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;] scatter_axes = scatter_matrix(housing[attributes], figsize=(12, 8)); . Handling missing values . Option A: Dropping values in the columns with NaN . housing.dropna(subset=[&quot;total_bedrooms&quot;]) . Option B: Drop that column entirely . housing.drop(&quot;total_bedrooms&quot;, axis=1) . Option C: Fill missing value with some central tendency . attribute_median = housing[&quot;total_bedrooms&quot;].median() housing[&quot;total_bedrooms&quot;].fillna( attribute_median, inplace=True ) . Checking the NULL enties in the dataset . sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head() . Get number of NULL entries in the dataframe columns . null_columns=food_df.columns[food_df.isnull().any()] food_df[null_columns].isnull().sum() . Print full rows having NULL entries in the df . is_NaN = food_df.isnull() row_has_NaN = is_NaN.any(axis=1) rows_with_NaN = food_df[row_has_NaN] . Dropping NULL only from a particular column . df_income_drop_na = df.dropna(subset=[&#39;INCOME2&#39;]) . Join two datasets . 1. Inner join . Only returns rows with matching values in both df. . A_B = A.merge(B, on = &lt;common column name&gt;, suffixes = tuples to append the name of columns with similar names) . Remember that .merge() only returns rows where the values match in both tables. . 2. Merging more than one table . df1.merge(df2, on=&#39;col_A&#39;) .merge(df3, on=&#39;col_B&#39;) .merge(df4, on=&#39;col_C&#39;) . 3. Merge across multiple columns tags . df1.merge( df2, on = [&#39;col_A&#39;, &#39;col_B&#39;]) . Searching . Find columns names based on a string . df_raw_data.columns[df_raw_data.columns.str.contains(&#39;STRING_SUBSET&#39;)] . Filter entries in the column based on the threshold . Data has indian-inspired international cuisines which are not what we are interested in cuisin_counts = food_df[&#39;Cuisine&#39;].value_counts() cuisin_counts_more_than_50 = cuisin_counts.iloc[np.where(cuisin_counts &gt; 50)] food_df_top_cuisine = food_df.loc[ food_df[&#39;Cuisine&#39;].isin(list(cuisin_counts_more_than_50.index)) ] #Dropping entries in `food_df` which have non-ind . | . Clean up entries with partial matches . df.loc[df[&#39;Store Name&#39;].str.contains(&#39;Wal&#39;, case=False), &#39;Store_Group_1&#39;] = &#39;Walmarts&#39; . south_indian_tag = [&#39;Chettinad&#39;, &#39;Andhra&#39;, &#39;Karnataka&#39;, &#39;Tamil Nadu&#39;, &#39;Kerala Recipes&#39;, &#39;South Indian Recipes&#39;] food_df_top_cuisine.loc[food_df_top_cuisine[&#39;Cuisine&#39;].isin(south_indian_tag), &#39;Combined_cuisine&#39;] = &#39;South Indian&#39; . With or statements . String_filter_option = [&#39;cond_1&#39;, &#39;cond_2&#39;] pandas_dataframe[ Pandas_dataframe[ &#39;columns&#39; ].str.contains(&#39;|&#39;.join(string_filter_option)) ] . Filter rows in the pandas df with another list . month_list = [&#39;May&#39;,&#39;Jun&#39;,&#39;Jul&#39;,&#39;Aug&#39;,&#39;Sep&#39;] df_pH.loc[df_pH[&#39;Month&#39;].isin(month_list)] . Filter out values using names: Making a separate list of those that DO NOT satisfy the constraint . no_bands = halftime_musicians[ ~halftime_musicians.musician.str.contains(&#39;Marching&#39;) ] . Statistics &amp; Distributions . Histogram . df.hist(&#39;WTKG3&#39;) . CDF and PDF . # Functions for PMF and CDF, we will come to those later in the notebook def pmf(pandas_series): values, counts = np.unique(pandas_series, return_counts = True) pmf = np.c_[ values, counts / sum(counts) ] return pmf def cdf(pandas_series): values, counts = np.unique(pandas_series, return_counts = True) pmf = np.c_[ values, counts / sum(counts) ] cdf = np.zeros(shape=pmf.shape) for i in range(0, pmf.shape[0]): cdf[i] = [pmf[i][0], np.sum(pmf[:i+1], axis=0)[-1]] return cdf . Confidence interval . A bootstrap analysis of the reduction of deaths due to handwashing . boot_mean_diff = [] for i in range(3000): boot_before = before_proportion.sample(frac=1, replace=True) boot_after = after_proportion.sample(frac=1, replace=True) boot_mean_diff.append( boot_after.mean() - boot_before.mean() ) . Calculating a 95% confidence interval from boot_mean_diff . confidence_interval = pd.Series(boot_mean_diff).quantile([0.025,0.975]) . Convert variables . Convert continuous variable to discrete . pd.cut . Example 1: . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], #bins around 2-5 income bracket labels=[1, 2, 3, 4, 5]) . Use cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins. . Example 2: . pd.cut(iris_df[&#39;sepal_length&#39;], bins=3, right=True, labels=[&#39;low&#39;,&#39;med&#39;,&#39;high&#39;], retbins=True) . Fine tune the labeling . def convert_to_cat(panda_series): first_quarter = panda_series.describe()[&#39;25%&#39;] third_quarter = panda_series.describe()[&#39;75%&#39;] print(first_quarter, third_quarter) cat_list = [&#39;temp&#39;] * len(panda_series) for i, entry in enumerate(panda_series): if entry &lt;= first_quarter: cat_list[i] = &#39;SMALL&#39; elif first_quarter &lt; entry &lt;= third_quarter: cat_list[i] = &#39;MED&#39; else: cat_list[i] = &#39;LARGE&#39; return cat_list . Cateogorical variables to one-hot . # Pandas get dummies is one option pd.get_dummies(iris_df[&#39;sepal_width_cat&#39;], prefix=&#39;sepal_width&#39;) . One-hot discrete variable with more granularity . def OHE_discreet(point, pandas_series, intervals): &#39;&#39;&#39; define range for one-hot, for every entry find the closest value in the one-hot &#39;&#39;&#39; z = np.linspace(min(pandas_series), max(pandas_series), intervals) ohe = np.zeros(len(z)) ohe[np.argmin(abs(z - point)**2)] = 1 return ohe iris_df[&#39;sepal_width_OHE&#39;] = iris_df[&#39;sepal_width&#39;].apply(OHE_discreet, args=(iris_df[&#39;sepal_width&#39;], 11)) . Grouping data by entries in a row: . Example 1 . licenses_zip_ward.groupby(&#39;alderman&#39;).agg({&#39;income&#39;:&#39;median&#39;}) . Estimate the statistic of ‘income’ after grouping the dataframe by row entries in column ‘alderman’ . Example 2 . counted_df = licenses_owners.groupby(&#39;title&#39;).agg({&#39;account&#39;:&#39;count&#39;}) . I want to know the number of account each unique title entry has in the df. Here the column account was counted and the total entries were reported when the data frame was first grouped by entries in the title column. . Example 3 . Groupby multiple columns and show the counts . # Create a column that will store the month data[&#39;month&#39;] = data[&#39;date&#39;].dt.month # Create a column that will store the year data[&#39;year&#39;] = data[&#39;date&#39;].dt.year # Group by the month and year and count the pull requests counts = data.groupby([&#39;month&#39;,&#39;year&#39;])[&#39;pid&#39;].count() . Example 4 . Group and pivot table. Find the number of pull_request for the repo every year for the two authors: . # The developers we are interested in authors = [&#39;xeno-by&#39;, &#39;soc&#39;] # Get all the developers&#39; pull requests by_author = pulls.loc[ pulls[&#39;user&#39;].isin(authors) ] by_author[&#39;year&#39;] = by_author[&#39;date&#39;].dt.year # Count the number of pull requests submitted each year counts = by_author.groupby([&#39;user&#39;, &#39;year&#39;]).agg({&#39;pid&#39;: &#39;count&#39;}).reset_index() # Convert the table to a wide format counts_wide = counts.pivot_table(index=&#39;year&#39;, columns=&#39;user&#39;, values=&#39;pid&#39;, fill_value=0) # Plot the results counts_wide .",
            "url": "http://pgg1610.github.io/blog_fastpages/exploratory-data-analysis/machine-learning/resources/2021/06/13/pandas_resources.html",
            "relUrl": "/exploratory-data-analysis/machine-learning/resources/2021/06/13/pandas_resources.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Material-informatics Literature and Resources",
            "content": "Last update: 28th May 2021 . Material Informatics is the solid-state, inorganic chemistry focused cousin to its organic chemistry contemporary: Cheminformatics. In spirit, the aim of Material Informatics is similar to Cheminformatics; it offers a promising avenue to augment traditional material R&amp;D processes. Amplify the conventional material discovery task using data, analytics, and identify chemical spaces, and structure in the data, which are interesting and probe those rigorously using first-principles techniques and/or experimentation. . The potential application of material informatics can be seen in: Microelectronics, aerospace, and automotive to defense, clean energy, and health services, where ever there’s a demand for new advanced materials at even greater rates and lower costs. . Application of material informatics in atomic-scale modeling: . In case of molecular-level modeling of material properties, concepts developed in material informatics, statistics, and ML can be used for: . Descriptor driven screening of computational models . | Discover new science and relations from large computational datasets . | Applying surrogate models to enable fast materials development . | Undertake global optimization routines using surrogate models for composition and property predictions. . | Machine learning in atomic-scale modeling is often used to replace expensive ab initio methods with cheaper approximations. While certainly lucractive an additional consideration for ML use-case is its utility as a surrogate model to help researchers identify interesting regions in the material space. It also helps to decode the ‘intuition’ and serendipity involved in material development and hopefully provide a rigorous data driven basis for a design decision. . Below are few reviews, articles, and resources I’ve found that document the state-of-the-art for material informatics. It goes without saying that this is a highly biased and a non-exhaustive listing of articles covering only the ones I’ve read. The idea with this document is to provide a starting point in understanding the general status of the field. . Special Issues: . Nature Materials collection of review articles discussing the role of computation for material design . | Matter journal’s Material prediction using data and ML prediction . | Nature Communications compendium on ML for material modelling . | . Reviews: . C. Chen, Y. Zuo, W. Ye, X. Li, Z. Deng, and S. P. Ong, “A Critical Review of Machine Learning of Energy Materials,” Adv. Energy Mater., vol. 1903242, p. 1903242, Jan. 2020. . | J. Schmidt, M. R. G. Marques, S. Botti, and M. A. L. Marques, “Recent advances and applications of machine learning in solid-state materials science,” npj Comput. Mater., vol. 5, no. 1, p. 83, Dec. 2019. . | J. Noh, G. H. Gu, S. Kim, and Y. Jung, “Machine-enabled inverse design of inorganic solid materials: promises and challenges,” Chem. Sci., vol. 11, no. 19, pp. 4871–4881, 2020. . | S. M. Moosavi, K. M. Jablonka, and B. Smit, “The Role of Machine Learning in the Understanding and Design of Materials,” J. Am. Chem. Soc., no. Figure 1, p. jacs.0c09105, Nov. 2020. . | F. Häse, L. M. Roch, P. Friederich, and A. Aspuru-Guzik, “Designing and understanding light-harvesting devices with machine learning,” Nat. Commun., vol. 11, no. 1, pp. 1–11, 2020. . | M. Moliner, Y. Román-Leshkov, and A. Corma, “Machine Learning Applied to Zeolite Synthesis: The Missing Link for Realizing High-Throughput Discovery,” Acc. Chem. Res., vol. 52, no. 10, pp. 2971–2980, 2019. . | Best practices in material informatics: . A. Y. T. Wang et al., “Machine Learning for Materials Scientists: An Introductory Guide toward Best Practices,” Chem. Mater., vol. 32, no. 12, pp. 4954–4965, 2020. . Featurizations possible: . Similar to other machine-learning development efforts – featurization or descriptors used to convert material entries in machine-readable format is crucial for the eventual performance of any statistical model. Over the years there has been tremendous progress in describing the periodic solid crystal structures. Some of the key articles I’ve liked are mentioned below: . Reviews: . A. P. Bartók, R. Kondor, and G. Csányi, “On representing chemical environments,” Phys. Rev. B - Condens. Matter Mater. Phys., vol. 87, no. 18, pp. 1–16, 2013. . | A. Seko, H. Hayashi, K. Nakayama, A. Takahashi, and I. Tanaka, “Representation of compounds for machine-learning prediction of physical properties,” Phys. Rev. B, vol. 95, no. 14, pp. 1–11, 2017. . | K. T. Schütt, H. Glawe, F. Brockherde, A. Sanna, K. R. Müller, and E. K. U. Gross, “How to represent crystal structures for machine learning: Towards fast prediction of electronic properties,” Phys. Rev. B - Condens. Matter Mater. Phys., vol. 89, no. 20, pp. 1–5, 2014. . | . Articles: . 1. Composition based: . L. Ward et al., “Including crystal structure attributes in machine learning models of formation energies via Voronoi tessellations,” Phys. Rev. B, vol. 96, no. 2, 2017 | . Predicting properties of crystalline compounds using a representation consisting of attributes derived from the Voronoi tessellation of its structure and composition based features is both twice as accurate as existing methods and can scale to large training set sizes. Also the representations are insensitive to changes in the volume of a crystal, which makes it possible to predict the properties of the crystal without needing to compute the DFT-relaxed geometry as input. Random forrest algorithm used for the prediction . A. Wang, S. Kauwe, R. Murdock, and T. Sparks, “Compositionally-Restricted Attention-Based Network for Materials Property Prediction.” 20-Feb-2020. | . Using attention-based graph networks on material composition to predict material properties. . Goodall, R.E.A., Lee, A.A. Predicting materials properties without crystal structure: deep representation learning from stoichiometry. Nat Commun 11, 6280 (2020) | . Similar to the previous article in spirit, here authors use material composition to generate weighted graphs and predict material properties . 2. Structural based: . T. Xie and J. C. Grossman, “Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties,” Phys. Rev. Lett., vol. 120, no. 14, p. 145301, 2018. | . Material modeling benchmark studies: . Bartel, C.J., Trewartha, A., Wang, Q. et al. A critical examination of compound stability predictions from machine-learned formation energies. npj Comput Mater 6, 97 (2020) | . Investigate if ML models can distinguish materials wrt thermodynamic stability and not just formation energies. Learning formation energy from composition alone is fine for MAE and RMSE representations. Propose that graph-based methods reduce the MAE by roughly 50% compared with the best performing compositional model. Show that including structural information is advantageous when predicting formation energies. . A. J. Chowdhury, W. Yang, E. Walker, O. Mamun, A. Heyden, and G. A. Terejanu, “Prediction of Adsorption Energies for Chemical Species on Metal Catalyst Surfaces Using Machine Learning,” J. Phys. Chem. C, vol. 122, no. 49, pp. 28142–28150, 2018 | . Consider various encoding scheme and machine learning models to predict single adsorbate binding energy for carbon-based adsorabtes on transition metal surfaces. They show linear methods and scaling relationship hold well compared to ML methods. They found that for ML models to succeed, it is not necessary to use advanced (geometric) coordinate-based descriptors; simple descriptors, such as bond count, can provide satisfactory results. As many catalysis and materials science problems require significant time to generate each data point, in many cases the ML models would need to work with a relatively small-sized dataset . Rosen, Andrew; Iyer, Shaelyn; Ray, Debmalya; Yao, Zhenpeng; Aspuru-Guzik, Alan; Gagliardi, Laura; et al. (2020): Machine Learning the Quantum-Chemical Properties of Metal–Organic Frameworks for Accelerated Materials Discovery with a New Electronic Structure Database. ChemRxiv. Preprint | . Articles: . There is a rich history of using statistical model and data mining for predicting bulk inorganic crystal properties. The review articles mentioned in the above section discuss those areas quite nicely. In this section particularly focusses on papers looking at apply informatics to encode surfaces for modeling heterogeneous catalyst surfaces, which is fairly new and very active research direction: . Ma, X., Li, Z., Achenie, L.E.K., and Xin, H. (2015). Machine-learning-augmented chemisorption model for CO2 electroreduction catalyst screening. J. Phys. Chem. Lett. 6, 3528–3533. . | F. Liu, S. Yang, and A. J. Medford, “Scalable approach to high coverages on oxides via iterative training of a machine-learning algorithm,” ChemCatChem, vol. 12, no. 17, pp. 4317–4330, 2020. . | C. S. Praveen and A. Comas-Vives, “Design of an Accurate Machine Learning Algorithm to Predict the Binding Energies of Several Adsorbates on Multiple Sites of Metal Surfaces,” ChemCatChem, vol. n/a, no. n/a, 2020. . | Z. Li, L. E. K. Achenie, and H. Xin, “An Adaptive Machine Learning Strategy for Accelerating Discovery of Perovskite Electrocatalysts,” ACS Catal., vol. 10, no. 7, pp. 4377–4384, 2020. . | R. García-Muelas and N. López, “Statistical learning goes beyond the d-band model providing the thermochemistry of adsorbates on transition metals,” Nat. Commun., vol. 10, no. 1, p. 4687, Dec. 2019. . | M. Rueck, B. Garlyyev, F. Mayr, A. S. Bandarenka, and A. Gagliardi, “Oxygen Reduction Activities of Strained Platinum Core-Shell Electrocatalysts Predicted by Machine Learning,” J. Phys. Chem. Lett., 2020. . | W. Xu, M. Andersen, and K. Reuter, “Data-Driven Descriptor Engineering and Refined Scaling Relations for Predicting Transition Metal Oxide Reactivity,” ACS Catal., vol. 11, no. 2, pp. 734–742, Jan. 2021. . | Liu, F., Yang, S. &amp; Medford, A. J. Scalable approach to high coverages on oxides via iterative training of a machine-learning algorithm. ChemCatChem 12, 4317–4330 (2020). . | . Graph-network based approaches for encoding and predicting surface binding energies: . Back, S. et al. Convolutional Neural Network of Atomic Surface Structures to Predict Binding Energies for High-Throughput Screening of Catalysts. J. Phys. Chem. Lett. 10, 4401–4408 (2019) . | Lym, J., Gu, G. H., Jung, Y. &amp; Vlachos, D. G. Lattice convolutional neural network modeling of adsorbate coverage effects. J. Phys. Chem. C 123, 18951–18959 (2019). . | . Adsorbate binding predictions have been recently extended to cover high-entropy alloy surfaces as well: . T. A. A. Batchelor et al., “Complex solid solution electrocatalyst discovery by computational prediction and high‐throughput experimentation,” Angew. Chemie Int. Ed., p. anie.202014374, Dec. 2020. . | J. K. Pedersen, T. A. A. Batchelor, D. Yan, L. E. J. Skjegstad, and J. Rossmeisl, “Surface electrocatalysis on high-entropy alloys,” Curr. Opin. Electrochem., vol. 26, p. 100651, Apr. 2021. . | Z. Lu, Z. W. Chen, and C. V. Singh, “Neural Network-Assisted Development of High-Entropy Alloy Catalysts: Decoupling Ligand and Coordination Effects,” Matter, vol. 3, no. 4, pp. 1318–1333, 2020. . | . Global optimization methods: . M. K. Bisbo and B. Hammer, “Efficient global structure optimization with a machine learned surrogate model,” Phys. Rev. Lett., vol. 124, no. 8, p. 86102, 2019. . | J. Dean, M. G. Taylor, and G. Mpourmpakis, “Unfolding adsorption on metal nanoparticles: Connecting stability with catalysis,” Sci. Adv., vol. 5, no. 9, 2019. . | . Uncertainty quantification (UQ): . A. Wang et al., “A Framework for Quantifying Uncertainty in DFT Energy Corrections.” 19-May-2021 | . Method to comment on the uncertainty of DFT errors which accounts for both sources of uncertainty: experimental and model parameters. Fit energy corrections using a set of 222 binary and ternary compounds for which experimental and computed values are present. Quantifying this uncertainty can help reveal cases wherein empirically-corrected DFT calculations are limited to differentiate between stable and unstable phases. Validate this approach on Sc-W-O phase diagram analysis. . Feng, J., Lansford, J. L., Katsoulakis, M. A., &amp; Vlachos, D. G. (2020). Explainable and trustworthy artificial intelligence for correctable modeling in chemical sciences. Science advances, 6(42) | . Propose Bayesian networks, type of probabilistic graphical models, to integrate physics- and chemistry-based data and uncertainty. Demonstrate this framework in searching for the optimal reaction rate and oxygen binding energy for the oxygen reduction reaction (ORR) using the volcano model. Their model is able to comment on the source of uncertainty in the model. . K. Tran, W. Neiswanger, J. Yoon, Q. Zhang, E. Xing, and Z. W. Ulissi, “Methods for comparing uncertainty quantifications for material property predictions,” pp. 1–29, Dec. 2019 | . Helpful overview and benchmark of various model flavors and metrics to understand ways of reporting the confidence in model predictions for material properties. Interesting convolution-Fed Gaussian Process (CFGP) model framework looked into which is a combination of CGCNN and GP: pooled outputs of the convolutional layers of the network as features in a new GP. This was also their best model from the collection. Nice overview of different metrics used for comparing methods for UQ. . Active learning: . A. Seko and S. Ishiwata, “Prediction of perovskite-related structures in ACuO3-x (A = Ca, Sr, Ba, Sc, Y, La) using density functional theory and Ba,” Phys. Rev. B, vol. 101, no. 13, p. 134101, Apr. 2020. . | K. Tran and Z. W. Ulissi, Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution, vol. 1, no. 9. Springer US, 2018. . | D. Xue, P. V. Balachandran, J. Hogden, J. Theiler, D. Xue, and T. Lookman, “Accelerated search for materials with targeted properties by adaptive design,” Nat. Commun., vol. 7, pp. 1–9, 2016. . | . Surrogate optimizer and accelerating TS searches: . O.-P. Koistinen, F. B. Dagbjartsdóttir, V. Ásgeirsson, A. Vehtari, and H. Jónsson, “Nudged elastic band calculations accelerated with Gaussian process regression,” J. Chem. Phys., vol. 147, no. 15, p. 152720, Oct. 2017. . | J. A. Garrido Torres, P. C. Jennings, M. H. Hansen, J. R. Boes, and T. Bligaard, “Low-Scaling Algorithm for Nudged Elastic Band Calculations Using a Surrogate Machine Learning Model,” Phys. Rev. Lett., vol. 122, no. 15, pp. 1–6, 2019. . | E. Garijo del Río, J. J. Mortensen, and K. W. Jacobsen, “Local Bayesian optimizer for atomic structures,” Phys. Rev. B, vol. 100, no. 10, pp. 1–9, 2019. . | . Combining experiments + theory: . E. O. Ebikade, Y. Wang, N. Samulewicz, B. Hasa, and D. Vlachos, “Active learning-driven quantitative synthesis–structure–property relations for improving performance and revealing active sites of nitrogen-doped carbon for the hydrogen evolution reaction,” React. Chem. Eng., 2020. . | A. Smith, A. Keane, J. A. Dumesic, G. W. Huber, and V. M. Zavala, “A machine learning framework for the analysis and prediction of catalytic activity from experimental data,” Appl. Catal. B Environ., vol. 263, no. October 2019, p. 118257, 2020. . | M. Zhong et al., Accelerated discovery of CO2 electrocatalysts using active machine learning, vol. 581, no. 7807. 2020. . | A. J. Saadun et al., “Performance of Metal-Catalyzed Hydrodebromination of Dibromomethane Analyzed by Descriptors Derived from Statistical Learning,” ACS Catal., vol. 10, no. 11, pp. 6129–6143, Jun. 2020. . | J. L. Lansford and D. G. Vlachos, “Infrared spectroscopy data- and physics-driven machine learning for characterizing surface microstructure of complex materials,” Nat. Commun., vol. 11, no. 1, p. 1513, Dec. 2020 . | Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments . | Materials genes of heterogeneous catalysis from clean experiments and artificial intelligence . | N. Artrith, Z. Lin, and J. G. Chen, “Predicting the Activity and Selectivity of Bimetallic Metal Catalysts for Ethanol Reforming using Machine Learning,” ACS Catal., vol. 10, no. 16, pp. 9438–9444, Aug. 2020. . | S. Nellaiappan et al., “High-Entropy Alloys as Catalysts for the CO2 and CO Reduction Reactions: Experimental Realization,” ACS Catal., vol. 10, no. 6, pp. 3658–3663, 2020. . | . Reaction Network Predictions: . M. Liu et al., “Reaction Mechanism Generator v3.0: Advances in Automatic Mechanism Generation,” J. Chem. Inf. Model., May 2021. | . Newest version of RMG (v3) is updated to Python v3. It has ability to generate heterogeneous catalyst models, uncertainty analysis to conduct first order sensitivity analysis. RMG dataset for the thermochemical and kinetic parameters have been expanded. . A Chemically Consistent Graph Architecture for Massive Reaction Networks Applied to Solid-Electrolyte Interphase Formation. ChemRxiv. Blau, Samuel; Patel, Hetal; Spotte-Smith, Evan; Xie, Xiaowei; Dwaraknath, Shyam; Persson, Kristin (2020) | . Develop a multi-reactant representation scheme to look at arbitrary reactant product pairs. Apply this technique to understand electrochemical reaction network for Li-ion solid electrolyte interphase. . Predicting chemical reaction pathways in solid state material synthesis | . Chemical reaction network model to predict synthesis pathway for exotic oxides. Solid-state synthesis procedures for YMnO3, Y2Mn2O7, Fe2SiS4, and YBa2Cu3O6.5 are proposed and compared to literature pathways. Finally apply the algorithm to search for a probable synthesis route to make MgMo3(PO4)3O, battery cathode material that has yet to be synthesized. . Discovering Competing Electrocatalytic Mechanisms and Their Overpotentials: Automated Enumeration of Oxygen Evolution Pathways, A. Govind Rajan and E. A. Carter, J. Phys. Chem. C, vol. 124, no. 45, pp. 24883–24898, Nov. 2020. . | Accurate Thermochemistry of Complex Lignin Structures via Density Functional Theory, Group Additivity, and Machine Learning, Q. Li, G. Wittreich, Y. Wang, H. Bhattacharjee, U. Gupta, and D. G. Vlachos, ACS Sustain. Chem. Eng., vol. 9, no. 8, pp. 3043–3049, Mar. 2021. . | Ziyun Wang, Yuguang Li, Jacob Boes et al. CO2 Electrocatalyst Design Using Graph Theory, 21 September 2020, Preprint . | . Generative Models: . Review: . J. Noh et al., “Inverse Design of Solid-State Materials via a Continuous Representation,” Matter, vol. 1, no. 5, pp. 1370–1384, 2019. . Articles: . S. Kim, J. Noh, G. H. Gu, A. Aspuru-Guzik, and Y. Jung, “Generative Adversarial Networks for Crystal Structure Prediction,” pp. 1–37, 2020 . | B. Kim, S. Lee, and J. Kim, “Inverse design of porous materials using artificial neural networks,” Sci. Adv., vol. 6, no. 1, 2020 . | Z. Yao et al., “Inverse Design of Nanoporous Crystalline Reticular Materials with Deep Generative Models,” 2020 . | . Semantically constrained graph-based code for presenting a MOFs. Target property directed optimization. Encode MOFs as edges, vertices, topologies. Edges are molecular fragments with two connecting points, verticies contain node information, topologies indicate a definite framework. Supramolecular Variational Autoencoder (SmVAE) with several corresponding components that oversee encoding and decoding each part of the MOF: Map the frameworks with discrete representations (RFcodes) into continuous vectors (z) and then back. . Discovering Relationships between OSDAs and Zeolites through Data Mining and Generative Neural Networks | . Datasets: . While we can attribute the recent interest in material informatics to democratization of data analytics and ML packages, growing set of benchmark datasets of materials from multiple research institution has been crucial for development of new methods, algorithms and providing a consistent set of comparison. . OC20 dataset (CMU + Facebook). Paper. Github | . Dataset comprising of surface heterogeneous adsorbates. . Catalysis Hub from SUNCAT. Website | . Surface Reactions database contains thousands of reaction energies and barriers from density functional theory (DFT) calculations on surface systems . Materials Project | . Besides providing a collection of over 130,000 inorganic compounds and 49,000 molecules and counting, with calculated phase diagrams, structural, thermodynamic, electronic, magnetic, and topological properties it also provides analysis tools for post-processing. . OQMD from Chris Wolverton’s Group | . 815,000+ materials with calculated thermodynamic and structural properties. . ICSD | . 210,000+ inorganic crystal structures from literature. Requires subscription. . JARVIS by NIST | . Includes calculated materials properties, 2D materials, and tools for ML and high-throughput tight-binding. . C2DB | . Structural, thermodynamic, elastic, electronic, magnetic, and optical properties of around 4000 two-dimensional (2D) materials distributed over more than 40 different crystal structures. . AFLOW | . Millions of materials and calculated properties, focusing on alloys. . Citrination | . Contributed and curated datasets from Citrine Informatics . MDPS | . Fascinating resource linking scientific publications using the Pauling File database (relational database of published literature for material scientists) . Curated list of material informatics packages | . Packages: . Perovskite oxide stability . | DOSNet . | Open catalysis dataset . | AENet . | AMP . | AMPtorch (PyTorch implementation of AMP) . | Feature engineering for Perovskite’s electronic structure properties . | MEGNET . | SISSO . | Catlearn . | Matminer . | Mat2Vec . | .",
            "url": "http://pgg1610.github.io/blog_fastpages/chemical-science/machine-learning/resources/2021/05/21/MaterialInformatics_Resources.html",
            "relUrl": "/chemical-science/machine-learning/resources/2021/05/21/MaterialInformatics_Resources.html",
            "date": " • May 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Cheminformatics basics doodle 0.1 - RDkit",
            "content": "Walkthrough on reading, visualizing, manipulating molecules and their properties using Pandas and RDKit for Cheminformatics-related tasks. In this file we will be primarly using SMILES to describe the molecules of choice. . What are SMILES? . Simplified molecular input line entry system (SMILES) is a form of line notation for describing the structure of chemical species using text strings. More details can be found here . Other resources: . Nice low-barrier intro to using some basic functions in RDkit: Xinhao Lin&#39;s RDKit Cheatsheet, I&#39;ve adopted some of the functions frrom that blog in here: | Rdkit Tutorial github | Patrick Walters&#39; Learning Cheminformatics Post | Getting Started RDKit Official Blog | Video walkthrough tutorial on RDKit by Jan Jensen @ University of Copanhagen | . Compilation of various recipes submitted by the community: . RDkit Cookbook | . import os import pandas as pd import numpy as np . The majority of the basic molecular functionality is found in module rdkit.Chem . import rdkit from rdkit import Chem #This gives us most of RDkits&#39;s functionality from rdkit.Chem import Draw from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules IPythonConsole.ipython_useSVG=True #SVG&#39;s tend to look nicer than the png counterparts print(rdkit.__version__) # Mute all errors except critical Chem.WrapLogs() lg = rdkit.RDLogger.logger() lg.setLevel(rdkit.RDLogger.CRITICAL) . 2021.03.2 . import matplotlib.pyplot as plt from matplotlib.pyplot import cm # High DPI rendering for mac %config InlineBackend.figure_format = &#39;retina&#39; # Plot matplotlib plots with white background: %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 15, &#39;axes.titlesize&#39; : 15, &#39;axes.labelsize&#39; : 15, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 12, &#39;ytick.labelsize&#39; : 12, } plt.rcParams.update(plot_params) . Basics . mol = Chem.MolFromSmiles(&quot;c1ccccc1&quot;) . mol is a special type of RDkit object . type(mol) . rdkit.Chem.rdchem.Mol . To display the molecule: . mol . By default SMILES (atleast the ones we deal with RDkit) accounts H atoms connected with C atoms implicitly based on the valence of the bond. You can add H explicitly using the command below: . Chem.AddHs(mol) . Chem.RemoveHs(mol) . def mol_with_atom_index(mol): for atom in mol.GetAtoms(): atom.SetAtomMapNum(atom.GetIdx()) return mol . mol_with_atom_index(mol) . IPythonConsole.drawOptions.addAtomIndices = True mol = Chem.MolFromSmiles(&quot;c1ccccc1&quot;) mol . Descriptors for molecules . We can find more information about the molecule using rdkit.Chem.Descriptors : More information here . from rdkit.Chem import Descriptors . mol_wt = Descriptors.ExactMolWt(mol) print(&#39;Mol Wt: {:0.3f}&#39;.format(mol_wt)) . Mol Wt: 78.047 . heavy_mol_wt = Descriptors.HeavyAtomMolWt(mol) print(&#39;Heavy Mol Wt: {:0.3f}&#39;.format(heavy_mol_wt)) . Heavy Mol Wt: 72.066 . ring_count = Descriptors.RingCount(mol) print(&#39;Number of rings: {:0.3f}&#39;.format(ring_count)) . Number of rings: 1.000 . rotatable_bonds = Descriptors.NumRotatableBonds(mol) print(&#39;Number of rotatable bonds: {:0.3f}&#39;.format(rotatable_bonds)) . Number of rotatable bonds: 0.000 . num_aromatic_rings = Descriptors.NumAromaticRings(mol) print(&#39;Number of aromatic rings: {:0.3f}&#39;.format(num_aromatic_rings)) . Number of aromatic rings: 1.000 . Get some more information about the molecule: . MolToMolBlock: To get Coordinates and bonding details for the molecule . mol_block = Chem.MolToMolBlock(mol) print(mol_block) . RDKit 2D 6 6 0 0 0 0 0 0 0 0999 V2000 1.5000 0.0000 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 0.7500 -1.2990 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 -0.7500 -1.2990 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 -1.5000 0.0000 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 -0.7500 1.2990 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 0.7500 1.2990 0.0000 C 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 0 2 3 1 0 3 4 2 0 4 5 1 0 5 6 2 0 6 1 1 0 M END . Little more on the molecule drawing . Additional discussion on different ways to represnt and draw molecules in RDkit. This section will work in RDkit version &gt; 2020.03. . I am following the code introduced in the official RDkit blogpost . Nice example found on Pen&#39;s blogpost | . from collections import defaultdict from rdkit.Chem.Draw import rdMolDraw2D from IPython.display import SVG . diclofenac = Chem.MolFromSmiles(&#39;O=C(O)Cc1ccccc1Nc1c(Cl)cccc1Cl&#39;) diclofenac . Substructure highlights: Let&#39;s look at the the C=O and the -NH species in the molecule . sub_pattern = Chem.MolFromSmarts(&#39;O=CCccN&#39;) hit_ats = list(diclofenac.GetSubstructMatch(sub_pattern)) hit_bonds = [] for bond in sub_pattern.GetBonds(): aid1 = hit_ats[bond.GetBeginAtomIdx()] aid2 = hit_ats[bond.GetEndAtomIdx()] hit_bonds.append( diclofenac.GetBondBetweenAtoms(aid1, aid2).GetIdx() ) . d2d = rdMolDraw2D.MolDraw2DSVG(400, 400) # or MolDraw2DCairo to get PNGs rdMolDraw2D.PrepareAndDrawMolecule(d2d, diclofenac, highlightAtoms=hit_ats, highlightBonds=hit_bonds) d2d.FinishDrawing() SVG(d2d.GetDrawingText()) . Specify individual color and bonds . rings = diclofenac.GetRingInfo() type(rings) . rdkit.Chem.rdchem.RingInfo . colors = [(0.8,0.0,0.8),(0.8,0.8,0),(0,0.8,0.8),(0,0,0.8)] athighlights = defaultdict(list) arads = {} for i,rng in enumerate(rings.AtomRings()): for aid in rng: athighlights[aid].append(colors[i]) arads[aid] = 0.3 bndhighlights = defaultdict(list) for i,rng in enumerate(rings.BondRings()): for bid in rng: bndhighlights[bid].append(colors[i]) . d2d = rdMolDraw2D.MolDraw2DSVG(400,400) d2d.DrawMoleculeWithHighlights(diclofenac,&#39;diclofenac&#39;,dict(athighlights),dict(bndhighlights),arads,{}) d2d.FinishDrawing() SVG(d2d.GetDrawingText()) . Reading dataset of molecules from a csv file . Here we will use Pandas, RDkit to make molecule object for the small sample of molecules. . Sample data . sample_df = pd.read_csv(&#39;./data/sample_molecules.csv&#39;, sep=&#39;,&#39;) . sample_df.head(5) . Name SMILE . 0 Cyclopropane | C1CC1 | . 1 Ethylene | C=C | . 2 Methane | C | . 3 t-Butanol | CC(C)(C)O | . 4 ethane | CC | . sample_df.shape . (115, 2) . from rdkit.Chem import PandasTools . PandasTools module helps add mol molecule objects from RDKit as per the SMILES in the dataframe . PandasTools.AddMoleculeColumnToFrame(sample_df, smilesCol=&#39;SMILE&#39;) . Check the new ROMol columns being appended in the dataframe . sample_df.columns . Index([&#39;Name&#39;, &#39;SMILE&#39;, &#39;ROMol&#39;], dtype=&#39;object&#39;) . sample_df.head(1) . Name SMILE ROMol . 0 Cyclopropane | C1CC1 | | . Visualize the dataframe, add properties of interest at the bottom, you can add index too if need . PandasTools.FrameToGridImage(sample_df[:20], legendsCol=&#39;Name&#39;, molsPerRow=4) . LogP Dataset . (From Wikipedia) The partition coefficient, abbreviated P, is defined as the ratio of the concentrations of a solute between two immisible solvents at equilibrium. Most commonly, one of the solvents is water, while the second is hydrophobic, such as 1-octanol. . $$ log P_ text{oct/wat} = log left( frac{ big[ text{solute} big]_ text{octanol}^ text{un-ionized}}{ big[ text{solute} big]_ text{water}^ text{un-ionized}} right)$$ . Hence the partition coefficient measures how hydrophilic (&quot;water-loving&quot;) or hydrophobic (&quot;water-fearing&quot;) a chemical substance is. Partition coefficients are useful in estimating the distribution of drugs within the body. Hydrophobic drugs with high octanol-water partition coefficients are mainly distributed to hydrophobic areas such as lipid bilayers of cells. Conversely, hydrophilic drugs (low octanol/water partition coefficients) are found primarily in aqueous regions such as blood serum. . The dataset used in this notebook is obtained from Kaggle. This dataset features relatively simple molecules along with their LogP value. This is a synthetic dataset created using XLogP and does not contain experimental validation. . logP_df = pd.read_csv(&#39;./data/logP_dataset.csv&#39;, sep=&#39;,&#39;, header=None, names=[&#39;SMILES&#39;, &#39;LogP&#39;]) . logP_df.head(5) . SMILES LogP . 0 C[C@H]([C@@H](C)Cl)Cl | 2.3 | . 1 C(C=CBr)N | 0.3 | . 2 CCC(CO)Br | 1.3 | . 3 [13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O | 2.0 | . 4 CCCOCCP | 0.6 | . logP_df.shape . (14610, 2) . Visualize the SMILE string . mol_temp = logP_df.iloc[420] . mol_temp . SMILES [2H][C]([2H])[Cl+]Cl LogP 1.6 Name: 420, dtype: object . mol_obj = Chem.MolFromSmiles(mol_temp[&#39;SMILES&#39;]) mol_obj . print(Chem.MolToMolBlock(mol_obj)) . RDKit 2D 5 4 0 0 0 0 0 0 0 0999 V2000 1.2990 0.7500 0.0000 H 0 0 0 0 0 0 0 0 0 0 0 0 0.0000 0.0000 0.0000 C 0 0 0 0 0 3 0 0 0 0 0 0 -1.2990 0.7500 0.0000 H 0 0 0 0 0 0 0 0 0 0 0 0 -0.0000 -1.5000 0.0000 Cl 0 0 0 0 0 2 0 0 0 0 0 0 -1.2990 -2.2500 0.0000 Cl 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 0 2 3 1 0 2 4 1 0 4 5 1 0 M CHG 1 4 1 M RAD 1 2 2 M ISO 2 1 2 3 2 M END . Take a small sample from QM9 dataset . logP_df_smol = logP_df.sample(20).reset_index(drop=True) . logP_df_smol.head(2) . SMILES LogP . 0 CCCSSCC | 2.2 | . 1 C(CO)C(N)Br | -0.1 | . logP_df_smol.shape . (20, 2) . PandasTools module helps add mol molecule objects from RDKit as per the SMILES in the dataframe . PandasTools.AddMoleculeColumnToFrame(logP_df_smol, smilesCol=&#39;SMILES&#39;) . Check the new ROMol columns being appended in the dataframe . logP_df_smol.columns . Index([&#39;SMILES&#39;, &#39;LogP&#39;, &#39;ROMol&#39;], dtype=&#39;object&#39;) . logP_df_smol[&#39;ROMol&#39;][0] . Visualize the dataframe, add properties of interest at the bottom, you can add index too if need . PandasTools.FrameToGridImage(logP_df_smol, legendsCol=&#39;LogP&#39;, molsPerRow=3, subImgSize=(200,200)) . Vanilla linear regression . Let&#39;s try building a model to predict a molecule&#39;s logP value given other descriptors. We will try simple molecular descriptors and check the performance. Some molecule discriptors we will consider: . Molecular weight | Number of rotatable bonds | Number of aromatic compounds | logP_df.head(4) . SMILES LogP . 0 C[C@H]([C@@H](C)Cl)Cl | 2.3 | . 1 C(C=CBr)N | 0.3 | . 2 CCC(CO)Br | 1.3 | . 3 [13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O | 2.0 | . As before we will first convert the SMILES string into a rdkit.Chem.rdchem.Mol object, let&#39;s write a convenience function to do so . _count = 0 for i in range(mol.GetNumAtoms()): if mol.GetAtomWithIdx(i).GetIsAromatic(): _count = _count + 1 print(_count) . 6 . def generate_variables(smiles_list): variable_array = {&#39;SMILES&#39;:[], &#39;ROMol&#39;:[], &#39;Mol_Wt&#39;:[],&#39;Num_Aromatic_rings&#39;:[], &#39;Num_rotate_bonds&#39;:[], &#39;Ratio_Aromatic&#39;:[], &#39;Valence_electrons&#39;:[]} for smile_entry in smiles_list: mol_object = Chem.MolFromSmiles(smile_entry) mol_wt = Descriptors.MolWt(mol_object) mol_aromatic_rings = Descriptors.NumAromaticRings(mol_object) mol_rotatable_bonds = Descriptors.NumRotatableBonds(mol_object) # Calculate % of aromatic atoms in the compd mol_num_heavy_atom = Descriptors.HeavyAtomCount(mol_object) _count_aromatic = 0 for i in range(mol_object.GetNumAtoms()): if mol_object.GetAtomWithIdx(i).GetIsAromatic() == True: _count_aromatic = _count_aromatic + 1 mol_aromatic_ratio = _count_aromatic / mol_num_heavy_atom mol_val_electrons = Descriptors.NumValenceElectrons(mol_object) variable_array[&#39;SMILES&#39;].append(smile_entry) variable_array[&#39;ROMol&#39;].append(mol_object) variable_array[&#39;Mol_Wt&#39;].append(mol_wt) variable_array[&#39;Num_Aromatic_rings&#39;].append(mol_aromatic_rings) variable_array[&#39;Num_rotate_bonds&#39;].append(mol_rotatable_bonds) variable_array[&#39;Ratio_Aromatic&#39;].append(mol_aromatic_ratio) variable_array[&#39;Valence_electrons&#39;].append(mol_val_electrons) return variable_array . Look at a subset from the total logP data . df_5k = logP_df.sample(5000, random_state=42) . variable_dict = generate_variables(df_5k.SMILES) variable_df = pd.DataFrame(variable_dict, columns=variable_dict.keys()) . df_var_5k = df_5k.merge(variable_df, on=&#39;SMILES&#39;) df_var_5k.head(2) . SMILES LogP ROMol Mol_Wt Num_Aromatic_rings Num_rotate_bonds Ratio_Aromatic Valence_electrons . 0 CNCSC | 0.5 | | 91.179 | 0 | 2 | 0.0 | 32 | . 1 CNCSC=C | 1.0 | | 103.190 | 0 | 3 | 0.0 | 36 | . Setup model . df_var_5k.columns . Index([&#39;SMILES&#39;, &#39;LogP&#39;, &#39;ROMol&#39;, &#39;Mol_Wt&#39;, &#39;Num_Aromatic_rings&#39;, &#39;Num_rotate_bonds&#39;, &#39;Ratio_Aromatic&#39;, &#39;Valence_electrons&#39;], dtype=&#39;object&#39;) . fig, ax = plt.subplots(1,1, figsize=(10,10)) df_var_5k.hist(ax = ax); . &lt;ipython-input-48-9e19fbae4460&gt;:2: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared df_var_5k.hist(ax = ax); . Split into train and validation set . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split df_train, df_test = train_test_split(df_var_5k, test_size=0.3, random_state=42) . print(df_train.shape, df_test.shape) . (3500, 8) (1500, 8) . Drop some columns that arent that useful for prediction . X_train = df_train.drop(columns = [&#39;LogP&#39;,&#39;ROMol&#39;,&#39;SMILES&#39;, &#39;Ratio_Aromatic&#39;, &#39;Num_Aromatic_rings&#39;]).values y_train = df_train.LogP.values . Pre-process input data to normalize the scale of the descriptors . std_scaler = StandardScaler() X_train_std = std_scaler.fit_transform(X_train) . from sklearn.linear_model import LinearRegression . model = LinearRegression() model.fit(X_train_std, y_train) . LinearRegression() . y_pred = model.predict(X_train_std) . plt.scatter(y_train, y_pred, alpha=0.6) plt.xlabel(&#39;True Value&#39;) plt.ylabel(&#39;Predicted&#39;) . Text(0, 0.5, &#39;Predicted&#39;) . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(y_train, y_pred, alpha=0.6, label=&#39;Linear Regression&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] # Linear fit reg = np.polyfit(y_train, y_pred, deg=1) ax.plot(lims, reg[0] * np.array(lims) + reg[1], &#39;r--&#39;, linewidth=1.5, label=&#39;Linear Fit&#39;) ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;Parity Line&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlabel(&#39;True value&#39;) ax.set_ylabel(&#39;Model predicted&#39;) ax.legend(loc=&#39;best&#39;); . Evaluate model success . One of the simplest ways we can evaluate the success of a linear model is using the coefficient of determination (R2) which compares the variation in y alone to the variation remaining after we fit a model. Another way of thinking about it is comparing our fit line with the model that just predicts the mean of y for any value of x. . Another common practice is to look at a plot of the residuals to evaluate our ansatz that the errors were normally distributed. . In practice, now that we have seen some results, we should move on to try and improve the model. We won&#39;t do that here, but know that your work isn&#39;t done after your first model (especially one as cruddy as this one). It&#39;s only just begun! . Calculate R2 using: $$R^2 =1 - frac{ sum (y_i - hat{y})^2}{ sum (y_i - bar{y})^2}$$ . SS_residuals = np.sum( (y_train - y_pred)**2 ) SS_total = np.sum( (y_train - np.mean(y_train))**2 ) r2 = 1 - SS_residuals / SS_total print(r2) . 0.12764393851965716 . from sklearn.metrics import r2_score print(&#39;R2 score: {}&#39;.format(r2_score(y_train, y_pred))) . R2 score: 0.12764393851965716 . Using ensemble-based model . from sklearn.ensemble import RandomForestRegressor . model = RandomForestRegressor() model.fit(X_train_std, y_train) . RandomForestRegressor() . y_pred = model.predict(X_train_std) . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(y_train, y_pred, alpha=0.6, label=&#39;Linear Regression&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] # Linear fit reg = np.polyfit(y_train, y_pred, deg=1) ax.plot(lims, reg[0] * np.array(lims) + reg[1], &#39;r--&#39;, linewidth=1.5, label=&#39;Linear Fit&#39;) ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;Parity Line&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlabel(&#39;True value&#39;) ax.set_ylabel(&#39;Model predicted&#39;) ax.legend(loc=&#39;best&#39;); . def display_performance(y_true, y_pred): from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error r2 = r2_score(y_true, y_pred) mae = mean_absolute_error(y_true, y_pred) rmse = np.sqrt(mean_squared_error(y_true, y_pred)) print(&#39;R2: {0:0.2f} n&#39; &#39;MAE: {1:0.2f} n&#39; &#39;RMSE: {2:0.2f}&#39;.format(r2, mae, rmse)) return(r2, mae, rmse) . display_performance(y_train,y_pred); . R2: 0.91 MAE: 0.29 RMSE: 0.38 . Fingerprints . Compress molecules into vectors for mathetical operations and comparisons. First we will look at MorganFingerprint method. For this method we have to define the radius and the size of the vector being used. . More information on different Circular Fingerprints can be read at this blogpost. Highly recommended . Presentation by Gregory Landrum (creator of RDkit) on Fingerprints . | RDkit Blog entry of visualizing the fingerprint bitvectors. Using the new fingerprint bit rendering code . | . from rdkit.Chem import AllChem . radius = 2 # How far from the center node should we look at? ecfp_power = 10 # Size of the fingerprint vectors ECFP = [ np.array(AllChem.GetMorganFingerprintAsBitVect(m, radius, nBits = 2**ecfp_power)) for m in df_train[&#39;ROMol&#39;] ] . len(ECFP) . 3500 . df_train[&#39;ECFP&#39;] = ECFP . &lt;ipython-input-69-027caa4baff4&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df_train[&#39;ECFP&#39;] = ECFP . df_train.sample(2) . SMILES LogP ROMol Mol_Wt Num_Aromatic_rings Num_rotate_bonds Ratio_Aromatic Valence_electrons ECFP . 4645 CC(C)(C)[PH2+][O-] | 0.3 | | 106.105 | 0 | 0 | 0.0 | 38 | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | . 10 CCC[C-](C)Cl | 2.5 | | 105.588 | 0 | 2 | 0.0 | 38 | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | . X_train = df_train.ECFP.values X_train = np.stack(X_train, axis=0) y_train = df_train.LogP.values . std_scaler = StandardScaler() X_train_std = std_scaler.fit_transform(X_train) . model = RandomForestRegressor() model.fit(X_train_std, y_train) . RandomForestRegressor() . y_pred = model.predict(X_train_std) . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(y_train, y_pred, alpha=0.6, label=&#39;Linear Regression&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] # Linear fit reg = np.polyfit(y_train, y_pred, deg=1) ax.plot(lims, reg[0] * np.array(lims) + reg[1], &#39;r--&#39;, linewidth=1.5, label=&#39;Linear Fit&#39;) ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;Parity Line&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlabel(&#39;True value&#39;) ax.set_ylabel(&#39;Model predicted&#39;) ax.legend(loc=&#39;best&#39;); . display_performance(y_train,y_pred); . R2: 0.96 MAE: 0.17 RMSE: 0.24 . Similarity . RDKit provides tools for different kinds of similarity search, including Tanimoto, Dice, Cosine, Sokal, Russel… and more. Tanimoto is a very widely use similarity search metric because it incorporates substructure matching. Here is an example . ref_mol = df_var_5k.iloc[4234][&#39;ROMol&#39;] . ref_mol . ref_ECFP4_fps = AllChem.GetMorganFingerprintAsBitVect(ref_mol, radius=2) . df_var_5k_ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,2) for x in df_var_5k[&#39;ROMol&#39;]] . Estimate the similarity of the molecules in the dataset to the reference dataset, there are multiple ways of doing it: we are using Tanimoto fingerprints . from rdkit import DataStructs similarity_efcp4 = [DataStructs.FingerprintSimilarity(ref_ECFP4_fps, x) for x in df_var_5k_ECFP4_fps] . df_var_5k[&#39;Tanimoto_Similarity (ECFP4)&#39;] = similarity_efcp4 df_var_5k[&#39;Tanimoto_Similarity (ECFP4)&#39;] = df_var_5k[&#39;Tanimoto_Similarity (ECFP4)&#39;].round(3) PandasTools.FrameToGridImage(df_var_5k[:10], legendsCol=&quot;Tanimoto_Similarity (ECFP4)&quot;, molsPerRow=4) . df_var_5k = df_var_5k.sort_values([&#39;Tanimoto_Similarity (ECFP4)&#39;], ascending=False) PandasTools.FrameToGridImage(df_var_5k[:10], legendsCol=&quot;Tanimoto_Similarity (ECFP4)&quot;, molsPerRow=4) .",
            "url": "http://pgg1610.github.io/blog_fastpages/chemical-science/python/data-analysis/2021/05/11/rdkit_basics_first.html",
            "relUrl": "/chemical-science/python/data-analysis/2021/05/11/rdkit_basics_first.html",
            "date": " • May 11, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Cheminformatics Literature and Resources",
            "content": "Last update: 3rd July 2021 . Noteworthy blogs to follow: . Patrick Walters Blog on Cheminformatics Pat Walter’s Cheminformatics Resources list | . | Is Life Worth Living . | Andrew White’s ML for Molecules and Materials Online Book . | Cheminformia . | Depth-First . | DrugDiscovery.NET - Andreas Bender | Online resources . Pen’s Python cookbook for Cheminformatics . | Patrick Walter’s Cheminformatics Hands-on workshop . | Andrea Volkmer, TeachOpenCADD: a teaching platform for computer-aided drug design (CADD) . | Chem LibreText collection from ACS Division of Chemical Education . | . Reviews: . F. Strieth-Kalthoff, F. Sandfort, M. H. S. Segler, and F. Glorius, Machine learning the ropes: principles, applications and directions in synthetic chemistry, Chem. Soc. Rev | . Pedagogical account of various machine learning techniques, models, representation schemes from perspective of synthetic chemistry. Covers different applications of machine learning in synthesis planning, property prediction, molecular design, and reactivity prediction . Mariia Matveieva &amp; Pavel Polishchuk. Benchmarks for interpretation of QSAR models. Github. Patrick Walter’s blog on the paper. | . Paper outlining good practices for interpretating QSAR (Quantative Structure-Property Prediction) models. Good set of heuristics and comparison in the paper in terms of model interpretability. Create 6 synthetic datasets with varying complexity for QSAR tasks. The authors compare interpretability of graph-based methods to conventional QSAR methods. In regards to performance graph-based models show low interpretation compared to conventional QSAR method. . W. Patrick Walters &amp; Regina Barzilay. Applications of Deep Learning in Molecule Generation and Molecular Property Prediction | . Recent review summarising the state of the molecular property prediction and structure generation research. In spite of exciting recent advances in the modeling efforts, there is a need to generate better (realistic) training data, assess model prediction confidence, and metrics to quantify molecular generation performance. . Navigating through the Maze of Homogeneous Catalyst Design with Machine Learning . | Coley, C. W. Defining and Exploring Chemical Spaces. Trends in Chemistry 2020 . | Applications of Deep learning in molecular generation and molecular property prediction . | Utilising Graph Machine Learning within Drug Discovery and Development . | . Industry-focused drug discovery reviews . A. Bender and I. Cortés-Ciriano, “Artificial intelligence in drug discovery: what is realistic, what are illusions? Part 1: Ways to make an impact, and why we are not there yet,” Drug Discov. Today, vol. 26, no. 2, pp. 511–524, 2021 . | A. H. Göller et al., “Bayer’s in silico ADMET platform: a journey of machine learning over the past two decades,” Drug Discov. Today, vol. 25, no. 9, pp. 1702–1709, 2020. . | J. Shen and C. A. Nicolaou, “Molecular property prediction: recent trends in the era of artificial intelligence,” Drug Discov. Today Technol., vol. 32–33, no. xx, pp. 29–36, 2019. . | . Special Journal Issues: . Nice collection of recent papers in Nature Communications on ML application and modeling . | Journal of Medicinal Chemistry compendium of AI in Drug discovery issue . | Account of Chemical Research Special Issue on advances in data-driven chemistry research . | Specific Articles . Few key papers which I have found useful when learning more about the state-of-the-art in Cheminformatics. I’ve tried to categorize them roughly based on their area of application: . Representation: . Representation of Molecules in NN: Molecular representation in AI-driven drug discovery: review and guide . | Screening of energetic molecules – comparing different representations . | M. Krenn, F. Hase, A. Nigam, P. Friederich, and A. Aspuru-Guzik, “Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation,” Mach. Learn. Sci. Technol., pp. 1–9, 2020 . | Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models . | . Comparative study of descriptor-based and graph-based models using public data set. Used descriptor-based models (XGBoost, RF, SVM, using ECFP) and compared them to graph-based models (GCN, GAT, AttentiveFP, MPNN). They show descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency with SVM having best predictions. Graph-based methods are good for multi-task learning. . Matthew Clark, et. al. DNA-encoded small-molecule libraries (DEL). C&amp;EN article on the topic | . New form of storing huge amounts of molecule related data using DNA. Made partially possible by low cost of DNA sequencing. Each molecule in the storage is attached with a DNA strand which encode information about its recipe. . Follow up to the work with Machine Learning for hit finding. (Link) | . DNA encodings for discovery of novel small-molecule protein inhibitors. Outline a process for building a ML model using DEL. Compare graph convolutions to random forest for classification tasks with application to protein target binding. Graph models seemed to achieve high hit rate comapred to random forest. Apply diversity, logistical, structural filtering to search for novel candidates. . Uncertainty quantification: . Alan Aspuru-Guzik perspective on uncertainty and confidence . | Uncertainty Quantification Using Neural Networks for Molecular Property Prediction. J. Chem. Inf. Model. (2020) Hirschfeld, L., Swanson, K., Yang, K., Barzilay, R. &amp; Coley, C. W. . | . Benchmark different models and uncertainty metrics for molecular property prediction. . Evidential Deep learning for guided molecular property prediction and disocovery Ava Soleimany, Conor Coley, et. al.. Slides | . Train network to output the parameters of an evidential distribution. One forward-pass to find the uncertainty as opposed to dropout or ensemble - principled incorporation of uncertainties . Differentiable sampling of molecular geometries with uncertainty-based adversarial attacks . | J. P. Janet, S. Ramesh, C. Duan, H. J. Kulik, ACS Cent. Sci. 2020 . | . Conduct a global multi-objective optimization with expected improvement criterion. Find transition metal complex redox couples for Redox flow batteries that address stability, solubility, and redox potential metric. Use distance of a point from a training data in latent space as a metric to quantify uncertainty. . J. P. Janet, C. Duan, T. Yang, A. Nandy, H. J. Kulik, Chem. Sci. 2019, 10, 7913–7922 | . Distance from available data in NN latent space is used as a variable for low-cost, quantitative uncertainty metric that works for both inorganic and organic chemistry. Introduce a technique to calibrate latent distances enabling conversion of distance-based metric to error estimates in units of predicted property . Active Learning . Active learning provides strategies for efficient screening of subsets of the library. In many cases, we can identify a large portion of the most promising molecules with a fraction of the compute cost. . Reker, D. Practical Considerations for Active Machine Learning in Drug Discovery. Drug Discov. Today Technol. 2020 . | B. J. Shields et al., “Bayesian reaction optimization as a tool for chemical synthesis,” Nature, vol. 590, no. June 2020, p. 89, 2021. Github . | . Experimental design using Bayesian Optimization. . Transfer Learning . Approaching coupled cluster accuracy with a general-purpose neural network potential through transfer learning Transfer learning by training a network to DFT data and then retrain on a dataset of gold standard QM calculations (CCSD(T)/CBS) that optimally spans chemical space. The resulting potential is broadly applicable to materials science, biology, and chemistry, and billions of times faster than CCSD(T)/CBS calculations. . | Improving the generative performance of chemical autoencoders through transfer learning . | . Generative models: . Reviews . B. Sanchez-Lengeling and A. Aspuru-Guzik, “Inverse molecular design using machine learning: Generative models for matter engineering,” Science (80-. )., vol. 361, no. 6400, pp. 360–365, Jul. 2018 | . Benchmarks . MOSES - Benchmarking platform for generative models. | . Propose a platform to deploy and compare state-of-the-art generative models for exploring molecular space on same dataset. In addition the authors also propose list of metrics to evaluate the quality and diversity of the generated structures. . J. Zhang, R. Mercado, O. Engkvist, and H. Chen, “Comparative Study of Deep Generative Models on Chemical Space Coverage,” J. Chem. Inf. Model., vol. 61, no. 6, pp. 2572–2581, Jun. 2021. | . Interesting analysis from team at AstraZeneca R&amp;D. They look at the chemical space coverage accounted by the SOTA generative models. Proposes a metric for evaluating space coverage, and thereby comparing different SOTA models, using a reference data (GDB-13 in this case). The new metric computes how much of the GDB-13 dataset can be recovered by a model that is trained on small GDB subset. Generative models were trained on same 1M data points and 1B molecules were then sampled from each model. It was seen that at most 39% of the molecules in the parent dataset were sampled / generated by the model. Most models sampled the same compounds atleast twice. It was observed that graph-based model sampled much diverse molecules than string-based methods. Besides, the coverage of GAN-based models was worse compared to Language and Graph models. . Gao, W.; Coley, C. W. The Synthesizability of Molecules Proposed by Generative Models. J. Chem. Inf. Model. 2020 | . This paper looks at different ways of integrating synthesizability criteria into generative models. . Language models: . R. Gómez-Bombarelli et al., “Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules,” ACS Cent. Sci., vol. 4, no. 2, pp. 268–276, 2018 | . One of the first implementation of a variation auto-encoder for molecule generation . Penalized Variational Autoencoder . | SELFIES and generative models using STONED . | . Representation using SELFIES proposed to make it much more powerful . LSTM based (RNN) approaches to small molecule generation. Github . | Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. arXiv [cs.LG], 2020. . | SMILES-based deep generative scaffold decorator for de-novo drug design. Github . | . SMILES-based language model that generates molecules from scaffolds and can be trained from any arbitrary molecular set. Uses randomized SMILES to improve final prediction validity. . Graph-based . W. Jin, R. Barzilay, and T. Jaakkola, “Junction tree variational autoencoder for molecular graph generation,” 35th Int. Conf. Mach. Learn. ICML 2018, vol. 5, pp. 3632–3648, 2018 | . Junction tree based decoding. Define a grammar for the small molecule and find sub-units based on that grammar to construct a molecule. The molecule is generated in two-steps: first being generating the scaffold or backbone of the molelcule, then the nodes are added with molecular substructure as identified from the ‘molecular grammar’. . MPGVAE: Message passing graph networks for molecular generation, Daniel Flam-Shepherd et al 2021 Mach. Learn.: Sci. Technol. | . Introduce a graph generation model by building a Message Passing Neural Network (MPNNs) into the encoder and decoder of a VAE (MPGVAE). . ConfVAE: End-to-end framework for molecular conformation generation via bilevel programming | . Algorithm to predict 3D conforms from molecular graphs. . GraphINVENT: R. Mercado, T. Rastemo, E. Lindelöf, G. Klambauer and O. Engkvist, “Graph networks for molecular design,” Mach. Learn. Sci. Technol., vol. 2, no. 2, p. 25023, 2021. Github. Blogpost | . GraphINVENT uses a tiered deep neural network architecture to probabilistically generate new molecules a single bond at a time . GANs . MolGAN: An implicit generative model for small molecular graphs, N. De Cao and T. Kipf, 2018 | . Generative adversarial network for finding small molecules using graph networks, quite interesting. Avoids issues arising from node ordering that are associated with likelihood based methods by using an adversarial loss instead (GAN) . LatentGAN: A de novo molecular generation method using latent vector based generative adversarial network | . Molecular generation strategy is described which combines an autoencoder and a GAN. Generator and discriminator network do not use SMILES strings as input, but instead n-dimensional vectors derived from the code-layer of an autoencoder trained as a SMILES heteroencoder that way syntax issues are expected to be addressed. . Reaction Network Predictions: . Recent review on the matter from Reiher group: . The Exploration of Chemical Reaction Networks | . Perspective article summarising their position on the current state of research and future considerations on developing better reaction network models. Break down the analysis of reaction networks as into 3 classes (1) Front Open End: exploration of products from reactants (2) Backward Open Start: Know the product and explore potential reactants (3) Start to End: Product and reactant known, explore the likely intermediates. . Nice summary of potential challenges in the field: . Validating exploration algorithms on a consistent set of reaction system. Need to generate a comparative metric to benchmark different algorithms. | Considering effect of solvents and/or protein embeddings in the analysis . | Previous review article by same group: Exploration of Reaction Pathways and Chemical Transformation Networks | . Technical details of various algorithms being implemented for reaction mechanism discovery at the time of writing the review. . Articles: . M. Liu et al., “Reaction Mechanism Generator v3.0: Advances in Automatic Mechanism Generation,” J. Chem. Inf. Model., May 2021 | . Newest version of RMG (v3) is updated to Python v3. It has ability to generate heterogeneous catalyst models, uncertainty analysis to conduct first order sensitivity analysis. RMG dataset for the thermochemical and kinetic parameters have been expanded. . More and Faster: Simultaneously Improving Reaction Coverage and Computational Cost in Automated Reaction Prediction Tasks | . Presents an algorithmic improvement to the reaction network prediction task through their YARP (Yet Another Reaction Program) methodology. Shown to reduce computational cost of optimization while improving the diversity of identified products and reaction pathways. . Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction Follow-up: Quantitative interpretation explains machine learning models for chemical reaction prediction and uncovers bias | . | Automatic discovery of chemical reactions using imposed activation . | Machine learning in chemical reaction space | . Look at exploration of reaction space rather than compound space. SOAP kernel for representing the moelcules. Estimate atomization energy for the molecules using ML. Calculate the d(AE) for different ML-estimated AEs. Reaction energies (RE) are estimated and uncertainty propogation is used to estimate the errors. Uncorrelated constant error propogation. 30,000 bond breaking reaction steps Rad-6-RE network used. RE prediction is not as good as AE. . C. W. Coley et al., “A graph-convolutional neural network model for the prediction of chemical reactivity,” Chem. Sci., vol. 10, no. 2, pp. 370–377, 2019. . | Prediction of Organic Reaction Outcomes Using Machine Learning, ACS Cent. Sci. 2017 . | . Code / Packages: . Molecular AI department at AstraZeneca R&amp;D . | GHOST: Generalized threshold shifting procedure. Paper . | . Automates the selection of decision threshold for imbalanced classification task. The assumption for this method to work is the similar characteristics (like imbalance ratio) of training and test data. . MOSES - Benchmarking platform for generative models (PyTorch Implementation). Github | . Benchmarking platform to implement molecular generative models. It also provides a set of metrics to evaluate the quality and diversity of the generated molecules. A benchmark dataset (subset of ZINC) is provided for training the models. . Reinvent 2.0 - an AI tool forr de novo drug design. Github | . Production-ready tool for de novo design from Astra Zeneca. It can be effectively applied on drug discovery projects that are striving to resolve either exploration or exploitation problems while navigating the chemical space. Language model with SMILE output and trained by “randomizing” the SMILES representation of the input data. Implement reinforcement-leraning for directing the model towards relevant area of interest. . Schnet by Jacobsen et. al. (Neural message passing). Github. Tutorial . | OpenChem. Github . | DeepChem. Website . | . DeepChem aims to provide a high quality open-source toolchain that democratizes the use of deep-learning in drug discovery, materials science, quantum chemistry, and biology - from Github . Chainer-Chemistry | . “Chainer Chemistry is a deep learning framework (based on Chainer) with applications in Biology and Chemistry. It supports various state-of-the-art models (especially GCNN - Graph Convolutional Neural Network) for chemical property prediction” - from their Github repo introduction . FastJTNN - python 3 version of the JT-NN . | DimeNet++ - extension of Directional message pasing working (DimeNet). Github . | [BondNet - Graph neural network model for predicting bond dissociation energies, considers both homolytic and heterolytic bond breaking]. Github . | PhysNet . | RNN based encoder software . | AutodE . | DScribe . | RMG - Reaction Mechanism Generator . | . Tool to generate chemical reaction networks. Includes Arkane, package for calculating thermodynamics from quantum mechanical calculations. . Helpful utilities: . RD-Kit Get Atom Indices in the SMILE: | Datamol for manipulating RDKit molecules | . | Papers with code benchmark for QM9 energy predictions . | Molecular generation models benchmark | . Molecules datasets: . GDB Dataset . | Quantum Machine: Website listing useful datasets including QM9s and MD trajectory . | Github repository listing databases for Drug Discovery . | .",
            "url": "http://pgg1610.github.io/blog_fastpages/chemical-science/machine-learning/resources/2021/05/08/Cheminformatics_Resources.html",
            "relUrl": "/chemical-science/machine-learning/resources/2021/05/08/Cheminformatics_Resources.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Mateiral informatics sample project",
            "content": "A random forest regression model is built to predict the heat capacity ($C_p$) of solid inorganic materials at different temperatures. The dataset is collected from the NIST JANAF Thermochemical Table . This project is adapted from recent publication looking at best practices for setting up mateial informatics task. . A. Y. T. Wang et al., “Machine Learning for Materials Scientists: An Introductory Guide toward Best Practices,” Chem. Mater., vol. 32, no. 12, pp. 4954–4965, 2020. | . import os import pandas as pd import numpy as np np.random.seed(42) . import matplotlib.pyplot as plt from matplotlib.pyplot import cm # High DPI rendering for mac %config InlineBackend.figure_format = &#39;retina&#39; # Plot matplotlib plots with white background: %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 15, &#39;axes.titlesize&#39; : 15, &#39;axes.labelsize&#39; : 15, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 12, &#39;ytick.labelsize&#39; : 12, } plt.rcParams.update(plot_params) . Loading and cleaning the data . root_dir = os.getcwd() csv_file_path = os.path.join(root_dir, &#39;material_cp.csv&#39;) df = pd.read_csv(csv_file_path) . df.sample(5) . FORMULA CONDITION: Temperature (K) PROPERTY: Heat Capacity (J/mol K) . 1207 F2Hg1 | 1000.0 | 89.538 | . 3910 Na2O2 | 400.0 | 97.721 | . 1183 Fe0.877S1 | 298.0 | 49.883 | . 100 B2Mg1 | 298.0 | 47.823 | . 3879 N5P3 | 1000.0 | 265.266 | . print(df.shape) . (4583, 3) . df.describe().round(2) . CONDITION: Temperature (K) PROPERTY: Heat Capacity (J/mol K) . count 4579.00 | 4576.00 | . mean 1170.92 | 107.48 | . std 741.25 | 67.02 | . min -2000.00 | -102.22 | . 25% 600.00 | 61.31 | . 50% 1000.00 | 89.50 | . 75% 1600.00 | 135.65 | . max 4700.00 | 494.97 | . Rename columns for better data handling . rename_dict = {&#39;FORMULA&#39;:&#39;formula&#39;, &#39;CONDITION: Temperature (K)&#39;:&#39;T&#39;, &#39;PROPERTY: Heat Capacity (J/mol K)&#39;:&#39;Cp&#39;} df = df.rename(columns=rename_dict) . df . formula T Cp . 0 B2O3 | 1400.0 | 134.306 | . 1 B2O3 | 1300.0 | 131.294 | . 2 B2O3 | 1200.0 | 128.072 | . 3 B2O3 | 1100.0 | 124.516 | . 4 B2O3 | 1000.0 | 120.625 | . ... ... | ... | ... | . 4578 Zr1 | 450.0 | 26.246 | . 4579 Zr1 | 400.0 | 25.935 | . 4580 Zr1 | 350.0 | 25.606 | . 4581 Zr1 | 300.0 | NaN | . 4582 Zr1 | 298.0 | 25.202 | . 4583 rows × 3 columns . Check for null entries in the dataset . columns_has_NaN = df.columns[df.isnull().any()] df[columns_has_NaN].isnull().sum() . formula 4 T 4 Cp 7 dtype: int64 . Show the null entries in the dataframe . is_NaN = df.isnull() row_has_NaN = is_NaN.any(axis=1) df[row_has_NaN] . formula T Cp . 22 Be1I2 | 700.0 | NaN | . 1218 NaN | 1300.0 | 125.353 | . 1270 NaN | 400.0 | 79.036 | . 2085 C1N1Na1 | 400.0 | NaN | . 2107 Ca1S1 | 1900.0 | NaN | . 3278 NaN | NaN | 108.787 | . 3632 H2O2Sr1 | NaN | NaN | . 3936 NaN | 2000.0 | 183.678 | . 3948 Nb2O5 | 900.0 | NaN | . 3951 Nb2O5 | 600.0 | NaN | . 3974 Ni1 | NaN | 30.794 | . 4264 O3V2 | NaN | 179.655 | . 4581 Zr1 | 300.0 | NaN | . df_remove_NaN = df.dropna(subset=[&#39;formula&#39;,&#39;Cp&#39;,&#39;T&#39;]) . df_remove_NaN.isnull().sum() . formula 0 T 0 Cp 0 dtype: int64 . Remove unrealistic values from the dataset . df_remove_NaN.describe() . T Cp . count 4570.000000 | 4570.000000 | . mean 1171.366355 | 107.469972 | . std 741.422702 | 67.033623 | . min -2000.000000 | -102.215000 | . 25% 600.000000 | 61.301500 | . 50% 1000.000000 | 89.447500 | . 75% 1600.000000 | 135.624250 | . max 4700.000000 | 494.967000 | . T_filter = (df_remove_NaN[&#39;T&#39;] &lt; 0) Cp_filter = (df_remove_NaN[&#39;Cp&#39;] &lt; 0) df_remove_NaN_neg_values = df_remove_NaN.loc[(~T_filter) &amp; (~Cp_filter)] print(df_remove_NaN_neg_values.shape) . (4564, 3) . Splitting data . The dataset in this exercise contains different formulae, Cp and T for that entry as a function of T. There are lot of repeated formulae and there is a chance that randomly splitting the dataset in train/val/test would lead to leaks of material entries between 3 sets. . To avoid this the idea is to generate train/val/test such that all material entries belonging a particular type are included in only that set. Eg: B2O3 entries are only in either train/val/test set. To do so let&#39;s first find the unique material entries in the set and sample those without replacement when making the new train/val/test set . df = df_remove_NaN_neg_values.copy() . df . formula T Cp . 0 B2O3 | 1400.0 | 134.306 | . 1 B2O3 | 1300.0 | 131.294 | . 2 B2O3 | 1200.0 | 128.072 | . 3 B2O3 | 1100.0 | 124.516 | . 4 B2O3 | 1000.0 | 120.625 | . ... ... | ... | ... | . 4577 Zr1 | 500.0 | 26.564 | . 4578 Zr1 | 450.0 | 26.246 | . 4579 Zr1 | 400.0 | 25.935 | . 4580 Zr1 | 350.0 | 25.606 | . 4582 Zr1 | 298.0 | 25.202 | . 4564 rows × 3 columns . from sklearn.model_selection import train_test_split train_df, test_df = train_test_split(df, test_size=0.4, random_state=42) . There are going to be couple of materials which are going to be present in training and test both . train_set = set(train_df[&#39;formula&#39;].unique()) test_set = set(test_df[&#39;formula&#39;].unique()) # Check for intersection with val and test len(train_set.intersection(test_set)) . 243 . Start with unique splitting task . len(df[&#39;formula&#39;].unique()) . 244 . Out of 244 unique materials entries, 233 are present in both training and test. This is problematic for model building especially since we&#39;re going to featurize the materials using solely the composition-based features. . f_entries = df[&#39;formula&#39;].value_counts()[:50] fig, ax = plt.subplots(1,1, figsize=(5,20)) ax.barh(f_entries.index, f_entries.values) ax.tick_params(axis=&#39;x&#39;, rotation=90); . df[&#39;formula&#39;].unique()[:10] . array([&#39;B2O3&#39;, &#39;Be1I2&#39;, &#39;Be1F3Li1&#39;, &#39;Al1Cl4K1&#39;, &#39;Al2Be1O4&#39;, &#39;B2H4O4&#39;, &#39;B2Mg1&#39;, &#39;Be1F2&#39;, &#39;B1H4Na1&#39;, &#39;Br2Ca1&#39;], dtype=object) . Creating train/val/test manually . unique_entries = df[&#39;formula&#39;].unique() . train_set = 0.7 val_set = 0.2 test_set = 1 - train_set - val_set . num_entries_train = int( train_set * len(unique_entries) ) num_entries_val = int( val_set * len(unique_entries) ) num_entries_test = int( test_set * len(unique_entries) ) . print(num_entries_train, num_entries_val, num_entries_test) . 170 48 24 . train_formulae = np.random.choice(unique_entries, num_entries_train, replace=False) unique_entries_minus_train = [i for i in unique_entries if i not in train_formulae] # Val formula val_formulae = np.random.choice(unique_entries_minus_train, num_entries_val, replace=False) unique_entries_minus_train_val = [i for i in unique_entries_minus_train if i not in val_formulae] # Test formula test_formulae = unique_entries_minus_train_val.copy() . print(len(train_formulae), len(val_formulae), len(test_formulae)) . 170 48 26 . train_points = df.loc[ df[&#39;formula&#39;].isin(train_formulae) ] val_points = df.loc[ df[&#39;formula&#39;].isin(val_formulae) ] test_points = df.loc[ df[&#39;formula&#39;].isin(test_formulae) ] . print(train_points.shape, val_points.shape, test_points.shape) . (3131, 3) (944, 3) (489, 3) . train_set = set(train_points[&#39;formula&#39;].unique()) val_set = set(val_points[&#39;formula&#39;].unique()) test_set = set(test_points[&#39;formula&#39;].unique()) # Check for intersection with val and test print(len(train_set.intersection(val_set)), len(train_set.intersection(test_set))) . 0 0 . Model fitting . Featurization . Composition-based feature vector (CBFV) is used to describe each mateiral entry (eg: Cr2O3) with set of elemental and composition based numbers. . from cbfv.composition import generate_features . rename_columns = {&#39;Cp&#39;:&#39;target&#39;} train_points[&#39;Type&#39;] = &#39;Train&#39; val_points[&#39;Type&#39;] = &#39;Val&#39; test_points[&#39;Type&#39;] = &#39;Test&#39; total_data = pd.concat([train_points, val_points, test_points], ignore_index=True); total_data = total_data.rename(columns=rename_columns) . /Users/pghaneka/miniconda3/envs/torch_38/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /Users/pghaneka/miniconda3/envs/torch_38/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until /Users/pghaneka/miniconda3/envs/torch_38/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. . total_data.sample(5) . formula T target Type . 3833 I1K1 | 1400.0 | 74.601 | Val | . 4215 Cr2O3 | 298.0 | 120.366 | Test | . 1290 I2Mo1 | 1000.0 | 91.458 | Train | . 1578 I2Zr1 | 700.0 | 97.445 | Train | . 2379 Na2O5Si2 | 1700.0 | 292.880 | Train | . train_df = total_data.loc[ total_data[&#39;Type&#39;] == &#39;Train&#39; ].drop(columns=[&#39;Type&#39;]).reset_index(drop=True) val_df = total_data.loc[ total_data[&#39;Type&#39;] == &#39;Val&#39; ].drop(columns=[&#39;Type&#39;]).reset_index(drop=True) test_df = total_data.loc[ total_data[&#39;Type&#39;] == &#39;Test&#39; ].drop(columns=[&#39;Type&#39;]).reset_index(drop=True) . Sub-sampling . Only some points from the original training data train_df are used to ensure the analysis is tractable . train_df.shape . (3131, 3) . train_df = train_df.sample(n=1000, random_state=42) train_df.shape . (1000, 3) . X_unscaled_train, y_train, formulae_entry_train, skipped_entry = generate_features(train_df, elem_prop=&#39;oliynyk&#39;, drop_duplicates=False, extend_features=True, sum_feat=True) X_unscaled_val, y_val, formulae_entry_val, skipped_entry = generate_features(val_df, elem_prop=&#39;oliynyk&#39;, drop_duplicates=False, extend_features=True, sum_feat=True) X_unscaled_test, y_test, formulae_entry_test, skipped_entry = generate_features(test_df, elem_prop=&#39;oliynyk&#39;, drop_duplicates=False, extend_features=True, sum_feat=True) . Processing Input Data: 100%|██████████| 1000/1000 [00:00&lt;00:00, 26074.09it/s] Assigning Features...: 0%| | 0/1000 [00:00&lt;?, ?it/s] . Featurizing Compositions... . Assigning Features...: 100%|██████████| 1000/1000 [00:00&lt;00:00, 13526.13it/s] . Creating Pandas Objects... . Processing Input Data: 100%|██████████| 944/944 [00:00&lt;00:00, 28169.72it/s] Assigning Features...: 0%| | 0/944 [00:00&lt;?, ?it/s] . Featurizing Compositions... . Assigning Features...: 100%|██████████| 944/944 [00:00&lt;00:00, 14855.23it/s] . Creating Pandas Objects... . Processing Input Data: 100%|██████████| 489/489 [00:00&lt;00:00, 25491.43it/s] Assigning Features...: 100%|██████████| 489/489 [00:00&lt;00:00, 12626.83it/s] . Featurizing Compositions... Creating Pandas Objects... . X_unscaled_train.head(5) . sum_Atomic_Number sum_Atomic_Weight sum_Period sum_group sum_families sum_Metal sum_Nonmetal sum_Metalliod sum_Mendeleev_Number sum_l_quantum_number ... range_Melting_point_(K) range_Boiling_Point_(K) range_Density_(g/mL) range_specific_heat_(J/g_K)_ range_heat_of_fusion_(kJ/mol)_ range_heat_of_vaporization_(kJ/mol)_ range_thermal_conductivity_(W/(m_K))_ range_heat_atomization(kJ/mol) range_Cohesive_energy T . 0 64.0 | 139.938350 | 10.5 | 50.0 | 23.25 | 1.0 | 2.75 | 0.0 | 289.25 | 4.75 | ... | 2009873.29 | 5.748006e+06 | 26.002708 | 0.112225 | 252.450947 | 88384.346755 | 4759.155119 | 41820.25 | 4.410000 | 1100.0 | . 1 58.0 | 119.979000 | 10.0 | 40.0 | 18.00 | 1.0 | 2.00 | 0.0 | 231.00 | 4.00 | ... | 505663.21 | 1.328602e+06 | 8.381025 | 0.018225 | 36.496702 | 28866.010000 | 1597.241190 | 4830.25 | 0.511225 | 1100.0 | . 2 27.0 | 58.691000 | 6.0 | 17.0 | 10.00 | 1.0 | 0.00 | 1.0 | 115.00 | 3.00 | ... | 43890.25 | 1.277526e+05 | 1.210000 | 0.062500 | 301.890625 | 1179.922500 | 6.502500 | 2652.25 | 0.230400 | 3400.0 | . 3 36.0 | 72.144000 | 7.0 | 18.0 | 9.00 | 1.0 | 1.00 | 0.0 | 95.00 | 1.00 | ... | 131841.61 | 2.700361e+05 | 0.067600 | 0.001600 | 11.636627 | 5148.062500 | 9973.118090 | 2550.25 | 0.255025 | 2900.0 | . 4 80.0 | 162.954986 | 19.0 | 120.0 | 56.00 | 0.0 | 8.00 | 0.0 | 659.00 | 8.00 | ... | 16129.00 | 5.659641e+04 | 0.826963 | 0.018225 | 0.021993 | 21.791158 | 0.010922 | 6241.00 | 0.555025 | 1300.0 | . 5 rows × 177 columns . formulae_entry_train.head(5) . 0 Mo1O2.750 1 Fe1S2 2 B1Ti1 3 Ca1S1 4 N5P3 Name: formula, dtype: object . X_unscaled_train.shape . (1000, 177) . Feature scaling . X_unscaled_train.columns . Index([&#39;sum_Atomic_Number&#39;, &#39;sum_Atomic_Weight&#39;, &#39;sum_Period&#39;, &#39;sum_group&#39;, &#39;sum_families&#39;, &#39;sum_Metal&#39;, &#39;sum_Nonmetal&#39;, &#39;sum_Metalliod&#39;, &#39;sum_Mendeleev_Number&#39;, &#39;sum_l_quantum_number&#39;, ... &#39;range_Melting_point_(K)&#39;, &#39;range_Boiling_Point_(K)&#39;, &#39;range_Density_(g/mL)&#39;, &#39;range_specific_heat_(J/g_K)_&#39;, &#39;range_heat_of_fusion_(kJ/mol)_&#39;, &#39;range_heat_of_vaporization_(kJ/mol)_&#39;, &#39;range_thermal_conductivity_(W/(m_K))_&#39;, &#39;range_heat_atomization(kJ/mol)&#39;, &#39;range_Cohesive_energy&#39;, &#39;T&#39;], dtype=&#39;object&#39;, length=177) . X_unscaled_train.describe().round(2) . sum_Atomic_Number sum_Atomic_Weight sum_Period sum_group sum_families sum_Metal sum_Nonmetal sum_Metalliod sum_Mendeleev_Number sum_l_quantum_number ... range_Melting_point_(K) range_Boiling_Point_(K) range_Density_(g/mL) range_specific_heat_(J/g_K)_ range_heat_of_fusion_(kJ/mol)_ range_heat_of_vaporization_(kJ/mol)_ range_thermal_conductivity_(W/(m_K))_ range_heat_atomization(kJ/mol) range_Cohesive_energy T . count 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | ... | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | . mean 66.57 | 147.21 | 11.28 | 46.27 | 23.19 | 1.28 | 2.64 | 0.08 | 292.01 | 3.73 | ... | 579042.62 | 1803422.99 | 8.45 | 3.34 | 181.31 | 28201.58 | 3305.55 | 14959.38 | 1.70 | 1195.38 | . std 48.94 | 116.53 | 6.33 | 36.29 | 16.68 | 0.76 | 2.32 | 0.31 | 210.15 | 2.59 | ... | 750702.41 | 2017584.79 | 17.52 | 10.61 | 413.13 | 36421.94 | 4474.33 | 22191.74 | 2.45 | 760.90 | . min 4.00 | 7.95 | 2.00 | 1.00 | 1.00 | 0.00 | 0.00 | 0.00 | 3.00 | 0.00 | ... | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25% 31.00 | 65.12 | 6.00 | 18.00 | 10.00 | 1.00 | 1.00 | 0.00 | 126.00 | 2.00 | ... | 20619.03 | 199191.78 | 0.23 | 0.01 | 1.88 | 1451.91 | 119.61 | 729.00 | 0.09 | 600.00 | . 50% 55.00 | 118.00 | 10.00 | 36.00 | 20.00 | 1.00 | 2.00 | 0.00 | 247.00 | 3.00 | ... | 222030.88 | 1169819.78 | 1.25 | 0.05 | 20.92 | 18260.29 | 1607.65 | 5312.67 | 0.63 | 1054.00 | . 75% 86.00 | 182.15 | 15.00 | 72.00 | 36.00 | 2.00 | 4.00 | 0.00 | 442.00 | 5.00 | ... | 882096.64 | 3010225.00 | 9.33 | 0.12 | 171.31 | 40317.25 | 4968.28 | 18080.67 | 2.08 | 1600.00 | . max 278.00 | 685.60 | 41.00 | 256.00 | 113.00 | 4.00 | 15.00 | 2.00 | 1418.00 | 19.00 | ... | 3291321.64 | 8535162.25 | 93.11 | 44.12 | 2391.45 | 168342.03 | 40198.47 | 95733.56 | 10.59 | 4600.00 | . 8 rows × 177 columns . X_unscaled_train[&#39;range_heat_of_vaporization_(kJ/mol)_&#39;].hist(); . from sklearn.preprocessing import StandardScaler, normalize . stdscaler = StandardScaler() X_train = stdscaler.fit_transform(X_unscaled_train) X_val = stdscaler.transform(X_unscaled_val) X_test = stdscaler.transform(X_unscaled_test) . pd.DataFrame(X_train, columns=X_unscaled_train.columns).describe().round(2) . sum_Atomic_Number sum_Atomic_Weight sum_Period sum_group sum_families sum_Metal sum_Nonmetal sum_Metalliod sum_Mendeleev_Number sum_l_quantum_number ... range_Melting_point_(K) range_Boiling_Point_(K) range_Density_(g/mL) range_specific_heat_(J/g_K)_ range_heat_of_fusion_(kJ/mol)_ range_heat_of_vaporization_(kJ/mol)_ range_thermal_conductivity_(W/(m_K))_ range_heat_atomization(kJ/mol) range_Cohesive_energy T . count 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | ... | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | . mean -0.00 | -0.00 | -0.00 | 0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | ... | 0.00 | 0.00 | -0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.00 | . std 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | ... | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | . min -1.28 | -1.20 | -1.47 | -1.25 | -1.33 | -1.68 | -1.14 | -0.27 | -1.38 | -1.44 | ... | -0.77 | -0.89 | -0.48 | -0.31 | -0.44 | -0.77 | -0.74 | -0.67 | -0.69 | -1.57 | . 25% -0.73 | -0.70 | -0.83 | -0.78 | -0.79 | -0.36 | -0.71 | -0.27 | -0.79 | -0.67 | ... | -0.74 | -0.80 | -0.47 | -0.31 | -0.43 | -0.73 | -0.71 | -0.64 | -0.66 | -0.78 | . 50% -0.24 | -0.25 | -0.20 | -0.28 | -0.19 | -0.36 | -0.27 | -0.27 | -0.21 | -0.28 | ... | -0.48 | -0.31 | -0.41 | -0.31 | -0.39 | -0.27 | -0.38 | -0.43 | -0.43 | -0.19 | . 75% 0.40 | 0.30 | 0.59 | 0.71 | 0.77 | 0.96 | 0.59 | -0.27 | 0.71 | 0.49 | ... | 0.40 | 0.60 | 0.05 | -0.30 | -0.02 | 0.33 | 0.37 | 0.14 | 0.15 | 0.53 | . max 4.32 | 4.62 | 4.69 | 5.78 | 5.39 | 3.59 | 5.34 | 6.18 | 5.36 | 5.89 | ... | 3.61 | 3.34 | 4.83 | 3.84 | 5.35 | 3.85 | 8.25 | 3.64 | 3.62 | 4.48 | . 8 rows × 177 columns . pd.DataFrame(X_train, columns=X_unscaled_train.columns)[&#39;range_heat_of_vaporization_(kJ/mol)_&#39;].hist() . &lt;AxesSubplot:&gt; . X_train = normalize(X_train) X_val = normalize(X_val) X_test = normalize(X_test) . pd.DataFrame(X_train, columns=X_unscaled_train.columns).describe().round(2) . sum_Atomic_Number sum_Atomic_Weight sum_Period sum_group sum_families sum_Metal sum_Nonmetal sum_Metalliod sum_Mendeleev_Number sum_l_quantum_number ... range_Melting_point_(K) range_Boiling_Point_(K) range_Density_(g/mL) range_specific_heat_(J/g_K)_ range_heat_of_fusion_(kJ/mol)_ range_heat_of_vaporization_(kJ/mol)_ range_thermal_conductivity_(W/(m_K))_ range_heat_atomization(kJ/mol) range_Cohesive_energy T . count 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | ... | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | 1000.00 | . mean -0.01 | -0.01 | -0.00 | -0.00 | -0.00 | 0.00 | -0.00 | -0.01 | -0.00 | 0.00 | ... | 0.00 | 0.01 | -0.00 | -0.01 | -0.00 | 0.01 | 0.00 | -0.00 | -0.00 | 0.00 | . std 0.07 | 0.07 | 0.07 | 0.07 | 0.07 | 0.08 | 0.07 | 0.07 | 0.07 | 0.08 | ... | 0.08 | 0.08 | 0.07 | 0.07 | 0.07 | 0.08 | 0.09 | 0.08 | 0.08 | 0.09 | . min -0.12 | -0.11 | -0.13 | -0.11 | -0.11 | -0.12 | -0.11 | -0.04 | -0.12 | -0.13 | ... | -0.08 | -0.10 | -0.06 | -0.05 | -0.05 | -0.08 | -0.12 | -0.09 | -0.09 | -0.19 | . 25% -0.06 | -0.06 | -0.06 | -0.06 | -0.06 | -0.04 | -0.06 | -0.03 | -0.06 | -0.05 | ... | -0.05 | -0.05 | -0.03 | -0.03 | -0.04 | -0.05 | -0.05 | -0.05 | -0.05 | -0.06 | . 50% -0.02 | -0.03 | -0.02 | -0.02 | -0.02 | -0.03 | -0.02 | -0.02 | -0.02 | -0.02 | ... | -0.03 | -0.03 | -0.03 | -0.02 | -0.03 | -0.02 | -0.03 | -0.04 | -0.04 | -0.02 | . 75% 0.03 | 0.02 | 0.06 | 0.06 | 0.06 | 0.06 | 0.05 | -0.02 | 0.07 | 0.05 | ... | 0.04 | 0.04 | 0.01 | -0.02 | -0.00 | 0.02 | 0.03 | 0.02 | 0.01 | 0.05 | . max 0.25 | 0.26 | 0.19 | 0.20 | 0.19 | 0.25 | 0.20 | 0.40 | 0.19 | 0.24 | ... | 0.22 | 0.23 | 0.29 | 0.32 | 0.43 | 0.26 | 0.58 | 0.26 | 0.24 | 0.38 | . 8 rows × 177 columns . pd.DataFrame(X_train, columns=X_unscaled_train.columns)[&#39;range_heat_of_vaporization_(kJ/mol)_&#39;].hist() . &lt;AxesSubplot:&gt; . Model fitting . from time import time from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error . model = RandomForestRegressor(random_state=42) . %%time model.fit(X_train, y_train) . CPU times: user 5.51 s, sys: 31.7 ms, total: 5.54 s Wall time: 5.57 s . RandomForestRegressor(random_state=42) . def display_performance(y_true, y_pred): r2 = r2_score(y_true, y_pred) mae = mean_absolute_error(y_true, y_pred) rmse = np.sqrt(mean_squared_error(y_true, y_pred)) print(&#39;R2: {0:0.2f} n&#39; &#39;MAE: {1:0.2f} n&#39; &#39;RMSE: {2:0.2f}&#39;.format(r2, mae, rmse)) return(r2, mae, rmse) . y_pred = model.predict(X_val) display_performance(y_val,y_pred); . R2: 0.81 MAE: 14.03 RMSE: 20.48 . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(y_val, y_pred, alpha=0.6, label=&#39;Random Forest&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] # Linear fit reg = np.polyfit(y_val, y_pred, deg=1) ax.plot(lims, reg[0] * np.array(lims) + reg[1], &#39;r--&#39;, linewidth=1.5, label=&#39;Linear Fit&#39;) ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;Parity Line&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlabel(&#39;True value&#39;) ax.set_ylabel(&#39;Model predicted&#39;) ax.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x7fb9372fb610&gt; . Feature Importance . feature_name = [i for i in X_unscaled_train.columns] . len(feature_name) . 177 . X_train.shape . (1000, 177) . len(model.estimators_) . 100 . mean_feature_importance = model.feature_importances_ std_feature_importance = np.std([ tree.feature_importances_ for tree in model.estimators_ ], axis=0) . feat_imp_df = pd.DataFrame({&#39;name&#39;:feature_name, &#39;mean_imp&#39;:mean_feature_importance, &#39;std_dev&#39;:std_feature_importance}) . feat_imp_df_top = feat_imp_df.sort_values(&#39;mean_imp&#39;, ascending=False)[:20] . feat_imp_df_top[:5] . name mean_imp std_dev . 24 sum_valence_s | 0.383415 | 0.273328 | . 12 sum_Covalent_Radius | 0.205463 | 0.220244 | . 17 sum_MB_electonegativity | 0.098704 | 0.212963 | . 31 sum_Number_of_unfilled_f_valence_electrons | 0.076559 | 0.028590 | . 176 T | 0.049666 | 0.008509 | . fig, ax = plt.subplots(1,1, figsize=(30,3)) ax.bar(feat_imp_df_top[&#39;name&#39;], feat_imp_df_top[&#39;mean_imp&#39;], yerr=feat_imp_df_top[&#39;std_dev&#39;]) ax.tick_params(axis=&#39;x&#39;, rotation=90) ax.set_title(&#39;Feature Importance&#39;); . top_feature_list = feat_imp_df.loc[ feat_imp_df[&#39;mean_imp&#39;] &gt; 0.001 ][&#39;name&#39;] . len(top_feature_list) . 40 . X_train_df = pd.DataFrame(X_train, columns=feature_name) X_val_df = pd.DataFrame(X_val, columns=feature_name) X_train_short = X_train_df[list(top_feature_list)] X_val_short = X_val_df[list(top_feature_list)] . print(X_train_short.shape, X_train.shape) . (1000, 40) (1000, 177) . Refit a new model on small feature set . model_small = RandomForestRegressor(random_state=42) . %%time model_small.fit(X_train_short, y_train) . CPU times: user 1.41 s, sys: 13.9 ms, total: 1.43 s Wall time: 1.44 s . RandomForestRegressor(random_state=42) . y_pred = model_small.predict(X_val_short) display_performance(y_val, y_pred); . R2: 0.81 MAE: 13.87 RMSE: 20.40 . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(y_val, y_pred, alpha=0.6, label=&#39;Random Forest&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] # Linear fit reg = np.polyfit(y_val, y_pred, deg=1) ax.plot(lims, reg[0] * np.array(lims) + reg[1], &#39;r--&#39;, linewidth=1.5, label=&#39;Linear Fit&#39;) ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;Parity Line&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlabel(&#39;True value&#39;) ax.set_ylabel(&#39;Model predicted&#39;) ax.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x7fb934fa1310&gt; . Cross-validation . Combine train and validation set to generate one train - test set for cross-validation . X_y_train = np.c_[X_train_short, y_train] X_y_train.shape . (1000, 41) . np.unique(X_y_train[:,-1] - y_train) . array([0.]) . X_y_val = np.c_[X_val_short, y_val] . X_Y_TRAIN = np.vstack((X_y_train, X_y_val)) . X_TRAIN = X_Y_TRAIN[:,0:-1].copy() Y_TRAIN = X_Y_TRAIN[:,-1].copy() print(X_TRAIN.shape, Y_TRAIN.shape) . (1944, 40) (1944,) . from sklearn.model_selection import cross_validate def display_score(scores, metric): score_key = &#39;test_{}&#39;.format(metric) print(metric) print(&#39;Mean: {}&#39;.format(scores[score_key].mean())) print(&#39;Std dev: {}&#39;.format(scores[score_key].std())) . %%time _scoring = [&#39;neg_root_mean_squared_error&#39;, &#39;neg_mean_absolute_error&#39;] forest_scores = cross_validate(model, X_TRAIN, Y_TRAIN, scoring = _scoring, cv=5) . CPU times: user 11.1 s, sys: 66 ms, total: 11.1 s Wall time: 11.2 s . display_score(forest_scores, _scoring[0]) . neg_root_mean_squared_error Mean: -15.22268277329878 Std dev: 3.677396464443359 . display_score(forest_scores, _scoring[1]) . neg_mean_absolute_error Mean: -9.559763633911203 Std dev: 2.786793874037375 . Hyperparameter Optimization . import joblib from sklearn.model_selection import RandomizedSearchCV . random_forest_base_model = RandomForestRegressor(random_state=42) param_grid = { &#39;bootstrap&#39;:[True], &#39;min_samples_leaf&#39;:[5,10,100,200,500], &#39;min_samples_split&#39;:[5,10,100,200,500], &#39;n_estimators&#39;:[100,200,400,500], &#39;max_features&#39;:[&#39;auto&#39;,&#39;sqrt&#39;,&#39;log2&#39;], &#39;max_depth&#39;:[5,10,15,20] } . CV_rf = RandomizedSearchCV(estimator=random_forest_base_model, n_iter=50, param_distributions=param_grid, scoring=&#39;neg_root_mean_squared_error&#39;, cv = 5, verbose = 1, n_jobs=-1, refit=True) . %%time with joblib.parallel_backend(&#39;multiprocessing&#39;): CV_rf.fit(X_TRAIN, Y_TRAIN) . Fitting 5 folds for each of 50 candidates, totalling 250 fits CPU times: user 646 ms, sys: 183 ms, total: 829 ms Wall time: 1min 5s . print(CV_rf.best_params_, CV_rf.best_score_) . {&#39;n_estimators&#39;: 100, &#39;min_samples_split&#39;: 10, &#39;min_samples_leaf&#39;: 10, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_depth&#39;: 20, &#39;bootstrap&#39;: True} -19.161578679126375 . pd.DataFrame(CV_rf.cv_results_).sort_values(&#39;rank_test_score&#39;)[:5] . mean_fit_time std_fit_time mean_score_time std_score_time param_n_estimators param_min_samples_split param_min_samples_leaf param_max_features param_max_depth param_bootstrap params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score . 4 2.390702 | 0.061919 | 0.014793 | 0.000683 | 100 | 10 | 10 | auto | 20 | True | {&#39;n_estimators&#39;: 100, &#39;min_samples_split&#39;: 10,... | -19.636059 | -20.949326 | -16.321387 | -18.878979 | -20.022143 | -19.161579 | 1.568968 | 1 | . 20 1.299131 | 0.010285 | 0.033008 | 0.003166 | 200 | 10 | 10 | sqrt | 10 | True | {&#39;n_estimators&#39;: 200, &#39;min_samples_split&#39;: 10,... | -21.980440 | -23.012936 | -18.847124 | -19.781677 | -17.760789 | -20.276593 | 1.949783 | 2 | . 22 10.616245 | 0.066480 | 0.084979 | 0.025942 | 500 | 5 | 10 | auto | 5 | True | {&#39;n_estimators&#39;: 500, &#39;min_samples_split&#39;: 5, ... | -21.048335 | -23.039862 | -18.059296 | -18.935399 | -20.566808 | -20.329940 | 1.733000 | 3 | . 40 3.364752 | 0.126191 | 0.089052 | 0.004354 | 500 | 10 | 10 | sqrt | 15 | True | {&#39;n_estimators&#39;: 500, &#39;min_samples_split&#39;: 10,... | -22.245647 | -23.261791 | -18.796893 | -20.179776 | -17.747398 | -20.446301 | 2.061082 | 4 | . 2 0.448634 | 0.006245 | 0.015180 | 0.001035 | 100 | 5 | 10 | log2 | 20 | True | {&#39;n_estimators&#39;: 100, &#39;min_samples_split&#39;: 5, ... | -22.658766 | -23.491199 | -19.473837 | -20.457559 | -17.931118 | -20.802496 | 2.039804 | 5 | . best_model = CV_rf.best_estimator_ . best_model . RandomForestRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=10, random_state=42) .",
            "url": "http://pgg1610.github.io/blog_fastpages/chemical-science/python/machine-learning/2021/04/28/material_prop_walkthrough.html",
            "relUrl": "/chemical-science/python/machine-learning/2021/04/28/material_prop_walkthrough.html",
            "date": " • Apr 28, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Making equal spaces parity plots using Matplotlib",
            "content": "import os import matplotlib.pyplot as plt import numpy as np # High DPI rendering for mac %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . X = np.linspace(0,5,200) Y = 1.3*X + np.random.normal(0.01, size=X.shape) . Quick plotting . fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(X, Y) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) . Text(0, 0.5, &#39;Y&#39;) . Make plots with equal aspect ratio and axes . fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(X, Y, label=&#39;data&#39;) # Find limits for each axes lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;parity&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlim(lims) ax.set_ylim(lims) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) handles, labels = ax.get_legend_handles_labels() print(labels) ax.legend(handles=handles, labels=labels, title=&quot;Legend&quot;) . [&#39;parity&#39;, &#39;data&#39;] . &lt;matplotlib.legend.Legend at 0x11ad6a190&gt; . Slightly fancier output with parity and linear fit plots . fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(X, Y, alpha=0.6, label=&#39;data&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] # Linear fit line reg = np.polyfit(X, Y, deg=1) ax.plot(lims, reg[0] * np.array(lims) + reg[1], &#39;r--&#39;, linewidth=1.5, label=&#39;linear fit&#39;) # Parity plot ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;parity&#39;) #ax.set_aspect(&#39;equal&#39;) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) handles, labels = ax.get_legend_handles_labels() print(labels) # Put a legend to the right of the current axis ax.legend(handles=handles, labels=labels, title=&quot;Legend&quot;, loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5)) . [&#39;linear fit&#39;, &#39;parity&#39;, &#39;data&#39;] . &lt;matplotlib.legend.Legend at 0x11af6afd0&gt; .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-visualization/2021/04/08/matplotlib-equal-aspect.html",
            "relUrl": "/python/data-visualization/2021/04/08/matplotlib-equal-aspect.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Anamoly detection using autoencoders",
            "content": "An autoencoder is a type of unsupervised learning method. More than that, it is a type of &#39;generative&#39; model which once trained can potentially help generate synthetic data. In essense, it is a non-linear dimensionality reduction technique to learn the internal low-dimensional structure of the data. . An autoencoder model contains two components: . 1. An encoder: that takes an image as input, and outputs a low-dimensional embedding (representation) of the image. . 2. A decoder: that takes the low-dimensional embedding, and reconstructs the image. . . In esence autoencoder can be viewed as a dimensionality reduction tool for embedding the data in low-dimensional latent space which is non-linear. . Relationship to PCA . Autoencoder can be seen as a generalisation of principal component analysis to nonlinear manifolds. If we remove the nonlinearity (brought about by the activation functions) then the result of autoencoder will be in (some sense) equivalent to PCA. Now, however, the component vectors encoded by weights will not be orthogonal, like with PCA. . Anamoly Detection . Besides being used a generative model, it can be used as a anamoly detection method by considering the loss value between the decoded object and the encoded entity. By setting a threshold on the acceptable loss values we can train the model to flag any instances wherein the model&#39;s loss value exceed that limit and potentially is an anamolous digit. . Such an anamoly detection could be used in processes where images, sound, or signal is scanned and flagged for being out of spec. Google I/O in 2021 had a nice workshop on introducing Autoencoder and their utility in anomaly detection for detecting abnormal heart rhythm. Video . A simple autoencoder based on a CNN architecture will be built to encode and embed MNIST hand-written digit data. . Model Development . For the CNN stride and filter size is chose to ensure final vector in the bottleneck is commensurate with a single vector. To understand more on the how the stride and filter is chosen, or the effect of those parameters on the convolution, there&#39;s a helpful visualization and documentation here: https://github.com/vdumoulin/conv_arithmetic . import os import copy import numpy as np import torch import torch.nn as nn import tqdm.notebook as tqdm import torch.nn.functional as F torch.manual_seed(42); np.random.seed(42); . import matplotlib.pyplot as plt from matplotlib.pyplot import cm %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} . plot_params = { &#39;image.cmap&#39;:&#39;binary&#39;, &#39;image.interpolation&#39;:&#39;nearest&#39; } plt.rcParams.update(plot_params) . from torchvision import datasets, transforms . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) device . device(type=&#39;cuda&#39;) . transform_data = transforms.ToTensor() train_dataset = datasets.MNIST(root=&#39;input/&#39;, train=True, transform=transform_data, download=True) val_dataset = datasets.MNIST(root=&#39;input/&#39;, train=False, transform=transform_data, download=True) . input_tensor, label = train_dataset[0] print(&#39;MNIST dataset with {} train data and {} validation data&#39;.format(len(train_dataset), len(val_dataset))) print(&#39;Type of data in dataset: {} AND {}&#39;.format(type(input_tensor), type(label))) print(&#39;Input tensor image dimensions: {}&#39;.format(input_tensor.shape)) . MNIST dataset with 60000 train data and 10000 validation data Type of data in dataset: &lt;class &#39;torch.Tensor&#39;&gt; AND &lt;class &#39;int&#39;&gt; Input tensor image dimensions: torch.Size([1, 28, 28]) . class AutoEncoder(nn.Module): def __init__(self, latent_dimensions=10): super(AutoEncoder, self).__init__() self.encoder_module = nn.Sequential( nn.Conv2d(1, 16, 3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 32, 3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(32, 64, 7, stride=1, padding=0) ) self.decoder_module = nn.Sequential( nn.ConvTranspose2d(64, 32, 7, stride=1, padding=0), nn.ReLU(), nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), nn.ReLU(), nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid() ) self.NN_encode_to_latent = nn.Linear(64,latent_dimensions) self.NN_latent_to_decode = nn.Linear(latent_dimensions,64) def encoder(self, x): #Encode the points encode_x = self.encoder_module(x) batch_size, _, _, _ = encode_x.shape #Bottle neck layer - dont need this but useful when converting it to variational type encode_x = encode_x.view(batch_size, -1) x_encode_to_latent = self.NN_encode_to_latent(encode_x) return(x_encode_to_latent, batch_size) def decoder(self, x_encode_to_latent, batch_size): x_latent_to_decode = self.NN_latent_to_decode(x_encode_to_latent) # Decode the points latent_x_reshape = x_latent_to_decode.view(batch_size, -1, 1, 1) reconst = self.decoder_module(latent_x_reshape) return(reconst) def forward(self, x): latent_vector, batch_size = self.encoder(x) reconst = self.decoder(latent_vector, batch_size) return(reconst, latent_vector) . def train(model, data_loader, epoch, criterion, optimizer): counter = 0 epoch_loss_list = [] model.train() for i, data_batch in enumerate(data_loader): data, label = data_batch data = data.to(device) reconstruction, latent_x = model(data) loss = criterion(reconstruction, data) loss.backward() optimizer.step() optimizer.zero_grad() counter = counter + 1 epoch_loss_list.append(loss.item()) if i == 0: #Only append first batch outputs = (epoch, label, data.cpu().detach(), latent_x, reconstruction.cpu().detach()) mean_train_loss = sum(epoch_loss_list) / counter if epoch % 5 == 0: print(&#39;Training: Epoch: {0}, Loss: {1:0.3f}&#39;.format(epoch+1, mean_train_loss)) return outputs, epoch_loss_list, mean_train_loss def validation(model, data_loader, epoch, criterion): counter = 0 epoch_loss_list = [] model.eval() for i, data_batch in enumerate(data_loader): with torch.no_grad(): data, label = data_batch data = data.to(device) reconstruction, latent_x = model(data) loss = criterion(reconstruction, data) counter = counter + 1 epoch_loss_list.append(loss.item()) if i == 0: #Only append first batch outputs = (epoch, label, data.cpu().detach(), latent_x, reconstruction.cpu().detach()) mean_val_loss = sum(epoch_loss_list) / counter if epoch % 5 == 0: print(&#39;** Validation: Epoch: {0}, Loss: {1:0.3f}&#39;.format(epoch+1, mean_val_loss)) print(&#39;-&#39;*10) return outputs, epoch_loss_list, mean_val_loss . model = AutoEncoder(latent_dimensions=20) model = model.to(device) model . AutoEncoder( (encoder_module): Sequential( (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (3): ReLU() (4): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1)) ) (decoder_module): Sequential( (0): ConvTranspose2d(64, 32, kernel_size=(7, 7), stride=(1, 1)) (1): ReLU() (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1)) (3): ReLU() (4): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1)) (5): Sigmoid() ) (NN_encode_to_latent): Linear(in_features=64, out_features=20, bias=True) (NN_latent_to_decode): Linear(in_features=20, out_features=64, bias=True) ) . num_epochs=20 batch_size=64 learning_rate=1e-3 criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=1e-5) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, shuffle = True) . %%time train_output_array, val_output_array = [], [] train_loss_array, val_loss_array = [], [] for epoch in tqdm.tqdm(range(num_epochs)): train_out, train_loss_list, epoch_train_loss = train(model, train_loader, epoch, criterion, optimizer) val_out, val_loss_list, epoch_val_loss = validation(model, val_loader, epoch, criterion) train_loss_array.append(epoch_train_loss) val_loss_array.append(epoch_val_loss) train_output_array.append(train_out) val_output_array.append(val_out) # Append loss values for train and validation in the final epoch # Another option is to taken loss values for the epoch when the validation loss is lowest if epoch == num_epochs - 1: final_train_loss = train_loss_list final_val_loss = val_loss_list . Training: Epoch: 1, Loss: 0.047 ** Validation: Epoch: 1, Loss: 0.022 - Training: Epoch: 6, Loss: 0.009 ** Validation: Epoch: 6, Loss: 0.008 - Training: Epoch: 11, Loss: 0.008 ** Validation: Epoch: 11, Loss: 0.008 - Training: Epoch: 16, Loss: 0.008 ** Validation: Epoch: 16, Loss: 0.008 - CPU times: user 4min 14s, sys: 3.15 s, total: 4min 17s Wall time: 4min 17s . plt.plot(train_loss_array, label=&#39;Train loss&#39;) plt.plot(val_loss_array, label=&#39;Validation loss&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;MSE Loss&#39;) plt.legend(loc=&#39;best&#39;); . len(train_output_array) . 20 . for k in range(0, num_epochs, 4): digit_out = val_output_array[k][2].detach().numpy() label_out = val_output_array[k][1].detach().numpy() reconst_out = val_output_array[k][4].detach().numpy() fig, ax = plt.subplots(2,9, figsize=(9,2)) ax = ax.flatten() print(&#39;Epoch: {}&#39;.format(k)) for i, item in enumerate(digit_out): if i &lt; 9: ax[i].imshow(item[0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[i].set_title(&#39;Digit:{}&#39;.format(label_out[i])) ax[i].set_xticks([]) ax[i].set_yticks([]) for i, item in enumerate(reconst_out): if i &lt; 9: ax[i+9].imshow(item[0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[i+9].set_xticks([]) ax[i+9].set_yticks([]) plt.show() . Epoch: 0 . Epoch: 4 . Epoch: 8 . Epoch: 12 . Epoch: 16 . With subsequent epochs the reconstruction of the images becomes better. . img_temp = val_output_array[num_epochs-1][2].detach().numpy() print(img_temp.shape) model = model.cpu() ecode_img, vector_img = model(torch.tensor(img_temp)) print(criterion(ecode_img, torch.tensor(img_temp)).item()) print(ecode_img.shape, vector_img.shape) vector_img = vector_img.detach().numpy() print(vector_img.shape) ecode_img = ecode_img.detach().numpy() print(ecode_img.shape) . (64, 1, 28, 28) 0.007447564974427223 torch.Size([64, 1, 28, 28]) torch.Size([64, 20]) (64, 20) (64, 1, 28, 28) . Visualizing the reconstruction of a random validation data digit . fig, ax = plt.subplots(1,2,figsize=(5,5)) ax[0].imshow(img_temp[0][0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[0].set_title(&#39;Digit Input&#39;) ax[1].imshow(ecode_img[0][0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[1].set_title(&#39;Reconstructed Input&#39;) ax[0].set_xticks([]) ax[0].set_yticks([]) ax[1].set_xticks([]) ax[1].set_yticks([]); . print(len(final_train_loss), len(final_val_loss)) . 938 157 . Visualize the distribution of epoch losses for train and validation set from the last epoch. The statistics from this distribution will be used to set the threshold for the anamoly detection in the later section . fig, ax = plt.subplots(2,1, figsize=(10,5), sharex=True) ax[0].hist(final_train_loss, bins=100) ax[0].axvline(x = np.median(final_train_loss), color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=2.0, label=&#39;Median&#39;) ax[0].axvline(x = np.mean(final_train_loss), color=&#39;cyan&#39;, linestyle=&#39;--&#39;, linewidth=2.0, label=&#39;Mean&#39;) ax[1].hist(final_val_loss, bins=100) ax[1].axvline(x = np.median(final_val_loss), color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=2.0,label=&#39;Median&#39;) ax[1].axvline(x = np.mean(final_val_loss), color=&#39;cyan&#39;, linestyle=&#39;--&#39;, linewidth=2.0,label=&#39;Mean&#39;) plt.legend(); . print(&#39;Mean Train Loss: {:0.6f}&#39;.format(np.mean(final_train_loss))) print(&#39;Mean Val Loss: {:0.6f}&#39;.format(np.mean(final_val_loss))) . Mean Train Loss: 0.007515 Mean Val Loss: 0.007516 . threshold_loss = np.mean(final_val_loss) + np.std(final_val_loss) print(&#39;Threshold loss is set at: {:0.6f}&#39;.format(threshold_loss)) . Threshold loss is set at: 0.008046 . Visualize latent space . label_collection = np.array([]) latent_space_collection = np.zeros((0,20)) for i, data_batch in enumerate(val_loader): with torch.no_grad(): data, label = data_batch reconstruction, latent_x = model(data) label_collection = np.concatenate((label.numpy(), label_collection)) latent_space_collection = np.vstack((latent_x.numpy(), latent_space_collection)) . print(label_collection.shape, latent_space_collection.shape) . (10000,) (10000, 20) . tSNE plots . import openTSNE as openTSNE print(&#39;openTSNE&#39;, openTSNE.__version__) . openTSNE 0.6.0 . %%time # BH and PCA_init by default Z1 = openTSNE.TSNE(n_jobs=-1, negative_gradient_method=&#39;bh&#39;).fit(latent_space_collection) . CPU times: user 2min 46s, sys: 2min 11s, total: 4min 57s Wall time: 39 s . from matplotlib.colors import ListedColormap import seaborn as sns cmap = ListedColormap(sns.husl_palette(len(np.unique(label_collection)))) fig, ax = plt.subplots(1,1, figsize=(10,10)) im = ax.scatter(Z1[:,0], Z1[:,1], s=10, c=label_collection, cmap=cmap, edgecolor=&#39;none&#39;) ax.set_xticks([]) ax.set_yticks([]) ax.set_xlabel(&#39;tSNE component 1&#39;) ax.set_ylabel(&#39;tSNE component 2&#39;) ax.set_title(&#39;t-SNE on Latent Space (MNIST data)&#39;) fig.subplots_adjust(right=0.8) cbar_ax = fig.add_axes([0.85, 0.25, 0.01, 0.5], label=&#39;digit&#39;) cbar = fig.colorbar(im, cax=cbar_ax, label=&#39;Digit&#39;) . /depot/jgreeley/apps/envs/gpu_env1/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError. warnings.warn(msg) . Embedding 20 dimensions in 2 dimensional tSNE space, the clusters for each digit become quite clear. It is interesting to see that cluster for 1 and 7 are quite close to another, similarly cluster for 5 and 3 which have been traditionally challenging to distinguish in the classifers . Linear interpolation in the latent space . print(latent_space_collection.shape) #Select random points for start and ending start_point_index = 4121 end_point_index = 9832 start_point_latent_vectors = latent_space_collection[start_point_index] end_point_latent_vectors = latent_space_collection[end_point_index] . (10000, 20) . num_steps = 10 trajectory_points = np.zeros((0,20)) for i in range(num_steps): z = start_point_latent_vectors * i/num_steps + end_point_latent_vectors * (num_steps - i) / num_steps trajectory_points = np.vstack((z, trajectory_points)) print(trajectory_points.shape) . (10, 20) . trajectory_latent_tensor = torch.tensor(trajectory_points) . reconstruction_images = model.decoder(trajectory_latent_tensor.float(), trajectory_latent_tensor.shape[0]) reconstruction_images.shape . torch.Size([10, 1, 28, 28]) . reconstruction_images = reconstruction_images.detach() . fig, ax = plt.subplots(1,num_steps, figsize=(num_steps+10,4)) ax = ax.flatten() for i in range(0, reconstruction_images.shape[0]): ax[i].imshow(reconstruction_images[i][0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[i].set_title(&#39;Step: {}&#39;.format(i+1)) ax[i].set_xticks([]) ax[i].set_yticks([]); . Visualizing the image generated from each embedding iterated in a linear fashion from start to finish shows the transition between end points . Denoising images . An autoencoder trained on cleaned image can be used to clear out blurry inputs from outside training set . rand_idx = 420 digit_image, label = val_dataset[rand_idx] digit_image = digit_image[0] plt.imshow(digit_image) plt.title(&#39;Image of {}&#39;.format(label)) plt.axis(&#39;off&#39;); . Add gaussian noise to the image. Code taken from Github . random_noise = np.random.randn(digit_image.shape[0], digit_image.shape[1]) digit_image_noise = np.clip( digit_image + random_noise * 0.2, 0, 1) plt.imshow(digit_image_noise) plt.title(&#39;Image of {}&#39;.format(label)) plt.axis(&#39;off&#39;); . digit_input_tensor = torch.tensor(digit_image_noise[np.newaxis, np.newaxis, ...]).float(); . /depot/jgreeley/apps/envs/gpu_env1/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). &#34;&#34;&#34;Entry point for launching an IPython kernel. . digit_input_tensor.shape . torch.Size([1, 1, 28, 28]) . predicted_image, _ = model(digit_input_tensor) predicted_image = predicted_image.detach().numpy() . plt.imshow(predicted_image[0][0]) plt.title(&#39;Image of {}&#39;.format(label)) plt.axis(&#39;off&#39;); . The results are much more useful in the case of variational autoencoder which is more robust to noise in the input data since the latent is less sparse due to the variational part . Anomaly Detection . def reconstruction_loss(input_image, _model=model, _criterion=criterion, plot=True): model = _model.cpu() input_image_tensor = torch.tensor(input_image) encoded_image, _ = model(input_image_tensor) loss_value = _criterion(encoded_image, input_image_tensor).item() encoded_image = encoded_image.detach().numpy() if plot == True: fig, ax = plt.subplots(1,3,figsize=(10,5)) ax[0].imshow(input_image[0][0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[0].set_title(&#39;Input Image&#39;) ax[1].imshow(encoded_image[0][0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[1].set_title(&#39;Reconstructed Input&#39;) ax[2].imshow(input_image[0][0] - encoded_image[0][0], cmap=cm.binary, interpolation=&#39;nearest&#39;) ax[2].set_title(&#39;Image Difference&#39;) ax[0].set_xticks([]) ax[0].set_yticks([]) ax[1].set_xticks([]) ax[1].set_yticks([]) ax[2].set_xticks([]) ax[2].set_yticks([]) return(loss_value) . random_entry_from_val_set = val_output_array[num_epochs-4][2].detach().numpy()[0] print(random_entry_from_val_set.shape) . (1, 28, 28) . input_image = random_entry_from_val_set[np.newaxis, ...] print(input_image.shape) . (1, 1, 28, 28) . loss_value = reconstruction_loss(input_image) compare_value = (&#39;Higher&#39; if loss_value &gt; threshold_loss else &#39;lower&#39;) anamoly_tag = (&#39;anomaly&#39; if compare_value == &#39;Higher&#39; else &#39;not an Anomaly&#39;) print(&#39;Loss value is {0:6f} which is {1} than set threshold, so this image is {2}&#39;.format(loss_value, compare_value, anamoly_tag)) . Loss value is 0.001904 which is lower than set threshold, so this image is not an Anomaly . Example . plt.imshow(input_image[0][0]) plt.axis(&#39;off&#39;); . temp_image_rotate = np.rot90(input_image, k=1, axes=(2,3)).copy() # To get rid of negative stride error . plt.imshow(temp_image_rotate[0][0]) plt.axis(&#39;off&#39;); . loss_value = reconstruction_loss(temp_image_rotate) compare_value = (&#39;Higher&#39; if loss_value &gt; threshold_loss else &#39;lower&#39;) anamoly_tag = (&#39;anomaly&#39; if compare_value == &#39;Higher&#39; else &#39;not an Anomaly&#39;) print(&#39;Loss value is {0:6f} which is {1} than set threshold, so this image is {2}&#39;.format(loss_value, compare_value, anamoly_tag)) . Loss value is 0.028646 which is Higher than set threshold, so this image is anomaly .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/machine-learning/2021/03/18/autoencoder.html",
            "relUrl": "/python/data-analysis/machine-learning/2021/03/18/autoencoder.html",
            "date": " • Mar 18, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Implement neural network from scratch for binary classification",
            "content": "In this notebook I build a simple neural network, having a single hidden layer. Next, I compare this model for its classification accuracy to a boilerplate logistic regression. . Implement a 2-class classification neural network with a single hidden layer | Use units with a non-linear activation function, such as tanh | Compute the cross entropy loss | Implement forward and backward propagation | . This notebook was inspired by Andrew Ng&#39;s Deep Learning Specialization tutorial on Coursera . import numpy as np import matplotlib.pyplot as plt import sklearn import sklearn.datasets as datasets import sklearn.linear_model import copy as copy %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} np.random.seed(42) # set a seed so that the results are consistent . Dataset . Code to make spirals is adapted from: . http://cs231n.github.io/neural-networks-case-study/ . N = 400 # number of points per class D = 2 # dimensionality K = 2 # number of spokes X = np.zeros((N*K,D)) # data matrix (each row = single example) Y = np.zeros(N*K, dtype=&#39;int&#39;) # class labels for j in range(K): ix = range(N*j,N*(j+1)) r = np.linspace(0, 1, N) # radius t = np.linspace(j*4.2, (j+1)*4.2, N) + np.random.randn(N)*0.2 # theta X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] Y[ix] = (0 if j % 2 == 0 else 1) X = copy.deepcopy(X.T) Y = copy.deepcopy(Y.reshape(-1,1).T) . fig, ax = plt.subplots(1,1, figsize=(8,8)) # lets visualize the data: ax.scatter(X[0, :], X[1, :], c=Y.ravel(), s=40, cmap=plt.cm.Spectral) ax.set_xlabel(&#39;$X_1$&#39;) ax.set_ylabel(&#39;$X_2$&#39;) ax.set_title(&#39;Visualize data&#39;) . Text(0.5, 1.0, &#39;Visualize data&#39;) . shape_X = X.shape shape_Y = Y.shape print (&#39;The shape of X is: &#39; + str(shape_X)) print (&#39;The shape of Y is: &#39; + str(shape_Y)) . The shape of X is: (2, 800) The shape of Y is: (1, 800) . Simple Logistic Regression . Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn&#39;s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset. . clf = sklearn.linear_model.LogisticRegression(); clf.fit(X.T, Y.ravel()); . Convenience function to plot a decision boundary for the classification model . def plot_decision_boundary(func, x_input, y_input): xx_1, xx_2 = np.mgrid[np.min(x_input[:,0]):np.max(x_input[:,0]):.01, np.min(x_input[:,1]):np.max(x_input[:,1]):.01] grid = np.c_[xx_1.ravel(), xx_2.ravel()] y_pred_grid = func(grid).reshape(xx_1.shape) y_pred = func(x_input) fig, ax = plt.subplots(figsize=(10, 10)) contour = ax.contourf(xx_1, xx_2, y_pred_grid, alpha=0.7, cmap=&quot;Spectral&quot;) ax.scatter(x_input[:,0], x_input[:, 1], c=y_pred, s=50, cmap=&quot;Spectral&quot;, edgecolor=&quot;white&quot;, linewidth=1) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.set(aspect=&#39;equal&#39;, xlim=(np.min(x_input[:,0]), np.max(x_input[:,0])), ylim=(np.min(x_input[:,1]),np.max(x_input[:,1])), xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;) . plot_decision_boundary(lambda x: clf.predict(x), X.T, Y.T) plt.title(&quot;Logistic Regression&quot;) . Text(0.5, 1.0, &#39;Logistic Regression&#39;) . LR_predictions = clf.predict(X.T) print (&#39;Accuracy of logistic regression: %d &#39; % float((np.dot(Y, LR_predictions) + np.dot(1-Y, 1-LR_predictions))/float(Y.size)*100) + &#39;% &#39; + &quot;(percentage of correctly labelled datapoints)&quot;) . Accuracy of logistic regression: 66 % (percentage of correctly labelled datapoints) . Interpretation: The dataset is not linearly separable, so logistic regression doesn&#39;t perform well. Hopefully a neural network will do better. . Neural Network model . Logistic regression did not work well on the dataset. Let&#39;s train a Neural Network with a single hidden layer and see if it does any better. . Here is basic framework for the model: . Mathematically: . For one example $x^{(i)}$: . $$ z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1]} tag{1} $$ . $$ a^{[1] (i)} = tanh(z^{[1] (i)}) tag{2} $$$$ z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]} tag{3} $$$$ hat{y}^{(i)} = a^{[2] (i)} = sigma(z^{ [2] (i)}) tag{4} $$$$ y^{(i)}_{prediction} = begin{cases} 1 &amp; mbox{if } a^{[2](i)} &gt; 0.5 0 &amp; mbox{otherwise } end{cases} tag{5} $$Given the predictions on all the examples, you can also compute the cost $J$ as follows: . $$ J = - frac{1}{m} sum limits_{i = 0}^{m} large left( small y^{(i)} log left(a^{[2] (i)} right) + (1-y^{(i)}) log left(1- a^{[2] (i)} right) large right) small tag{6} $$The general methodology to build a Neural Network is to: . Define the neural network structure ( # of input units, # of hidden units, etc). | Initialize the model&#39;s parameters | Loop: Implement forward propagation | Compute loss | Implement backward propagation to get the gradients | Update parameters (gradient descent) | . | Defining the neural network structure . Define three variables: . - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4) - n_y: the size of the output layer . def layer_sizes(X, Y, n_h=4): &quot;&quot;&quot; Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer &quot;&quot;&quot; n_x = X.shape[0] # size of input layer n_h = n_h n_y = Y.reshape(-1,1).T.shape[0] # size of output layer return (n_x, n_h, n_y) . (n_x, n_h, n_y) = layer_sizes(X, Y) print(&quot;The size of the input layer is: n_x = &quot; + str(n_x)) print(&quot;The size of the hidden layer is: n_h = &quot; + str(n_h)) print(&quot;The size of the output layer is: n_y = &quot; + str(n_y)) . The size of the input layer is: n_x = 2 The size of the hidden layer is: n_h = 4 The size of the output layer is: n_y = 1 . Initialize the model&#39;s parameters . Initialize the weights matrices with random values. Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). | . | Initialize the bias vectors as zeros. Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros. | . | . def initialize_parameters(n_x, n_h, n_y): &quot;&quot;&quot; Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) &quot;&quot;&quot; np.random.seed(42) # we set up a seed so that your output matches ours although the initialization is random. W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y,1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2} return parameters . Forward-pass . Implement forward_propagation(): . Retrieve each parameter from the dictionary &quot;parameters&quot; (which is the output of initialize_parameters()) by using parameters[&quot;..&quot;]. | Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set). | Values needed in the backpropagation are stored in &quot;cache&quot;. The cache will be given as an input to the backpropagation function. | . def sigmoid(x): z = 1/(1 + np.exp(-x)) return z . def forward_propagation(X, parameters): &quot;&quot;&quot; Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot; &quot;&quot;&quot; # Retrieve each parameter from the dictionary &quot;parameters&quot; W1 = parameters[&#39;W1&#39;] b1 = parameters[&#39;b1&#39;] W2 = parameters[&#39;W2&#39;] b2 = parameters[&#39;b2&#39;] ### END CODE HERE ### # Implement Forward Propagation Z1 = np.dot(W1,X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2,A1) + b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = {&quot;Z1&quot;: Z1, &quot;A1&quot;: A1, &quot;Z2&quot;: Z2, &quot;A2&quot;: A2} return A2, cache . Loss function . Compute the cost function as follows: . $$ J = - frac{1}{m} sum limits_{i = 1}^{m} large{(} small y^{(i)} log left(a^{[2] (i)} right) + (1-y^{(i)}) log left(1- a^{[2] (i)} right) large{)} small tag{13} $$ def compute_cost(A2, Y): &quot;&quot;&quot; Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- &quot;true&quot; labels vector of shape (1, number of examples) Returns: cost -- cross-entropy cost given equation (13) &quot;&quot;&quot; m = Y.shape[1] # number of example # Compute the cross-entropy cost logprobs = np.dot(Y,np.log(A2).T) + np.dot((1-Y),np.log((1-A2)).T) cost = -logprobs/m cost = float(np.squeeze(cost)) # makes sure cost is the dimension we expect. E.g., turns [[17]] into 17 assert(isinstance(cost, float)) return cost . Back-propogation . Using the cache computed during forward propagation, now implement backward propagation. . $$ frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } = frac{1}{m} (a^{[2](i)} - y^{(i)}) $$$$ frac{ partial mathcal{J} }{ partial W_2 } = frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } a^{[1] (i) T} $$$$ frac{ partial mathcal{J} }{ partial b_2 } = sum_i{ frac{ partial mathcal{J} }{ partial z_{2}^{(i)}}} $$$$ frac{ partial mathcal{J} }{ partial z_{1}^{(i)} } = W_2^T frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $$$$ frac{ partial mathcal{J} }{ partial W_1 } = frac{ partial mathcal{J} }{ partial z_{1}^{(i)} } X^T $$$$ frac{ partial mathcal{J} _i }{ partial b_1 } = sum_i{ frac{ partial mathcal{J} }{ partial z_{1}^{(i)}}} $$ $*$ denotes elementwise multiplication. | Gradients for each later: dW1 = $ frac{ partial mathcal{J} }{ partial W_1 }$ | db1 = $ frac{ partial mathcal{J} }{ partial b_1 }$ | dW2 = $ frac{ partial mathcal{J} }{ partial W_2 }$ | db2 = $ frac{ partial mathcal{J} }{ partial b_2 }$ | . | . def backward_propagation(parameters, cache, X, Y): &quot;&quot;&quot; Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;. X -- input data of shape (2, number of examples) Y -- &quot;true&quot; labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters &quot;&quot;&quot; m = X.shape[1] # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;. W1 = parameters[&#39;W1&#39;] W2 = parameters[&#39;W2&#39;] # Retrieve also A1 and A2 from dictionary &quot;cache&quot;. A1 = cache[&#39;A1&#39;] A2 = cache[&#39;A2&#39;] # Backward propagation: calculate dW1, db1, dW2, db2. dZ2 = A2 - Y dW2 = (1/m) * np.dot(dZ2,A1.T) db2 = (1/m) * np.sum(dZ2,axis=1, keepdims=True) dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = (1/m) * np.dot(dZ1, X.T) db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True) grads = {&quot;dW1&quot;: dW1, &quot;db1&quot;: db1, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2} return grads . General gradient descent formalism: $$ theta = theta - alpha frac{ partial J }{ partial theta }$$ . where: $ alpha$ is the learning rate and $ theta$ represents a parameter. . def update_parameters(parameters, grads, learning_rate = 1.2): &quot;&quot;&quot; Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters &quot;&quot;&quot; # Retrieve each parameter from the dictionary &quot;parameters&quot; W1 = parameters[&#39;W1&#39;] b1 = parameters[&#39;b1&#39;] W2 = parameters[&#39;W2&#39;] b2 = parameters[&#39;b2&#39;] # Retrieve each gradient from the dictionary &quot;grads&quot; dW1 = grads[&#39;dW1&#39;] db1 = grads[&#39;db1&#39;] dW2 = grads[&#39;dW2&#39;] db2 = grads[&#39;db2&#39;] # Update rule for each parameter W1 = W1 - learning_rate*dW1 b1 = b1 - learning_rate*db1 W2 = W2 - learning_rate*dW2 b2 = b2 - learning_rate*db2 parameters = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2} return parameters . Integrate previous parts nn_model() . def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): &quot;&quot;&quot; Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. &quot;&quot;&quot; np.random.seed(42) n_x, n_h, n_y = layer_sizes(X, Y, n_h=n_h) # Initialize parameters parameters = initialize_parameters(n_x, n_h, n_y) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;. A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;. cost = compute_cost(A2, Y) # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;. grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;. parameters = update_parameters(parameters, grads, learning_rate = 1.2) # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print (&quot;Cost after iteration %i: %f&quot; %(i, cost)) return parameters . Predictions . Use the model to predict: predict(). . Use forward propagation to predict results. . predictions = $y_{prediction} = mathbb 1 text = begin{cases} 1 &amp; text{if} activation &gt; 0.5 0 &amp; text{otherwise} end{cases}$ . def predict(parameters, X): &quot;&quot;&quot; Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) &quot;&quot;&quot; # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. A2, cache = forward_propagation(X, parameters) threshold = 0.5 predictions = (A2 &gt; threshold) return predictions . It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units. . parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True) . Cost after iteration 0: 0.693141 Cost after iteration 1000: 0.052671 Cost after iteration 2000: 0.040765 Cost after iteration 3000: 0.032499 Cost after iteration 4000: 0.027457 Cost after iteration 5000: 0.023722 Cost after iteration 6000: 0.020082 Cost after iteration 7000: 0.016282 Cost after iteration 8000: 0.013001 Cost after iteration 9000: 0.010872 . def plot_decision_boundary_NN(func, x_input, y_input, ax=None): xx_1, xx_2 = np.mgrid[np.min(x_input[:,0]):np.max(x_input[:,0]):.01, np.min(x_input[:,1]):np.max(x_input[:,1]):.01] grid = np.c_[xx_1.ravel(), xx_2.ravel()].T y_pred_grid = func(grid).reshape(xx_1.shape) y_pred = func(x_input.T) if ax == None: fig, ax = plt.subplots(1,1, figsize=(10,10)) contour = ax.contourf(xx_1, xx_2, y_pred_grid, alpha=0.7, cmap=&quot;Spectral&quot;) ax.scatter(x_input[:,0], x_input[:, 1], c=y_pred, s=50, cmap=&quot;Spectral&quot;, edgecolor=&quot;white&quot;, linewidth=1) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.set(aspect=&#39;equal&#39;, xlim=(np.min(x_input[:,0]), np.max(x_input[:,0])), ylim=(np.min(x_input[:,1]),np.max(x_input[:,1])), xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;) return ax . plot_decision_boundary_NN(lambda x: predict(parameters, x), X.T, Y.T) plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4)) . Text(0.5, 1.0, &#39;Decision Boundary for hidden layer size 4&#39;) . predictions = predict(parameters, X) print (&#39;Accuracy: %d&#39; % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + &#39;%&#39;) . Accuracy: 99% . Accuracy is really high compared to Logistic Regression. The model has spirals! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. . Tuning hidden layer size . Run the following code to observe different behaviors of the model for various hidden layer sizes. . plt.figure(figsize=(16, 32)) hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50] for i, n_h in enumerate(hidden_layer_sizes): ax = plt.subplot(5, 2,i+1) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary_NN(lambda x: predict(parameters, x), X.T, Y.T, ax) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) ax.title.set_text(&#39;Hidden Layer of size {} | Accuracy: {}%&#39;.format(n_h, accuracy)) . Reference: . http://scs.ryerson.ca/~aharley/neural-networks/ | http://cs231n.github.io/neural-networks-case-study/ | .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/pytorch/machine-learning/2021/02/05/NN_classification_from_scratch.html",
            "relUrl": "/python/pytorch/machine-learning/2021/02/05/NN_classification_from_scratch.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "t-SNE and UMAP - Effect of initialization on the dimensionality reduction",
            "content": "Recreating the dataset explored in the recent publication looking at the effect of random initializations and sub-methods in well-known dimensionality reduction techniques: Initialization is critical for preserving global data structure in both t-SNE and UMAP . Module used t-SNE: Link | Module used for UMAP: Link | . Key takeaways: . Using either t-SNE or UMAP over another is difficult to justify. There is no evidence per se that UMAP algorithm have any advantage over t-SNE in terms of preserving global structure. . | These algorithms should be used cautiously and with informative initialization by default . | In all embeddings, distances between clusters of points can be completely meaningless. It is often impossible to represent complex topologies in 2 dimensions, and embeddings should be approached with the utmost care when attempting to interpret their layout. . | The only cerrtainty is the closeness of the points and their similarity . | These methods don’t work that great if the intrinsic dimensionality of the data is higher than 2D . | High dimensional data sets typically have lower intrinsic dimensionality $ d &lt;&lt; D $ however $d$ may still be larger than 2 and preserving these distances faithfully might not always be possible. . | When using both UMAP or t-SNE, one must take care not to overinterpret the embedding structure or distances. . | . import numpy as np import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} import openTSNE, umap print(&#39;openTSNE&#39;, openTSNE.__version__) print(&#39;umap&#39;, umap.__version__) . openTSNE 0.6.0 umap 0.5.1 . from openTSNE import TSNE from umap import UMAP . 1. Looking at 2D circle . n = 7000 np.random.seed(42) X = np.random.randn(n, 3) / 1000 X[:,0] += np.cos(np.arange(n)*2*np.pi/n) X[:,1] += np.sin(np.arange(n)*2*np.pi/n) plt.plot(X[:,0], X[:,1]); plt.axis(&#39;equal&#39;); . %%time # BH is faster for this sample size Z1 = TSNE(n_jobs=-1, initialization=&#39;random&#39;, random_state=42, negative_gradient_method=&#39;bh&#39;).fit(X) Z2 = TSNE(n_jobs=-1, negative_gradient_method=&#39;bh&#39;).fit(X) . CPU times: user 48.3 s, sys: 760 ms, total: 49.1 s Wall time: 40.8 s . %%time Z3 = UMAP(init=&#39;random&#39;, random_state=42).fit_transform(X) Z4 = UMAP().fit_transform(X) . CPU times: user 58.4 s, sys: 2.59 s, total: 1min Wall time: 33.4 s . %%time from sklearn import decomposition pca_2D = decomposition.PCA(n_components=2) pca_2D.fit(X) Z5 = pca_2D.transform(X) . CPU times: user 6.17 ms, sys: 5.36 ms, total: 11.5 ms Wall time: 5.72 ms . from matplotlib.colors import ListedColormap cmap = ListedColormap(sns.husl_palette(n)) titles = [&#39;Data&#39;, &#39;t-SNE, random init&#39;, &#39;t-SNE, PCA init&#39;, &#39;UMAP, random init&#39;, &#39;UMAP, LE init&#39;, &#39;PCA&#39;] plt.figure(figsize=(10,2)) for i,Z in enumerate([X,Z1,Z2,Z3,Z4,Z5],1): plt.subplot(1,6,i) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;datalim&#39;) plt.scatter(Z[:,0], Z[:,1], s=1, c=np.arange(n), cmap=cmap, edgecolor=&#39;none&#39;, rasterized=True) plt.xticks([]) plt.yticks([]) plt.title(titles[i-1], fontsize=8) #sns.despine(left=True, bottom=True) . 2. Looking at hand-written digit data . from sklearn.datasets import load_digits digits = load_digits() . digits.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) . X = digits.data Y = digits.target . %%time # BH is faster for this sample size Z1 = TSNE(n_jobs=-1, initialization=&#39;random&#39;, random_state=42, negative_gradient_method=&#39;bh&#39;).fit(X) Z2 = TSNE(n_jobs=-1, negative_gradient_method=&#39;bh&#39;).fit(X) . CPU times: user 14.8 s, sys: 331 ms, total: 15.2 s Wall time: 13.3 s . %%time Z3 = UMAP(init=&#39;random&#39;, random_state=42).fit_transform(X) Z4 = UMAP().fit_transform(X) . CPU times: user 17.7 s, sys: 311 ms, total: 18 s Wall time: 12.5 s . %%time pca_2D = decomposition.PCA(n_components=2) pca_2D.fit(X) Z5 = pca_2D.transform(X) . CPU times: user 17.8 ms, sys: 16.7 ms, total: 34.5 ms Wall time: 10.2 ms . from matplotlib.colors import ListedColormap cmap = ListedColormap(sns.husl_palette(len(np.unique(Y)))) titles = [&#39;Data&#39;, &#39;t-SNE, random init&#39;, &#39;t-SNE, PCA init&#39;, &#39;UMAP, random init&#39;, &#39;UMAP, LE init&#39;, &#39;PCA&#39;] fig, ax = plt.subplots(3,2, figsize=(15,15)) ax = ax.flatten() for i,Z in enumerate([X,Z1,Z2,Z3,Z4,Z5],0): im = ax[i].scatter(Z[:,0], Z[:,1], s=10, c=Y, cmap=cmap, edgecolor=&#39;none&#39;) ax[i].set_xticks([]) ax[i].set_yticks([]) ax[i].set_title(titles[i], fontsize=15) fig.subplots_adjust(right=0.8) cbar_ax = fig.add_axes([0.85, 0.25, 0.01, 0.5], label=&#39;digit&#39;) cbar = fig.colorbar(im, cax=cbar_ax,label=&#39;Digit&#39;) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-visualization/2021/02/03/tSNEvsUMAP.html",
            "relUrl": "/python/data-visualization/2021/02/03/tSNEvsUMAP.html",
            "date": " • Feb 3, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Bayesian optimisation implementation",
            "content": "If $f$ (objective function) is cheap to evaluate we can sample various points and built a potential surface however, if the $f$ is expensive -- like in case of first-principles electronic structure calculations, it is important to minimize the number of $f$ calls and number of samples drawn from this evaluation. In that case, if an exact functional form for f is not available (that is, f behaves as a “black box”), what can we do? . Bayesian optimization proceeds by maintaining a probabilistic belief about $f$ and designing a so called acquisition function to determine where to evaluate the next function call. Bayesian optimization is particularly well-suited to global optimization problems where: . $f$ is an expensive black-box function | Analytical solution for the gradient of the function is difficult to evaluate | The idea is the find &quot;global&quot; minimum with least number of steps. Incorporating prior beliefs about the underlying process and update the prior with samples draw from the model to better estimate the posterior. . Model used for approximating the objective function is called the surrogate model. . Following are few links I have found useful in understanding the inner workings of the Bayesian opitmization and certain typical surrogate functions used in it: . Good introductory write-up on Bayesian optimization (Distill Blog) . | Nice lecture explaining the working of Gaussian Processes here . | . Surrogate model . A popular surrogate model applied for Bayesian optimization, although strictly not required, are Gaussian Processes (GPs). These are used to define a prior beliefs about the objective function. The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement. Herein, we could substitute this for a ANNs or other surrogate models. . Acquisition functions . Used to propose sampling points in the search space. Have to consider the trade-off between exploitation vs exploration. . Exploitation == sampling where objective function value is high . | Exploration == where uncertainty is high . | . Both correspond to high acquisition function value. The goal is the maximize the acquisition value to determine next sampling point. . Popular acquisition functions: . Maximum probability of improvement . PI involves sampling for points which improve on the current best objective function value. The point in the sample space with the highest probability of improvement, based on the value predicted by the surrogate function, is chosen as the next point for evaluating through the expensive method. However in this searching scheme we look only at the probability improvement and not the extent of improvement. This might lead it to get stuck in a local minima. Instead we can turn to the Expected value of improvement wherein we consider the extent of improvement as well. . | Expected improvement (EI) . | Upper confidence bound (UCB) . | Optimization strategy . Following strategy is followed when optimizing using Bayesian optimization: Find the next sampling point $ mathbf{x}_t$ by optimizing the acquisition function over a surrogate model (in this case a GP) fit over a distribution $ mathcal{D}_{1:t-1}$ | Evaluate $f$ at $f(x_{t})$ i.e. sample $f(x_{t})$ from the $f$ | Add the new point to the prior of the GP now $ mathcal{D}_{1:t} = ( mathcal{D}_{1:t-1}, (x_{t},f(x_{t})) )$ | . Expected improvement . Expected improvement is defined as: $$ mathrm{EI}( mathbf{x}) = max((f( mathbf{x}) - f( mathbf{x}^+), 0)) tag{1}$$ . where $f( mathbf{x}^+)$ is the value of the best sample so far and $ mathbf{x}^+$ is the location of that sample i.e. $ mathbf{x}^+ = mathrm{argmax}_{ mathbf{x}_i in mathbf{x}_{1:t}} f( mathbf{x}_i)$. The expected improvement can be evaluated analytically under the GP model . Expected improvement can be evaluated analytically for a GP model. . Before diving in the technical details, I would like to acknowledge the work of Noah Wichrowski (JHU) in building the necessary ground work for the bayesian optimization routine implemented in this article. . Implementation with Numpy and Scipy . import matplotlib.pyplot as plt import numpy as np %config InlineBackend.figure_format = &#39;retina&#39; # Plot matplotlib plots with white background: %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} from sklearn.gaussian_process import GaussianProcessRegressor as GPR from sklearn.gaussian_process import kernels . from Bayesian_optimization import plotting_utils, acquisition, objectives, opti . plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . ## Import the acquisition functions implemented . EI = acquisition.ExpectedImprovement(delta = 0.01) LCB = acquisition.LowerConfidenceBound(sigma = 1.96) . A One-Dimensional Example . Egg-carton objective function . objective = objectives.egg_carton (low, high) = (0.0, 10.0) domain = np.array([[low], [high]]) x_pts = np.linspace(low, high, 1000).reshape(-1, 1) y_pts = objective(x_pts) . num_sample_points = 10 noise_ = 0.1 generator = np.random.default_rng(42) x_sample = generator.uniform(low, high, size = (num_sample_points, 1)) y_sample = objective(x_sample, noise_) . fig, ax = plt.subplots(1,1, figsize=(8,3)) ax.plot(x_pts, y_pts, &#39;k-&#39;, linewidth=2.0, label=&#39;Ground truth&#39;) ax.plot(x_sample, y_sample, &#39;ro&#39;, label=&#39;Noisy sampled points&#39;) ax.set_xlabel(r&quot;$x$&quot;, fontsize = 20) ax.set_ylabel(r&quot;$f(x)$&quot;, fontsize = 20) ax.set_title(&quot;Initial setup&quot;, fontsize = 20) ax.legend(fontsize = 15) ax.grid(True) . Fit a GPR model (surrogate function) to the sampled points . constant = kernels.ConstantKernel() matern = kernels.Matern(nu = 2.5) rbf = kernels.RBF() gpr_model = GPR(kernel = constant*rbf, alpha = 1e-3, n_restarts_optimizer = 20, normalize_y = False, random_state = 42) gpr_model.fit(x_sample, y_sample) . GaussianProcessRegressor(alpha=0.001, kernel=1**2 * RBF(length_scale=1), n_restarts_optimizer=20, random_state=42) . (mean_pred, stddev_pred) = gpr_model.predict(x_pts, return_std = True) gpr_model.kernel_ . 2.78**2 * RBF(length_scale=0.782) . Plot the Initial Sample . (fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB) . Run a Few Iterations and Assess . pkwargs = {&quot;num_sample&quot;: 10, &quot;num_improve&quot;: 5, &quot;generator&quot;: generator} res_ec, _ = opti.bayesian_optimization(objective, gpr_model, LCB, domain, max_iter=10, noise=noise_, prop_kwargs = pkwargs) gpr_model.fit(res_ec[&quot;X&quot;], res_ec[&quot;y&quot;]) # Incorporate final point into plots. . 10 . GaussianProcessRegressor(alpha=0.001, kernel=1**2 * RBF(length_scale=1), n_restarts_optimizer=20, random_state=42) . (fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB, num_sample_points) . Run a Few More Iterations . res_ec, _ = opti.bayesian_optimization(objective, gpr_model, LCB, domain, noise=noise_, prop_kwargs = pkwargs) gpr_model.fit(res_ec[&quot;X&quot;], res_ec[&quot;y&quot;]) # Incorporate final point into plots. . 20 . GaussianProcessRegressor(alpha=0.001, kernel=1**2 * RBF(length_scale=1), n_restarts_optimizer=20, random_state=42) . (fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB, num_sample_points) . In total the noisy estimation of the ground-truth is conducted on 30 additional points. It is evident from the plot that most of those points are near the x = (4,6) since that is the minimum value region for the function. .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2021/01/27/Bayesian_Optimization.html",
            "relUrl": "/python/machine-learning/2021/01/27/Bayesian_Optimization.html",
            "date": " • Jan 27, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Estimating prediction confidence through dropout",
            "content": "Adapted from Deep Learning online course notes from NYU. Note link | Paper about using Dropout as a Bayesian Approximation | . Another notebook which uses PyTorch dropout: Link . New paper on evidential deep learning for guided molecular property prediction | . In addition to predicting a value from a model it is also important to know the confidence in that prediction. Dropout is one way of estimating this. After multiple rounds of predictions, the mean and standard deviation in the prediction can be viewed as the prediction value and the corresponding confidence in the prediction. It is important to note that this is different from the error in the prediction. The model may have error in the prediction but could be precise in that value. It is similar to the idea of accuracy vs precision. . When done with dropout -- the weights in the NN are scale by $ frac{1}{1-r}$ to account for dropping of the weights . Type of uncertainties: Aleaotric and Epistemic uncertainty . Aleatoric uncertainty captures noise inherent in the observations | Epistemic uncertainty accounts for uncertainty in the model | . The ideal way to measure epistemic uncertainty is to train many different models, each time using a different random seed and possibly varying hyperparameters. Then use all of them for each input and see how much the predictions vary. This is very expensive to do, since it involves repeating the whole training process many times. Fortunately, we can approximate the same effect in a less expensive way: by using dropout -- effectively training a huge ensemble of different models all at once. Each training sample is evaluated with a different dropout mask, corresponding to a different random subset of the connections in the full model. Usually we only perform dropout during training and use a single averaged mask for prediction. But instead, let&#39;s use dropout for prediction too. We can compute the output for lots of different dropout masks, then see how much the predictions vary. This turns out to give a reasonable estimate of the epistemic uncertainty in the outputs . import torch from torch import nn, optim import numpy as np . import matplotlib.pyplot as plt from matplotlib.pyplot import cm %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . print(torch.cuda.device_count()) # Device configuration device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) . 0 cpu . m = 40 x = (torch.rand(m) - 0.5) * 20 #Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1) y = x * torch.sin(x) #y = 2 * torch.exp( - torch.sin( (x/2)**2 )) . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.plot(x.numpy(), y.numpy(), &#39;o&#39;) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) ax.axis(&#39;equal&#39;); . class MLP(nn.Module): def __init__(self, hidden_layers=[20, 20], droprate=0.2, activation=&#39;relu&#39;): super(MLP, self).__init__() self.model = nn.Sequential() self.model.add_module(&#39;input&#39;, nn.Linear(1, hidden_layers[0])) if activation == &#39;relu&#39;: self.model.add_module(&#39;relu0&#39;, nn.ReLU()) elif activation == &#39;tanh&#39;: self.model.add_module(&#39;tanh0&#39;, nn.Tanh()) for i in range(len(hidden_layers)-1): self.model.add_module(&#39;dropout&#39;+str(i+1), nn.Dropout(p=droprate)) self.model.add_module(&#39;hidden&#39;+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1])) if activation == &#39;relu&#39;: self.model.add_module(&#39;relu&#39;+str(i+1), nn.ReLU()) elif activation == &#39;tanh&#39;: self.model.add_module(&#39;tanh&#39;+str(i+1), nn.Tanh()) self.model.add_module(&#39;dropout&#39;+str(i+2), nn.Dropout(p=droprate)) self.model.add_module(&#39;final&#39;, nn.Linear(hidden_layers[i+1], 1)) def forward(self, x): return self.model(x) . net = MLP(hidden_layers=[200, 100, 80], droprate=0.1).to(device) #Move model to the GPU print(net) . MLP( (model): Sequential( (input): Linear(in_features=1, out_features=200, bias=True) (relu0): ReLU() (dropout1): Dropout(p=0.1, inplace=False) (hidden1): Linear(in_features=200, out_features=100, bias=True) (relu1): ReLU() (dropout2): Dropout(p=0.1, inplace=False) (hidden2): Linear(in_features=100, out_features=80, bias=True) (relu2): ReLU() (dropout3): Dropout(p=0.1, inplace=False) (final): Linear(in_features=80, out_features=1, bias=True) ) ) . criterion = nn.MSELoss() optimizer = optim.Adam(net.parameters(), lr=0.005, weight_decay=0.00001) . x_dev = x.view(-1, 1).to(device) . for epoch in range(6000): x_dev = x.view(-1, 1).to(device) y_dev = y.view(-1, 1).to(device) y_hat = net(x_dev) loss = criterion(y_hat, y_dev) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print(&#39;Epoch[{}] - Loss:{}&#39;.format(epoch, loss.item())) . Epoch[0] - Loss:12.69531536102295 Epoch[500] - Loss:1.740363359451294 Epoch[1000] - Loss:1.177356243133545 Epoch[1500] - Loss:1.9534406661987305 Epoch[2000] - Loss:1.0815060138702393 Epoch[2500] - Loss:0.5457165837287903 Epoch[3000] - Loss:0.24786725640296936 Epoch[3500] - Loss:0.38788101077079773 Epoch[4000] - Loss:0.7004671096801758 Epoch[4500] - Loss:0.4352916181087494 Epoch[5000] - Loss:0.5015718936920166 Epoch[5500] - Loss:0.44577136635780334 . Define a separate continuous vector XX . XX = torch.linspace(-11, 11, 1000) . def predict_reg(model, X, T=10): &#39;&#39;&#39; Running the model in training mode. model = torch.model: NN implemented in pytorch X = torch.tensor: Input vector T = int: number of samples run OUT: Y_hat = sample of predictions from NN model Y_eval = average prediction value from NN model &#39;&#39;&#39; model = model.train() Y_hat = list() with torch.no_grad(): for t in range(T): X_out = model(X.view(-1,1).to(device)) Y_hat.append(X_out.cpu().squeeze()) Y_hat = torch.stack(Y_hat) model = model.eval() with torch.no_grad(): X_out = model(X.view(-1,1).to(device)) Y_eval = X_out.cpu().squeeze() return Y_hat, Y_eval . %%time y_hat, y_eval = predict_reg(net, XX, T=1000) mean_y_hat = y_hat.mean(axis=0) std_y_hat = y_hat.std(axis=0) . CPU times: user 11.9 s, sys: 94 ms, total: 12 s Wall time: 3.04 s . fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(XX.numpy(), mean_y_hat.numpy(), &#39;C1&#39;, label=&#39;prediction&#39;) ax.fill_between(XX.numpy(), (mean_y_hat + std_y_hat).numpy(), (mean_y_hat - std_y_hat).numpy(), alpha=0.5, color=&#39;C2&#39;, label=&#39;confidence&#39;) ax.plot(x.numpy(), y.numpy(), &#39;oC0&#39;, zorder=1, label=&#39;ground truth&#39;) ax.plot(XX.numpy(), (XX * torch.sin(XX)).numpy(), &#39;k--&#39;, alpha=0.4, zorder=0, label=&#39;original function&#39;) ax.set_title(&#39;Plotting the NN predictions and ground truth&#39;) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) ax.axis(&#39;equal&#39;) plt.legend(loc=&#39;best&#39;, fontsize=10) . &lt;matplotlib.legend.Legend at 0x7fb0b4eb73a0&gt; . The plot above is the combination of ground truth points (blue), original function (grey) and the predicted function (orange) with corresponding confidence interval at each ML-prediction (green). It is seen that the ML model does well for the region of X in the range of original points. Another interesting observation is the uncertainity of the model prediction, which is seen to increase in the space where training points dont exist e.g. X=(5,10) and X &gt; 10 or X &lt; -10. This shows that the model is highly uncertain outside the domain of training data (extrapolation) but does a decent job within the range of training data (interpolation). .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/pytorch/machine-learning/2021/01/11/Simple_Dropout.html",
            "relUrl": "/python/pytorch/machine-learning/2021/01/11/Simple_Dropout.html",
            "date": " • Jan 11, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Solving the birthday problem using simple counting",
            "content": "This problem and discussion herein was adapted from Computational Thinking course offered by MIT. Link to course page . Question from First Course in Probability (Sheldon Ross): . Given 20 people, what is the probability that, among the 12 months in the year, there are 4 months containing exactly 2 birthdays and 4 containing exactly 3 birthdays? Link to problem . import os import time import numpy as np np.random.seed(42) . Problem setup . Rather than using a formula to estimate the probability we can setup an experiment using for-loops to mimic the trials and then count the actual number of cases where the condition is met. . month_number = np.arange(1, 13) # array of months number_of_people = 20 #Number of people considered sample = np.random.choice(month_number, size=number_of_people, replace=True, ) . Here sample is the randomized set of 20 birthdays. Each entry is a month. Length of sample is same as number of people in the experiment, 20 in this case. . Highlight cases which satisfy the constraints: . 4 months have exactly 2 birthdays | 4 months have exactly 3 birthdays | For that first create an array with frequency of birthdays in each month: . frequency = np.zeros(len(month_number)) #This is zeros frequency array for _entry in sample: _index = _entry - 1 frequency[_index] = frequency[_index] + 1 . sample . array([ 7, 4, 11, 8, 5, 7, 10, 3, 7, 11, 11, 8, 5, 4, 8, 8, 3, 6, 5, 2]) . frequency . array([0., 1., 2., 2., 3., 1., 3., 4., 0., 1., 3., 0.]) . frequency array shows how many birthday are there in a month (Jan - Dec). . Once we have that, now we have to consider the conditions given in the problem. We can either do this using for/if statement or using numpy.argwhere, where the number of months satisfying the conditions is estimated directly. . len(np.argwhere(frequency == 2)) . 2 . Now putting it all together and doing multiple trials. . In this case, I do 1,500,000 trials and count the ratio of successes to the total trials. While this process takes time, it is better than remembering permutation and combination formulae ;P . This is a variant of monte-carlo simulation. . %%time # putting it all togther _number_of_trials = 1_500_000 _success = 0 for _ in range(_number_of_trials): sample = np.random.choice(month_number, size=number_of_people, replace=True) frequency = np.zeros( len(month_number) ) for _entry in sample: if _entry in month_number: _index = _entry - 1 frequency[_index] = frequency[_index] + 1 if ( len(np.argwhere(frequency == 2)) == 4 ) and ( len(np.argwhere(frequency == 3)) == 4 ): _success = _success + 1 print(&#39;Probability of success = {}&#39;.format(_success / _number_of_trials)) . Probability of success = 0.00106 CPU times: user 3min 8s, sys: 2.08 s, total: 3min 10s Wall time: 3min 10s . This answer is very close you&#39;d get from using probablity counting and formula! .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/2021/01/10/PnCbirthday.html",
            "relUrl": "/python/data-analysis/2021/01/10/PnCbirthday.html",
            "date": " • Jan 10, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Central limit theorem",
            "content": "What is central limit theorem? . The distribution of the sum of independent samples consisting of n points drawn from an arbitrary distribution approach a normal distribution as n increases. . If the distribution of the values has a mean and standard deviation, the distribution of sum is approximately given by $ N(n mu, n sigma^2)$ . Some points to keep in mind: . The values are to be drawn independently | The values have to come from same distribution | The underlying distribution should have finite mean and variance | The rate convergence to the normal distribution depends on the skewness of the parent distribution. | . We start with some crazy distribution that has got nothing to do with a normal distribution. Sample points from that distribution with some arbitrary sample size, following which we plot the sample mean (or sample sum) on a frequency table -- repeat this lot of times (tending to infinity) we end up getting a normal distribution of sample means! . The Central Limit Theorem explains the prevalence of normal distributions in the natural world. This limit is central to the ideas of hypothesis testing and helpful for estimating confidence intervals. . Khan Academy video explaining this | . Below a simple python experiment to show this in action. . import random as rand import numpy as np from scipy import stats # High DPI rendering for mac %config InlineBackend.figure_format = &#39;retina&#39; # Plot matplotlib plots with white background: %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} . import matplotlib.pyplot as plt import seaborn as sns plot_params = { &#39;font.size&#39; : 10, &#39;axes.titlesize&#39; : 10, &#39;axes.labelsize&#39; : 10, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 10, &#39;ytick.labelsize&#39; : 10, } plt.rcParams.update(plot_params) sns.color_palette(&#39;colorblind&#39;) . from numpy.random import default_rng rng = default_rng(42) . 1. Discrete distribution . For this case let&#39;s assume we have a dice which is unfair and does not ever land on 3 and 5, and lands more on 2 and 6. We can build this skewed probability into the dice using the weights. . dice = np.arange(1,7) # Dice numbers possible probabilities = [0.2, 0.3, 0.0, 0.2, 0.0, 0.3] #Weighted probabilites for the numbers . Define a function to draw samples from the dice and calculate the mean. . def sample_draw_mean(_trials=1000, _sample_size=1): sample_mean_trials = [] # Sample a number from the distribution equal to trials for i in range(_trials): sample = rng.choice(dice, size=_sample_size, p=probabilities, replace=True) sample_mean_trials.append(np.mean(sample)) return sample_mean_trials . Drawing sample_size=1 from the distribution multiple times, i.e. equal to num_of_trials variable . num_of_trials = 1000 sample_size = 1 sns.histplot(sample_draw_mean(_trials=num_of_trials, _sample_size=sample_size), bins=len(dice), stat=&#39;density&#39;, kde=True); plt.title(&#39;Visualize sample mean distribution for {} sample drawn {} times&#39;.format(sample_size, num_of_trials), fontsize=15); . For sample size of 1, the frequency of numbers rolled by the unfair dice relates to the probability we have set above. However we can start to define samples from that distribution wherein, instead of single number we draw (for example 4). . Plotting sampling distribution of sample mean . num_of_trials = 1000 sample_size = 4 sns.histplot(sample_draw_mean(_trials=num_of_trials, _sample_size=sample_size), bins=len(dice), stat=&#39;density&#39;, kde=True); plt.title(&#39;Visualize sample mean distribution for {} sample drawn {} times&#39;.format(sample_size, num_of_trials), fontsize=15); . num_of_trials = 1000 sample_size = 20 sns.histplot(sample_draw_mean(_trials=num_of_trials, _sample_size=sample_size), bins=len(dice), stat=&#39;density&#39;, kde=True); plt.title(&#39;Visualize sample mean distribution for {} sample drawn {} times&#39;.format(sample_size, num_of_trials), fontsize=15); . As we keep plotting the frequency distribution for the sample mean it starts to approach the normal distribution! . def normal_distribution(x, mean=0, sigma=1): out = (1/np.sqrt(2 * np.pi * sigma ** 2)) * np.exp(-1/2 * ((x - mean)/sigma)**2) return(out) . num_of_trials = 1000 sample_size = 20 fig, ax = plt.subplots(1,1, figsize=(5,5)) sample_means = np.sort(sample_draw_mean(_trials=num_of_trials, _sample_size=sample_size)) # Plot histogram density sns.histplot(sample_means, bins=len(dice), stat=&#39;density&#39;, kde=False, ax=ax) # Plot normal distribution ax.plot(sample_means, normal_distribution(sample_means, np.mean(sample_means), np.std(sample_means)), color=&#39;black&#39;, linestyle=&#39;--&#39;, label=&#39;Normal Distribution&#39;) # Plot sample mean ax.axvline(np.mean(sample_means), color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=2.0, label=&#39;Sample Mean&#39;) ax.set_xlabel(&#39;Dice number&#39;) plt.title(&#39;Visualize sample mean distribution for {} sample drawn {} times&#39;.format(sample_size, num_of_trials), fontsize=15); plt.tight_layout() . 2. Continuous distibution . beta = 5.0 num_of_trials = 1000 sample_size_list = [1, 10, 100, 500] . def generate_mean_samples(_beta, _iter, _sample_size): samples_mean = [] for i in range(_iter): sample_numbers = np.random.exponential(_beta, _sample_size) samples_mean.append(np.mean(sample_numbers)) return(samples_mean) . sample_plot_list = [] for n in sample_size_list: sample_plot_list.append((n, generate_mean_samples(beta, num_of_trials, n))) . fig, ax = plt.subplots(2,2, figsize=(10,10)) ax = ax.flatten() for i, entry in enumerate(sample_plot_list): sns.histplot(entry[1], stat=&#39;density&#39;, alpha=0.6, kde=False, ax=ax[i]) ax[i].set_title(&#39;Sample size: {}&#39;.format(entry[0])) sample_mean = np.mean(entry[1]) sample_std = np.std(entry[1]) normal_x = np.sort(entry[1]) # Plot normal distribution ax[i].plot(normal_x, normal_distribution(normal_x,sample_mean,sample_std), linewidth=4.0, color=&#39;black&#39;, linestyle=&#39;--&#39;, label=&#39;Normal Distribution&#39;) # Sample mean ax[i].axvline(sample_mean, color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=4.0, label=&#39;Sample Mean&#39;) ax[i].set_xlabel(&#39;Sample Mean&#39;) plt.suptitle(r&#39;Visualize sample mean distribution for Exponential distribution $ beta$={}, Sampled {} times&#39;.format(beta, num_of_trials)); plt.legend(bbox_to_anchor=(1.04,1), loc=&quot;upper left&quot;) #plt.tight_layout() . &lt;matplotlib.legend.Legend at 0x133a00070&gt; . 3. Is it a fair coin? . Estimate coin toss probability . A coin is flipped 30 times, you get 22 heads. Find if the coin is fair or not. That is, if the probability of getting heads-tails is 50%. . This can be solved by estimating the probability of getting heads / tails provided the above condition is met. . Since we can model the coin toss process (a priori model) using Bernoulli&#39;s distribution, we will estimate the probability of 22 heads considering a fair coin. This will be our Null Hypothesis. . Null hypothesis: The null hypothesis is a model of the system based on the assumption that the apparent effect was actually due to chance. . Assuming a bernoulli distribution: . $$X_{i} sim B(p)$$ . $$ P(N_H=22) = binom nx p^{22}(1-p)^{30-22} $$ . By central limit theorem: $$ sum_{i=1}^{30}{X_{i}} sim N(30p, 30(1-p)) $$ . From maximum likelihood estimate, more detailts on MLE can be found here.: . $$ hat{p} = 0.73 $$ . Estimate 95% confidence interval: . Assuming a normal distribution: $$ mu pm 1.96 sigma $$ | . $$ 30 hat{p} pm 1.96 sqrt{ 30 * (1- hat{p}) } $$ . $$ 22 pm 1.96 sqrt{( 30 * 0.26 )} = (16.4, 27.58) $$ . rng = np.random.default_rng(42) . Define a numpy.random.choice function to simulate coin tosses. This can repeated to 30 times. . sampling_30 = rng.choice([0,1], replace=True, size=30) # we can randint(2) here as well. . np.where is used to find the entries with heads, that way for each 30 coin tosses we can estimate how many heads are there. In this case we are treating heads as 1 and tails as 0 . len(np.where(sampling_30 == 1)[0]) # or just sum the list since all we have is 1 / 0 . 15 . sum(sampling_30) . 15 . Setup the problem to perform multiple trails of 30 coin tosses, when done with the trials we will keep an account of how many of those trials had 22 heads. . heads_condition = 22 num_heads_list = [] constraint_satisy = 0 num_trials = 5000 . for _ in range(num_trials): sampling_30 = rng.choice([0,1], replace=True, size=30, p=[0.50,0.50]) # A-priori fair coin toss model number_of_heads = len(np.where(sampling_30 == 1)[0]) num_heads_list.append(number_of_heads) if number_of_heads == heads_condition: constraint_satisy = constraint_satisy + 1 num_heads_list = np.array(num_heads_list) . len(num_heads_list) . 5000 . Defining a normal distribution function from scipy or we could also use the function defined previously. . from scipy.stats import norm x = np.linspace(min(num_heads_list), max(num_heads_list)) std_norm_coin = norm(np.mean(num_heads_list), np.std(num_heads_list)) . quantiles_95_confidence = np.quantile(num_heads_list, [0.025, 0.975]) . fig, ax = plt.subplots(1,1, figsize=(8,5)) # Plot histogram density sns.histplot(num_heads_list, stat=&#39;density&#39;, kde=False, ax=ax) # Plot normal distribution ax.plot(x, std_norm_coin.pdf(x), color=&#39;black&#39;, linestyle=&#39;--&#39;, label=&#39;Normal Distribution&#39;) # Plot sample mean ax.axvline(np.mean(num_heads_list), color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=2.0, label=&#39;Sample Mean&#39;) ax.axvline(heads_condition, color=&#39;blue&#39;, linestyle=&#39;-&#39;, linewidth=2.0, label=&#39;Experiment condition&#39;) # Plot confidence interval ax.axvspan(quantiles_95_confidence[0], quantiles_95_confidence[1], alpha=0.15, color=&#39;yellow&#39;,label=&#39;95% confidence interval&#39;) ax.set_xlabel(&#39;Number of heads in 30 coin tosses&#39;) plt.title(&#39;Visualize distribution of number of heads for 30 coin tosses sampled {} times&#39;.format(num_trials), fontsize=15); plt.legend(loc=&quot;upper left&quot;) plt.tight_layout() . p-value estimate . p_value = constraint_satisy / num_trials print(p_value) . 0.0042 . Since p-value is less than 0.05, this means the coin is not fair . For most problems, we only care about the order of magnitude: if the p-value is smaller that 1/100, the effect is likely to be real; if it is greater than 1/10, probably not. If you think there is a difference between a 4.8% (significant!) and 5.2% (not significant!), you are taking it too seriously. .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/2020/12/21/Central_limit_theorem.html",
            "relUrl": "/python/data-analysis/2020/12/21/Central_limit_theorem.html",
            "date": " • Dec 21, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Relational analysis of spices used in Indian cuisine",
            "content": "Find out which relations between different Indian spices . Spices are central to Indian cuisine. What is referred to colloquially as ‘Indian’ food is made of many different sub-cuisines. As a result, there are a plethora of spices usually brought up when considering ‘Indian’ food. Knowing which spices are most frequently used can help cooks novice or seasoned to make an informed decision about spices that promise the most bang for the buck. . I use a Kaggle dataset containing 6000+ recipes from https://www.archanaskitchen.com/. Using this data as base collection of recipes representing most of the indian food, I analyze which spices occur most freqeuntly and which spices are most connected to each other. . Dataset for Indian recipe: This dataset 6000+ recipe scrapped from | Link to the dataset | . import pandas as pd import numpy as np . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . Read the dataset . food_df = pd.read_csv(&#39;./data/IndianFoodDatasetCSV.csv&#39;) . food_df.columns . Index([&#39;Srno&#39;, &#39;RecipeName&#39;, &#39;TranslatedRecipeName&#39;, &#39;Ingredients&#39;, &#39;TranslatedIngredients&#39;, &#39;PrepTimeInMins&#39;, &#39;CookTimeInMins&#39;, &#39;TotalTimeInMins&#39;, &#39;Servings&#39;, &#39;Cuisine&#39;, &#39;Course&#39;, &#39;Diet&#39;, &#39;Instructions&#39;, &#39;TranslatedInstructions&#39;, &#39;URL&#39;], dtype=&#39;object&#39;) . food_df.shape . (6871, 15) . columns_to_drop = [&#39;CookTimeInMins&#39;, &#39;Servings&#39;, &#39;Course&#39;, &#39;Diet&#39;, &#39;Instructions&#39;, &#39;TranslatedInstructions&#39;, &#39;URL&#39;] food_df = food_df.drop(columns = columns_to_drop).dropna() . cuisines_to_drop = [&#39;Mexican&#39;, &#39;Italian Recipes&#39;, &#39;Thai&#39;, &#39;Chinese&#39;, &#39;Asian&#39;, &#39;Middle Eastern&#39;, &#39;European&#39;, &#39;Arab&#39;, &#39;Japanese&#39;, &#39;Vietnamese&#39;, &#39;British&#39;, &#39;Greek&#39;, &#39;French&#39;, &#39;Mediterranean&#39;, &#39;Sri Lankan&#39;, &#39;Indonesian&#39;, &#39;African&#39;, &#39;Korean&#39;, &#39;American&#39;, &#39;Carribbean&#39;, &#39;World Breakfast&#39;, &#39;Malaysian&#39;, &#39;Dessert&#39;, &#39;Afghan&#39;, &#39;Snack&#39;, &#39;Jewish&#39;, &#39;Brunch&#39;, &#39;Lunch&#39;, &#39;Continental&#39;, &#39;Fusion&#39;] food_df = food_df.loc[ ~ food_df[&#39;Cuisine&#39;].isin(cuisines_to_drop) ] #Dropping entries in `food_df` which have non-indian cuisines . food_df.shape . (4881, 8) . food_df.head(5) . Srno RecipeName TranslatedRecipeName Ingredients TranslatedIngredients PrepTimeInMins TotalTimeInMins Cuisine . 0 1 | Masala Karela Recipe | Masala Karela Recipe | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | 15 | 45 | Indian | . 1 2 | टमाटर पुलियोगरे रेसिपी - Spicy Tomato Rice (Re... | Spicy Tomato Rice (Recipe) | 2-1/2 कप चावल - पका ले,3 टमाटर,3 छोटा चमच्च बी... | 2-1 / 2 cups rice - cooked, 3 tomatoes, 3 teas... | 5 | 15 | South Indian Recipes | . 2 3 | Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | 20 | 50 | South Indian Recipes | . 3 4 | Gongura Chicken Curry Recipe - Andhra Style Go... | Gongura Chicken Curry Recipe - Andhra Style Go... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | 15 | 45 | Andhra | . 4 5 | आंध्रा स्टाइल आलम पचड़ी रेसिपी - Adrak Chutney ... | Andhra Style Alam Pachadi Recipe - Adrak Chutn... | 1 बड़ा चमच्च चना दाल,1 बड़ा चमच्च सफ़ेद उरद दाल,2... | 1 tablespoon chana dal, 1 tablespoon white ura... | 10 | 30 | Andhra | . Drop non-english entries for consistency . def filter_english(string): try: string.encode(&#39;utf-8&#39;).decode(&#39;ascii&#39;) out = True except UnicodeDecodeError: out = False return out . df = food_df.loc[ food_df[&#39;TranslatedIngredients&#39;].apply(filter_english) ] . df.shape . (4273, 8) . df = df.reset_index() . Generate a consistent list of Indian spices for better tabulation . Next for consistent tabulation I needed a list of spices to look for. Wikipedia has a page on Indian spices which lists various spices used in Indian cuisine. I use this list to search names of spices in the recipe entries. . wiki_file_pd = pd.read_html(&#39;https://en.wikipedia.org/wiki/List_of_Indian_spices&#39;) spices_list = wiki_file_pd[0][&#39;Standard English&#39;].copy().str.lower() #some important spices to add spices_to_add = pd.Series([&#39;black salt&#39;, &#39;green chillies&#39;, &#39;chilli powder&#39;]) #some spices are too common (such as pepper) or not a spice, but a vegetable, or are otherwise corrupted (for example, #cardamom is often listed as &quot;cardamom&quot; nto specifying whether it is black or green) spices_to_drop = [&#39;black pepper&#39;, &#39;capers&#39;, &#39;chili pepper powder&#39;, &#39;cinnamon buds&#39;, &#39;citric acid&#39;, &#39;garlic&#39;, &#39;capsicum&#39;, &#39;charoli&#39;, &#39;garcinia gummi-gutta&#39;, &#39;inknut&#39;, &#39;garcinia indica&#39;, &#39;black mustard seeds/raee&#39;, &#39;cumin seed ground into balls&#39;, &#39;dried ginger&#39;, &#39;green chili pepper&#39;, &#39;long pepper&#39;, &#39;four seeds&#39;, &#39;cubeb&#39;, &#39;gum tragacanth&#39;, &#39;jakhya&#39;, &#39;licorice powder&#39;, &#39;indian bedellium tree&#39;, &#39;mango extract&#39;, &#39;coriander powder&#39;, &#39;saffron pulp&#39;, &#39;black cardamom&#39;, &#39;brown mustard seed&#39;, &#39;black cumin&#39;, &#39;panch phoron&#39;] spices_list = spices_list.loc[ ~spices_list.isin(spices_to_drop) ].append(spices_to_add).reset_index(drop=True) . spices_list . 0 alkanet root 1 amchoor 2 asafoetida 3 celery / radhuni seed 4 bay leaf, indian bay leaf 5 cinnamon 6 cloves 7 coriander seed 8 cumin seed 9 curry tree or sweet neem leaf 10 fennel seed 11 fenugreek leaf 12 fenugreek seed 13 garam masala 14 ginger 15 green cardamom 16 indian gooseberry 17 kalpasi 18 mustard seed 19 nigella seed 20 nutmeg 21 mace 22 pomegranate seed 23 poppy seed 24 saffron 25 sesame seed 26 star aniseh 27 tamarind 28 thymol/carom seed 29 turmeric 30 white pepper 31 black salt 32 green chillies 33 chilli powder dtype: object . One more step is editing the spices so that my string counter can find different versions of the same spice. . spices_list = spices_list.str.replace(&#39;amchoor&#39;, &#39;amchur/amchoor/mango extract&#39;) .replace(&#39;asafoetida&#39;, &#39;asafetida/asafoetida/hing&#39;) .replace(&#39;thymol/carom seed&#39;, &#39;ajwain/thymol/carom seed&#39;) .replace(&#39;alkanet root&#39;, &#39;alkanet/alkanet root&#39;) .replace(&#39;chilli powder&#39;, &#39;red chilli powder/chilli powder/kashmiri red chilli powder&#39;) .replace(&#39;celery / radhuni seed&#39;, &#39;celery/radhuni seed&#39;) .replace(&#39;bay leaf, indian bay leaf&#39;, &#39;bay leaf/bay leaves/tej patta&#39;) .replace(&#39;curry tree or sweet neem leaf&#39;, &#39;curry leaf/curry leaves&#39;) .replace(&#39;fenugreek leaf&#39;, &#39;fenugreek/kasoori methi&#39;) .replace(&#39;nigella seed&#39;, &#39;nigella/black cumin&#39;) .replace(&#39;ginger&#39;, &#39;dried ginger/ginger powder&#39;) .replace(&#39;cloves&#39;, &#39;cloves/laung&#39;) .replace(&#39;green cardamom&#39;, &#39;cardamom/green cardamom/black cardamom&#39;) .replace(&#39;indian gooseberry&#39;, &#39;indian gooseberry/amla&#39;) .replace(&#39;coriander seed&#39;, &#39;coriander seed/coriander powder&#39;) .replace(&#39;star aniseh&#39;, &#39;star anise&#39;) .replace(&#39;cumin seed&#39;, &#39;cumin powder/cumin seeds/cumin/jeera&#39;) . spices_list . 0 alkanet/alkanet root 1 amchur/amchoor/mango extract 2 asafetida/asafoetida/hing 3 celery/radhuni seed 4 bay leaf/bay leaves/tej patta 5 cinnamon 6 cloves/laung 7 coriander seed/coriander powder 8 cumin powder/cumin seeds/cumin/jeera 9 curry leaf/curry leaves 10 fennel seed 11 fenugreek/kasoori methi 12 fenugreek seed 13 garam masala 14 dried ginger/ginger powder 15 cardamom/green cardamom/black cardamom 16 indian gooseberry/amla 17 kalpasi 18 mustard seed 19 nigella/black cumin 20 nutmeg 21 mace 22 pomegranate seed 23 poppy seed 24 saffron 25 sesame seed 26 star anise 27 tamarind 28 ajwain/thymol/carom seed 29 turmeric 30 white pepper 31 black salt 32 green chillies 33 red chilli powder/chilli powder/kashmiri red c... dtype: object . Ingredients in the recipes . ingredients_series = df[[&#39;TranslatedRecipeName&#39;,&#39;TranslatedIngredients&#39;]] . ingredients_series . TranslatedRecipeName TranslatedIngredients . 0 Masala Karela Recipe | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | . 1 Spicy Tomato Rice (Recipe) | 2-1 / 2 cups rice - cooked, 3 tomatoes, 3 teas... | . 2 Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | . 3 Gongura Chicken Curry Recipe - Andhra Style Go... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | . 4 Andhra Style Alam Pachadi Recipe - Adrak Chutn... | 1 tablespoon chana dal, 1 tablespoon white ura... | . ... ... | ... | . 4268 One Pot Punjabi Rajma Masala Recipe In Preethi... | 1 cup Rajma (Large Kidney Beans),1 inch Ginger... | . 4269 Saffron Paneer Peda Recipe | 2 cups Paneer (Homemade Cottage Cheese) - crum... | . 4270 Quinoa Phirnee Recipe (Quinoa Milk Pudding) | 1 cup Quinoa,3/4 cup Sugar,1 teaspoon Cardamom... | . 4271 Ullikadala Pulusu Recipe | Spring Onion Curry | 150 grams Spring Onion (Bulb &amp; Greens) - chopp... | . 4272 Kashmiri Style Kokur Yakhni Recipe-Chicken Coo... | 1 kg Chicken - medium pieces,1/2 cup Mustard o... | . 4273 rows × 2 columns . spices_list_column_to_add = {i: np.zeros(len(ingredients_series)) for i in spices_list.to_list()} . ingredients_series = ingredients_series.join(pd.DataFrame(spices_list_column_to_add)) . ingredients_series . TranslatedRecipeName TranslatedIngredients alkanet/alkanet root amchur/amchoor/mango extract asafetida/asafoetida/hing celery/radhuni seed bay leaf/bay leaves/tej patta cinnamon cloves/laung coriander seed/coriander powder ... saffron sesame seed star anise tamarind ajwain/thymol/carom seed turmeric white pepper black salt green chillies red chilli powder/chilli powder/kashmiri red chilli powder . 0 Masala Karela Recipe | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 Spicy Tomato Rice (Recipe) | 2-1 / 2 cups rice - cooked, 3 tomatoes, 3 teas... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 Gongura Chicken Curry Recipe - Andhra Style Go... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 Andhra Style Alam Pachadi Recipe - Adrak Chutn... | 1 tablespoon chana dal, 1 tablespoon white ura... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4268 One Pot Punjabi Rajma Masala Recipe In Preethi... | 1 cup Rajma (Large Kidney Beans),1 inch Ginger... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4269 Saffron Paneer Peda Recipe | 2 cups Paneer (Homemade Cottage Cheese) - crum... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4270 Quinoa Phirnee Recipe (Quinoa Milk Pudding) | 1 cup Quinoa,3/4 cup Sugar,1 teaspoon Cardamom... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4271 Ullikadala Pulusu Recipe | Spring Onion Curry | 150 grams Spring Onion (Bulb &amp; Greens) - chopp... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4272 Kashmiri Style Kokur Yakhni Recipe-Chicken Coo... | 1 kg Chicken - medium pieces,1/2 cup Mustard o... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4273 rows × 36 columns . Using the spice_list to find spice name in the recipe . I used regular expression to search for spice names in the entries . import re def search_spice(ingredient_string, spice_string): &#39;&#39;&#39; Check if a spice exists in the list of ingredients for a recipe &#39;&#39;&#39; spice_list = spice_string.split(&#39;/&#39;) for _spice in spice_list: if re.search(_spice.lower(), ingredient_string.lower()): return True break . for row, values in ingredients_series.iterrows(): for spice_entry in spices_list: if search_spice(values[&#39;TranslatedIngredients&#39;], spice_entry): ingredients_series.loc[row, spice_entry] = 1 else: ingredients_series.loc[row, spice_entry] = 0 . food_spice_mix = ingredients_series.drop([&#39;TranslatedIngredients&#39;], axis=1).reset_index(drop=True) . food_spice_mix.rename(columns={&#39;amchur/amchoor/mango extract&#39;:&#39;amchoor&#39;, &#39;asafetida/asafoetida/hing&#39;: &#39;asafoetida&#39;, &#39;ajwain/thymol/carom seed&#39;: &#39;ajwain&#39;, &#39;alkanet/alkanet root&#39;: &#39;alkanet root&#39;, &#39;red chilli powder/chilli powder/kashmiri red chilli powder&#39;: &#39;chilli powder&#39;, &#39;celery/radhuni seed&#39;: &#39;celery seeds&#39;, &#39;bay leaf/bay leaves/tej patta&#39;: &#39;bay leaf&#39;, &#39;curry leaf/curry leaves&#39;: &#39;curry leaves&#39;, &#39;fenugreek/kasoori methi&#39;: &#39;fenugreek leaf&#39;, &#39;nigella/black cumin&#39;: &#39;nigella seed&#39;, &#39;ginger&#39;: &#39;dried ginger&#39;, &#39;cloves/laung&#39;: &#39;cloves&#39;, &#39;cardamom/green cardamom/black cardamom&#39;: &#39;cardamom&#39;, &#39;indian gooseberry/amla&#39;: &#39;indian gooseberry&#39;, &#39;coriander seed/coriander powder&#39;: &#39;coriander seeds/powder&#39;, &#39;cumin powder/cumin seeds/cumin/jeera&#39;: &#39;cumin seeds/powder&#39;, &#39;dried ginger/ginger powder&#39;: &#39;ginger powder&#39;}, inplace=True) . food_spice_mix.columns . Index([&#39;TranslatedRecipeName&#39;, &#39;alkanet root&#39;, &#39;amchoor&#39;, &#39;asafoetida&#39;, &#39;celery seeds&#39;, &#39;bay leaf&#39;, &#39;cinnamon&#39;, &#39;cloves&#39;, &#39;coriander seeds/powder&#39;, &#39;cumin seeds/powder&#39;, &#39;curry leaves&#39;, &#39;fennel seed&#39;, &#39;fenugreek leaf&#39;, &#39;fenugreek seed&#39;, &#39;garam masala&#39;, &#39;ginger powder&#39;, &#39;cardamom&#39;, &#39;indian gooseberry&#39;, &#39;kalpasi&#39;, &#39;mustard seed&#39;, &#39;nigella seed&#39;, &#39;nutmeg&#39;, &#39;mace&#39;, &#39;pomegranate seed&#39;, &#39;poppy seed&#39;, &#39;saffron&#39;, &#39;sesame seed&#39;, &#39;star anise&#39;, &#39;tamarind&#39;, &#39;ajwain&#39;, &#39;turmeric&#39;, &#39;white pepper&#39;, &#39;black salt&#39;, &#39;green chillies&#39;, &#39;chilli powder&#39;], dtype=&#39;object&#39;) . food_spice_mix = food_spice_mix.sort_index(axis=1) . Generating a spice adjacency matrix . num_spice = len(spices_list) spice_col_name = [i for i in food_spice_mix.columns[1:].to_list()] spice_adj = pd.DataFrame(np.zeros(shape=(len(spices_list),len(spices_list))), columns= spice_col_name, index=spice_col_name) spice_adj_freq = pd.DataFrame(np.zeros(shape=(len(spices_list),len(spices_list))), columns= spice_col_name, index=spice_col_name) . for row, value in food_spice_mix.iterrows(): for i in spice_col_name: for j in spice_col_name: if (value[i] == 1) &amp; (value[j] == 1): spice_adj_freq.loc[i,j] += 1 spice_adj.loc[i,j] = 1 . Normalize the spice occurance frequency with the total entries in the main dataset . spice_adj_freq = spice_adj_freq / len(food_spice_mix) * 100 . spice_adj_freq.round(2) . ajwain alkanet root amchoor asafoetida bay leaf black salt cardamom celery seeds chilli powder cinnamon ... nigella seed nutmeg pomegranate seed poppy seed saffron sesame seed star anise tamarind turmeric white pepper . ajwain 5.22 | 0.00 | 0.70 | 1.45 | 0.98 | 0.07 | 1.05 | 0.00 | 3.49 | 1.33 | ... | 0.16 | 0.16 | 0.09 | 0.23 | 0.21 | 0.40 | 0.23 | 0.37 | 2.97 | 0.00 | . alkanet root 0.00 | 0.07 | 0.00 | 0.07 | 0.05 | 0.00 | 0.07 | 0.00 | 0.07 | 0.07 | ... | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . amchoor 0.70 | 0.00 | 4.98 | 1.54 | 0.37 | 0.21 | 0.35 | 0.02 | 3.86 | 0.47 | ... | 0.35 | 0.00 | 0.14 | 0.02 | 0.05 | 0.26 | 0.05 | 0.33 | 3.51 | 0.02 | . asafoetida 1.45 | 0.07 | 1.54 | 24.60 | 1.33 | 0.30 | 1.61 | 0.21 | 9.69 | 1.87 | ... | 0.30 | 0.14 | 0.09 | 0.54 | 0.28 | 1.57 | 0.26 | 5.17 | 15.05 | 0.02 | . bay leaf 0.98 | 0.05 | 0.37 | 1.33 | 10.70 | 0.09 | 6.04 | 0.07 | 6.72 | 6.65 | ... | 0.26 | 0.61 | 0.12 | 0.75 | 0.66 | 0.16 | 1.10 | 0.26 | 7.54 | 0.07 | . black salt 0.07 | 0.00 | 0.21 | 0.30 | 0.09 | 1.64 | 0.09 | 0.02 | 0.61 | 0.09 | ... | 0.02 | 0.00 | 0.09 | 0.00 | 0.09 | 0.02 | 0.02 | 0.28 | 0.33 | 0.00 | . cardamom 1.05 | 0.07 | 0.35 | 1.61 | 6.04 | 0.09 | 17.79 | 0.14 | 6.13 | 7.98 | ... | 0.07 | 1.08 | 0.23 | 1.47 | 3.14 | 0.35 | 1.29 | 0.42 | 6.60 | 0.07 | . celery seeds 0.00 | 0.00 | 0.02 | 0.21 | 0.07 | 0.02 | 0.14 | 0.80 | 0.37 | 0.19 | ... | 0.00 | 0.09 | 0.00 | 0.00 | 0.02 | 0.00 | 0.02 | 0.00 | 0.51 | 0.00 | . chilli powder 3.49 | 0.07 | 3.86 | 9.69 | 6.72 | 0.61 | 6.13 | 0.37 | 37.96 | 7.04 | ... | 0.84 | 0.44 | 0.47 | 1.38 | 0.68 | 1.59 | 0.82 | 3.58 | 28.39 | 0.05 | . cinnamon 1.33 | 0.07 | 0.47 | 1.87 | 6.65 | 0.09 | 7.98 | 0.19 | 7.04 | 13.13 | ... | 0.14 | 0.96 | 0.19 | 1.40 | 0.84 | 0.23 | 1.38 | 0.89 | 8.10 | 0.07 | . cloves 2.01 | 0.07 | 1.43 | 5.05 | 6.58 | 0.16 | 7.65 | 0.19 | 14.14 | 9.29 | ... | 0.35 | 0.70 | 0.23 | 1.59 | 0.80 | 0.96 | 1.43 | 3.93 | 17.29 | 0.02 | . coriander seeds/powder 1.76 | 0.00 | 2.41 | 5.13 | 4.91 | 0.26 | 4.31 | 0.47 | 16.26 | 4.35 | ... | 0.35 | 0.30 | 0.28 | 0.70 | 0.33 | 0.56 | 0.59 | 1.85 | 16.78 | 0.00 | . cumin seeds/powder 2.15 | 0.00 | 3.28 | 13.27 | 5.87 | 1.19 | 5.20 | 0.35 | 21.55 | 6.67 | ... | 0.89 | 0.54 | 0.51 | 1.24 | 0.54 | 1.92 | 0.91 | 5.87 | 27.45 | 0.07 | . curry leaves 0.61 | 0.00 | 0.44 | 12.54 | 1.01 | 0.12 | 1.17 | 0.05 | 8.64 | 2.04 | ... | 0.16 | 0.07 | 0.07 | 0.66 | 0.02 | 1.73 | 0.28 | 7.33 | 16.52 | 0.05 | . fennel seed 0.75 | 0.05 | 0.82 | 1.68 | 1.45 | 0.16 | 1.99 | 0.07 | 3.77 | 2.46 | ... | 0.70 | 0.26 | 0.09 | 0.91 | 0.42 | 0.37 | 0.75 | 0.82 | 4.21 | 0.00 | . fenugreek leaf 0.87 | 0.00 | 0.80 | 4.54 | 1.64 | 0.12 | 1.52 | 0.12 | 6.39 | 1.73 | ... | 0.70 | 0.02 | 0.16 | 0.37 | 0.07 | 0.89 | 0.26 | 3.23 | 8.71 | 0.00 | . fenugreek seed 0.14 | 0.00 | 0.47 | 3.16 | 0.35 | 0.07 | 0.37 | 0.07 | 2.36 | 0.61 | ... | 0.61 | 0.00 | 0.05 | 0.30 | 0.02 | 0.59 | 0.07 | 2.93 | 4.52 | 0.00 | . garam masala 1.43 | 0.00 | 2.29 | 3.18 | 4.40 | 0.26 | 3.98 | 0.28 | 14.07 | 4.00 | ... | 0.33 | 0.14 | 0.30 | 0.63 | 0.49 | 0.42 | 0.44 | 0.49 | 13.25 | 0.00 | . ginger powder 0.21 | 0.02 | 0.05 | 0.44 | 0.35 | 0.05 | 0.84 | 0.02 | 0.59 | 0.49 | ... | 0.00 | 0.19 | 0.02 | 0.02 | 0.12 | 0.07 | 0.00 | 0.07 | 0.47 | 0.00 | . green chillies 1.80 | 0.00 | 1.59 | 7.61 | 4.61 | 0.28 | 3.96 | 0.21 | 12.22 | 4.84 | ... | 0.77 | 0.19 | 0.33 | 1.19 | 0.33 | 1.08 | 0.66 | 2.36 | 17.11 | 0.07 | . indian gooseberry 0.05 | 0.00 | 0.00 | 0.30 | 0.00 | 0.02 | 0.00 | 0.02 | 0.16 | 0.00 | ... | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.00 | 0.05 | 0.21 | 0.00 | . kalpasi 0.02 | 0.00 | 0.00 | 0.02 | 0.02 | 0.00 | 0.07 | 0.00 | 0.02 | 0.07 | ... | 0.00 | 0.02 | 0.00 | 0.07 | 0.00 | 0.00 | 0.05 | 0.00 | 0.00 | 0.00 | . mace 0.37 | 0.00 | 0.02 | 0.09 | 0.82 | 0.00 | 1.08 | 0.05 | 0.80 | 1.05 | ... | 0.00 | 0.49 | 0.02 | 0.26 | 0.14 | 0.00 | 0.30 | 0.00 | 0.89 | 0.00 | . mustard seed 0.75 | 0.00 | 0.87 | 12.71 | 1.15 | 0.07 | 0.73 | 0.02 | 8.03 | 1.59 | ... | 0.70 | 0.09 | 0.07 | 0.80 | 0.05 | 1.64 | 0.28 | 6.74 | 15.35 | 0.00 | . nigella seed 0.16 | 0.00 | 0.35 | 0.30 | 0.26 | 0.02 | 0.07 | 0.00 | 0.84 | 0.14 | ... | 1.66 | 0.00 | 0.02 | 0.16 | 0.02 | 0.07 | 0.02 | 0.07 | 1.26 | 0.02 | . nutmeg 0.16 | 0.00 | 0.00 | 0.14 | 0.61 | 0.00 | 1.08 | 0.09 | 0.44 | 0.96 | ... | 0.00 | 1.52 | 0.00 | 0.30 | 0.23 | 0.07 | 0.28 | 0.05 | 0.56 | 0.00 | . pomegranate seed 0.09 | 0.00 | 0.14 | 0.09 | 0.12 | 0.09 | 0.23 | 0.00 | 0.47 | 0.19 | ... | 0.02 | 0.00 | 0.77 | 0.05 | 0.07 | 0.02 | 0.02 | 0.07 | 0.33 | 0.00 | . poppy seed 0.23 | 0.00 | 0.02 | 0.54 | 0.75 | 0.00 | 1.47 | 0.00 | 1.38 | 1.40 | ... | 0.16 | 0.30 | 0.05 | 3.25 | 0.19 | 0.26 | 0.37 | 0.42 | 1.80 | 0.02 | . saffron 0.21 | 0.00 | 0.05 | 0.28 | 0.66 | 0.09 | 3.14 | 0.02 | 0.68 | 0.84 | ... | 0.02 | 0.23 | 0.07 | 0.19 | 4.03 | 0.07 | 0.14 | 0.00 | 0.59 | 0.00 | . sesame seed 0.40 | 0.00 | 0.26 | 1.57 | 0.16 | 0.02 | 0.35 | 0.00 | 1.59 | 0.23 | ... | 0.07 | 0.07 | 0.02 | 0.26 | 0.07 | 4.19 | 0.07 | 0.82 | 1.61 | 0.00 | . star anise 0.23 | 0.00 | 0.05 | 0.26 | 1.10 | 0.02 | 1.29 | 0.02 | 0.82 | 1.38 | ... | 0.02 | 0.28 | 0.02 | 0.37 | 0.14 | 0.07 | 1.73 | 0.14 | 0.89 | 0.00 | . tamarind 0.37 | 0.00 | 0.33 | 5.17 | 0.26 | 0.28 | 0.42 | 0.00 | 3.58 | 0.89 | ... | 0.07 | 0.05 | 0.07 | 0.42 | 0.00 | 0.82 | 0.14 | 11.96 | 7.21 | 0.00 | . turmeric 2.97 | 0.00 | 3.51 | 15.05 | 7.54 | 0.33 | 6.60 | 0.51 | 28.39 | 8.10 | ... | 1.26 | 0.56 | 0.33 | 1.80 | 0.59 | 1.61 | 0.89 | 7.21 | 48.47 | 0.05 | . white pepper 0.00 | 0.00 | 0.02 | 0.02 | 0.07 | 0.00 | 0.07 | 0.00 | 0.05 | 0.07 | ... | 0.02 | 0.00 | 0.00 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.16 | . 34 rows × 34 columns . temp_name = [i.title() for i in spice_adj_freq.index.to_list()] spice_adj_freq[&#39;Plot_name&#39;] = temp_name . spice_adj_freq = spice_adj_freq.set_index(&#39;Plot_name&#39;) . spice_adj_freq.columns = temp_name . spice_adj_freq . Ajwain Alkanet Root Amchoor Asafoetida Bay Leaf Black Salt Cardamom Celery Seeds Chilli Powder Cinnamon ... Nigella Seed Nutmeg Pomegranate Seed Poppy Seed Saffron Sesame Seed Star Anise Tamarind Turmeric White Pepper . Plot_name . Ajwain 5.218816 | 0.000000 | 0.702083 | 1.450971 | 0.982916 | 0.070208 | 1.053124 | 0.000000 | 3.487011 | 1.333957 | ... | 0.163819 | 0.163819 | 0.093611 | 0.234028 | 0.210625 | 0.397847 | 0.234028 | 0.374444 | 2.972151 | 0.000000 | . Alkanet Root 0.000000 | 0.070208 | 0.000000 | 0.070208 | 0.046806 | 0.000000 | 0.070208 | 0.000000 | 0.070208 | 0.070208 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . Amchoor 0.702083 | 0.000000 | 4.984788 | 1.544582 | 0.374444 | 0.210625 | 0.351041 | 0.023403 | 3.861456 | 0.468055 | ... | 0.351041 | 0.000000 | 0.140417 | 0.023403 | 0.046806 | 0.257430 | 0.046806 | 0.327639 | 3.510414 | 0.023403 | . Asafoetida 1.450971 | 0.070208 | 1.544582 | 24.596302 | 1.333957 | 0.304236 | 1.614791 | 0.210625 | 9.688743 | 1.872221 | ... | 0.304236 | 0.140417 | 0.093611 | 0.538264 | 0.280833 | 1.567985 | 0.257430 | 5.172010 | 15.047976 | 0.023403 | . Bay Leaf 0.982916 | 0.046806 | 0.374444 | 1.333957 | 10.695062 | 0.093611 | 6.037912 | 0.070208 | 6.716593 | 6.646384 | ... | 0.257430 | 0.608472 | 0.117014 | 0.748888 | 0.655277 | 0.163819 | 1.099930 | 0.257430 | 7.535689 | 0.070208 | . Black Salt 0.070208 | 0.000000 | 0.210625 | 0.304236 | 0.093611 | 1.638193 | 0.093611 | 0.023403 | 0.608472 | 0.093611 | ... | 0.023403 | 0.000000 | 0.093611 | 0.000000 | 0.093611 | 0.023403 | 0.023403 | 0.280833 | 0.327639 | 0.000000 | . Cardamom 1.053124 | 0.070208 | 0.351041 | 1.614791 | 6.037912 | 0.093611 | 17.786099 | 0.140417 | 6.131524 | 7.980342 | ... | 0.070208 | 1.076527 | 0.234028 | 1.474374 | 3.135970 | 0.351041 | 1.287152 | 0.421250 | 6.599579 | 0.070208 | . Celery Seeds 0.000000 | 0.000000 | 0.023403 | 0.210625 | 0.070208 | 0.023403 | 0.140417 | 0.795694 | 0.374444 | 0.187222 | ... | 0.000000 | 0.093611 | 0.000000 | 0.000000 | 0.023403 | 0.000000 | 0.023403 | 0.000000 | 0.514861 | 0.000000 | . Chilli Powder 3.487011 | 0.070208 | 3.861456 | 9.688743 | 6.716593 | 0.608472 | 6.131524 | 0.374444 | 37.959279 | 7.044231 | ... | 0.842499 | 0.444652 | 0.468055 | 1.380763 | 0.678680 | 1.591388 | 0.819097 | 3.580623 | 28.387550 | 0.046806 | . Cinnamon 1.333957 | 0.070208 | 0.468055 | 1.872221 | 6.646384 | 0.093611 | 7.980342 | 0.187222 | 7.044231 | 13.128949 | ... | 0.140417 | 0.959513 | 0.187222 | 1.404166 | 0.842499 | 0.234028 | 1.380763 | 0.889305 | 8.097355 | 0.070208 | . Cloves 2.012637 | 0.070208 | 1.427568 | 5.054996 | 6.576176 | 0.163819 | 7.652703 | 0.187222 | 14.135268 | 9.290896 | ... | 0.351041 | 0.702083 | 0.234028 | 1.591388 | 0.795694 | 0.959513 | 1.427568 | 3.931664 | 17.294641 | 0.023403 | . Coriander Seeds/Powder 1.755207 | 0.000000 | 2.410484 | 5.125205 | 4.914580 | 0.257430 | 4.306108 | 0.468055 | 16.264919 | 4.352914 | ... | 0.351041 | 0.304236 | 0.280833 | 0.702083 | 0.327639 | 0.561666 | 0.585069 | 1.848818 | 16.779780 | 0.000000 | . Cumin Seeds/Powder 2.153054 | 0.000000 | 3.276387 | 13.269366 | 5.874093 | 1.193541 | 5.195413 | 0.351041 | 21.553943 | 6.669787 | ... | 0.889305 | 0.538264 | 0.514861 | 1.240346 | 0.538264 | 1.919026 | 0.912708 | 5.874093 | 27.451439 | 0.070208 | . Curry Leaves 0.608472 | 0.000000 | 0.444652 | 12.543880 | 1.006319 | 0.117014 | 1.170138 | 0.046806 | 8.635619 | 2.036040 | ... | 0.163819 | 0.070208 | 0.070208 | 0.655277 | 0.023403 | 1.731804 | 0.280833 | 7.325064 | 16.522350 | 0.046806 | . Fennel Seed 0.748888 | 0.046806 | 0.819097 | 1.684999 | 1.450971 | 0.163819 | 1.989235 | 0.070208 | 3.767845 | 2.457290 | ... | 0.702083 | 0.257430 | 0.093611 | 0.912708 | 0.421250 | 0.374444 | 0.748888 | 0.819097 | 4.212497 | 0.000000 | . Fenugreek Leaf 0.865902 | 0.000000 | 0.795694 | 4.540136 | 1.638193 | 0.117014 | 1.521179 | 0.117014 | 6.388954 | 1.731804 | ... | 0.702083 | 0.023403 | 0.163819 | 0.374444 | 0.070208 | 0.889305 | 0.257430 | 3.229581 | 8.705827 | 0.000000 | . Fenugreek Seed 0.140417 | 0.000000 | 0.468055 | 3.159373 | 0.351041 | 0.070208 | 0.374444 | 0.070208 | 2.363679 | 0.608472 | ... | 0.608472 | 0.000000 | 0.046806 | 0.304236 | 0.023403 | 0.585069 | 0.070208 | 2.925345 | 4.516733 | 0.000000 | . Garam Masala 1.427568 | 0.000000 | 2.293471 | 3.182776 | 4.399719 | 0.257430 | 3.978469 | 0.280833 | 14.065060 | 4.001872 | ... | 0.327639 | 0.140417 | 0.304236 | 0.631875 | 0.491458 | 0.421250 | 0.444652 | 0.491458 | 13.245963 | 0.000000 | . Ginger Powder 0.210625 | 0.023403 | 0.046806 | 0.444652 | 0.351041 | 0.046806 | 0.842499 | 0.023403 | 0.585069 | 0.491458 | ... | 0.000000 | 0.187222 | 0.023403 | 0.023403 | 0.117014 | 0.070208 | 0.000000 | 0.070208 | 0.468055 | 0.000000 | . Green Chillies 1.802013 | 0.000000 | 1.591388 | 7.605897 | 4.610344 | 0.280833 | 3.955067 | 0.210625 | 12.216242 | 4.844372 | ... | 0.772291 | 0.187222 | 0.327639 | 1.193541 | 0.327639 | 1.076527 | 0.655277 | 2.363679 | 17.107419 | 0.070208 | . Indian Gooseberry 0.046806 | 0.000000 | 0.000000 | 0.304236 | 0.000000 | 0.023403 | 0.000000 | 0.023403 | 0.163819 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.023403 | 0.000000 | 0.046806 | 0.210625 | 0.000000 | . Kalpasi 0.023403 | 0.000000 | 0.000000 | 0.023403 | 0.023403 | 0.000000 | 0.070208 | 0.000000 | 0.023403 | 0.070208 | ... | 0.000000 | 0.023403 | 0.000000 | 0.070208 | 0.000000 | 0.000000 | 0.046806 | 0.000000 | 0.000000 | 0.000000 | . Mace 0.374444 | 0.000000 | 0.023403 | 0.093611 | 0.819097 | 0.000000 | 1.076527 | 0.046806 | 0.795694 | 1.053124 | ... | 0.000000 | 0.491458 | 0.023403 | 0.257430 | 0.140417 | 0.000000 | 0.304236 | 0.000000 | 0.889305 | 0.000000 | . Mustard Seed 0.748888 | 0.000000 | 0.865902 | 12.707700 | 1.146735 | 0.070208 | 0.725486 | 0.023403 | 8.027147 | 1.591388 | ... | 0.702083 | 0.093611 | 0.070208 | 0.795694 | 0.046806 | 1.638193 | 0.280833 | 6.739995 | 15.352212 | 0.000000 | . Nigella Seed 0.163819 | 0.000000 | 0.351041 | 0.304236 | 0.257430 | 0.023403 | 0.070208 | 0.000000 | 0.842499 | 0.140417 | ... | 1.661596 | 0.000000 | 0.023403 | 0.163819 | 0.023403 | 0.070208 | 0.023403 | 0.070208 | 1.263749 | 0.023403 | . Nutmeg 0.163819 | 0.000000 | 0.000000 | 0.140417 | 0.608472 | 0.000000 | 1.076527 | 0.093611 | 0.444652 | 0.959513 | ... | 0.000000 | 1.521179 | 0.000000 | 0.304236 | 0.234028 | 0.070208 | 0.280833 | 0.046806 | 0.561666 | 0.000000 | . Pomegranate Seed 0.093611 | 0.000000 | 0.140417 | 0.093611 | 0.117014 | 0.093611 | 0.234028 | 0.000000 | 0.468055 | 0.187222 | ... | 0.023403 | 0.000000 | 0.772291 | 0.046806 | 0.070208 | 0.023403 | 0.023403 | 0.070208 | 0.327639 | 0.000000 | . Poppy Seed 0.234028 | 0.000000 | 0.023403 | 0.538264 | 0.748888 | 0.000000 | 1.474374 | 0.000000 | 1.380763 | 1.404166 | ... | 0.163819 | 0.304236 | 0.046806 | 3.252984 | 0.187222 | 0.257430 | 0.374444 | 0.421250 | 1.802013 | 0.023403 | . Saffron 0.210625 | 0.000000 | 0.046806 | 0.280833 | 0.655277 | 0.093611 | 3.135970 | 0.023403 | 0.678680 | 0.842499 | ... | 0.023403 | 0.234028 | 0.070208 | 0.187222 | 4.025275 | 0.070208 | 0.140417 | 0.000000 | 0.585069 | 0.000000 | . Sesame Seed 0.397847 | 0.000000 | 0.257430 | 1.567985 | 0.163819 | 0.023403 | 0.351041 | 0.000000 | 1.591388 | 0.234028 | ... | 0.070208 | 0.070208 | 0.023403 | 0.257430 | 0.070208 | 4.189094 | 0.070208 | 0.819097 | 1.614791 | 0.000000 | . Star Anise 0.234028 | 0.000000 | 0.046806 | 0.257430 | 1.099930 | 0.023403 | 1.287152 | 0.023403 | 0.819097 | 1.380763 | ... | 0.023403 | 0.280833 | 0.023403 | 0.374444 | 0.140417 | 0.070208 | 1.731804 | 0.140417 | 0.889305 | 0.000000 | . Tamarind 0.374444 | 0.000000 | 0.327639 | 5.172010 | 0.257430 | 0.280833 | 0.421250 | 0.000000 | 3.580623 | 0.889305 | ... | 0.070208 | 0.046806 | 0.070208 | 0.421250 | 0.000000 | 0.819097 | 0.140417 | 11.958811 | 7.208051 | 0.000000 | . Turmeric 2.972151 | 0.000000 | 3.510414 | 15.047976 | 7.535689 | 0.327639 | 6.599579 | 0.514861 | 28.387550 | 8.097355 | ... | 1.263749 | 0.561666 | 0.327639 | 1.802013 | 0.585069 | 1.614791 | 0.889305 | 7.208051 | 48.467119 | 0.046806 | . White Pepper 0.000000 | 0.000000 | 0.023403 | 0.023403 | 0.070208 | 0.000000 | 0.070208 | 0.000000 | 0.046806 | 0.070208 | ... | 0.023403 | 0.000000 | 0.000000 | 0.023403 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.046806 | 0.163819 | . 34 rows × 34 columns . fig, ax = plt.subplots(1,1, figsize=(10,10)) sns.heatmap(spice_adj_freq.round(2).corr(), ax=ax) #plt.savefig(&quot;heatmap.png&quot;, format=&quot;PNG&quot;, dpi=300) . &lt;AxesSubplot:&gt; . Using frequency adjacency matrix we can plot a heatmap showing the pair-wise occurence for a given pair of spices. The idea with such an analysis is that if we can check the variation of Spice 1 with all the other spices in the list and compare that to Spice 2’s variation with all the other spices in the list, if spice 1 and spice 2 should have similar variation. . This map itself is quite interesting. The color intensity of each title shows the frequency that pair of spice occurred together in a recipe. Brighter the color higher their occurence together. . Some prominent spice pairs which show similarity are: . Curry leaves and Mustard seeds . | Tumeric and Chilli Powder . | . Some pair of spices never occur together: . Saffron and Fenugreek seeds . | Nutmeg and Mustard Seeds . | . Those who cook or know indian recipes would see that these pairs make sense and thereby validate the correlation seen from corpus of Indian recipes. . With that analysis, we can go a step further and analyze this information in form of a circular network graph. Using this method of plotting, we can see the interactions between different spices. . Creating network . import networkx as nx . nodes_data = [(i, {&#39;count&#39;:spice_adj_freq.loc[i, i]}) for i in temp_name] . binary_int = [] for i in temp_name: binary_int.append((i, spice_adj_freq.loc[i].sort_values(ascending=False).index[1])) . spice_dict = {i : spice_adj_freq.loc[i, i] for i in temp_name } . spice_dict . {&#39;Ajwain&#39;: 5.218815820266792, &#39;Alkanet Root&#39;: 0.07020828457758016, &#39;Amchoor&#39;: 4.984788205008191, &#39;Asafoetida&#39;: 24.596302363678916, &#39;Bay Leaf&#39;: 10.695062017318044, &#39;Black Salt&#39;: 1.6381933068102035, &#39;Cardamom&#39;: 17.78609875965364, &#39;Celery Seeds&#39;: 0.7956938918792418, &#39;Chilli Powder&#39;: 37.959279194945005, &#39;Cinnamon&#39;: 13.128949216007488, &#39;Cloves&#39;: 28.34074420781652, &#39;Coriander Seeds/Powder&#39;: 20.336999765972386, &#39;Cumin Seeds/Powder&#39;: 43.59934472267727, &#39;Curry Leaves&#39;: 27.615258600514856, &#39;Fennel Seed&#39;: 7.208050549964897, &#39;Fenugreek Leaf&#39;: 12.965129885326467, &#39;Fenugreek Seed&#39;: 7.208050549964897, &#39;Garam Masala&#39;: 18.34776503627428, &#39;Ginger Powder&#39;: 1.357360168499883, &#39;Green Chillies&#39;: 29.815118183945703, &#39;Indian Gooseberry&#39;: 0.3978469459396209, &#39;Kalpasi&#39;: 0.07020828457758016, &#39;Mace&#39;: 1.2403463608705827, &#39;Mustard Seed&#39;: 24.315469225368595, &#39;Nigella Seed&#39;: 1.6615960683360638, &#39;Nutmeg&#39;: 1.5211794991809033, &#39;Pomegranate Seed&#39;: 0.7722911303533817, &#39;Poppy Seed&#39;: 3.2529838520945473, &#39;Saffron&#39;: 4.025274982447929, &#39;Sesame Seed&#39;: 4.189094313128949, &#39;Star Anise&#39;: 1.7318043529136438, &#39;Tamarind&#39;: 11.958811139714488, &#39;Turmeric&#39;: 48.46711912005617, &#39;White Pepper&#39;: 0.16381933068102036} . edges_data = [] for i in temp_name: for j in temp_name: if i != j: if spice_adj_freq.loc[i,j] != 0.0: edges_data.append((i, j, {&#39;weight&#39;:spice_adj_freq.loc[i,j], &#39;distance&#39;:1})) . #BUILD THE INITIAL FULL GRAPH G=nx.Graph() G.add_nodes_from(nodes_data) G.add_edges_from(edges_data) . print(nx.info(G)) . Name: Type: Graph Number of nodes: 34 Number of edges: 471 Average degree: 27.7059 . deg_l = {i:G.degree(i) for i in temp_name} . highest_centrality_node = max(deg_l.items(), key=lambda x: x[1])[0] . highest_centrality_node . &#39;Asafoetida&#39; . n = len(nodes_data) edges = G.edges() weights = [G[u][v][&#39;weight&#39;] for u,v in edges] w_arr = np.array(weights) norm_weight = (w_arr - w_arr.min())/(w_arr.max() - w_arr.min()) angle = [] angle_dict = {} node_list = sorted(G.nodes()) for i, node in zip(np.arange(n),node_list): theta = 2.0*np.pi*i/n angle.append((np.cos(theta),np.sin(theta))) angle_dict[node] = theta pos = {} for node_i, node in enumerate(node_list): pos[node] = angle[node_i] fig, ax = plt.subplots(figsize=(20,20)) margin=0.33 fig.subplots_adjust(margin, margin, 1.-margin, 1.-margin) ax.axis(&#39;equal&#39;) nx.draw(G,pos=pos,with_labels=False, node_size=[spice_dict[k]*20 for k in spice_dict], width=norm_weight*2.0, node_color=np.arange(n), cmap=plt.cm.viridis, ax=ax) description = nx.draw_networkx_labels(G,pos) r = fig.canvas.get_renderer() trans = plt.gca().transData.inverted() for node, t in description.items(): bb = t.get_window_extent(renderer=r) bbdata = bb.transformed(trans) radius = 1.1+bbdata.width/2 position = (radius*np.cos(angle_dict[node]),radius* np.sin(angle_dict[node])) t.set_position(position) t.set_rotation(angle_dict[node]*360.0/(2.0*np.pi)) t.set_clip_on(False) #plt.savefig(&quot;Graph.png&quot;, format=&quot;PNG&quot;, dpi=300) . Finally a networkx circular graph is made where each node is a spice entry. Each edge between a pair of spice is a connection provided those two spices are found together in a recipe. The size of the node is the frequency of that spice to occur in all of 6000 food recipes. The thickness of the edge connecting a give spice-pair is the normalized frequency that pair occured among 6000 recipes. . Representing the analysis this way we find few key takeaways: . Tumeric, Mustard Seeds, Chilli Powder, Corriander Seeds, Cumin Seeds, Curry Leaves, Green Chillies, Asafoetida are the key spices in the Indian cuisine. . | Most recipes use Tumeric + Chilli Powder + Cumin Powder (Seeds) in them. . | .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/exploratory-data-analysis/data-visualization/web-scrapping/2020/12/09/food_relations.html",
            "relUrl": "/python/exploratory-data-analysis/data-visualization/web-scrapping/2020/12/09/food_relations.html",
            "date": " • Dec 9, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Network analysis hands-on",
            "content": "Good resource to learn basics of Network science: . http://networksciencebook.com/chapter/0 | . Recent summary of Graph Network and their use in ML: . Relational inductive biases, deep learning, and graph networks | . Examples of Network graphs: . NetworkX Example dataset | Stanford Large Network Dataset Collection | Network building and manipulation will be done using NetworkX - a python package made for this exact function . import os import numpy as np import networkx as nx #-- PLOTTING PARAMS -# import matplotlib.pyplot as plt from matplotlib.pyplot import cm %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} . 1. Basics . Nodes: Points which are connected to each other. Can represent people, words, or atoms -- objects which have attributes of their own . Edges: Connection between the nodes - show how nodes (entities) are connected, bond distance, social network (friendships) -- property which connect the entities . G = nx.Graph() # Add a node G.add_node(42) # Add node from list of entities temp_list = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;] G.add_nodes_from(temp_list) . G.nodes . NodeView((42, &#39;A&#39;, &#39;B&#39;, &#39;C&#39;)) . G.remove_node(42) #This is definite node name and should exist in the network # Multiple nodes G.remove_nodes_from([&#39;A&#39;,&#39;Z&#39;,&#39;Blah&#39;]) #Here it is compared to the element to that in the list . G.nodes . NodeView((&#39;B&#39;, &#39;C&#39;)) . # this also adds nodes if they don&#39;t already exist G.add_edge(&#39;C&#39;,&#39;Z&#39;) . print(G.edges, G.nodes) . [(&#39;C&#39;, &#39;Z&#39;)] [&#39;B&#39;, &#39;C&#39;, &#39;Z&#39;] . G.add_edges_from([(&#39;B&#39;, &#39;C&#39;) , (&#39;B&#39;, &#39;Z&#39;)]) . G.edges . EdgeView([(&#39;B&#39;, &#39;C&#39;), (&#39;B&#39;, &#39;Z&#39;), (&#39;C&#39;, &#39;Z&#39;)]) . # remove multiple edges (list of tuples) G.remove_edges_from([(&#39;A&#39;, &#39;B&#39;) , (&#39;C&#39;, &#39;B&#39;)]) #Here list are commutative . G.edges . EdgeView([(&#39;B&#39;, &#39;Z&#39;), (&#39;C&#39;, &#39;Z&#39;)]) . G.number_of_nodes() . 3 . G.number_of_edges() . 2 . G.degree(&#39;B&#39;) . 1 . G.clear() . 2. Reading from a file . For example we will look at Facebook dataset installed from SNAP dataset . G = nx.read_edgelist(&#39;./data/facebook_combined.txt&#39;) . G.number_of_nodes() . 4039 . G.number_of_edges() . 88234 . dict_neighbors = G.neighbors(&#39;2&#39;) . G.degree(&#39;2&#39;) . 10 . list(dict_neighbors) . [&#39;0&#39;, &#39;20&#39;, &#39;115&#39;, &#39;116&#39;, &#39;149&#39;, &#39;226&#39;, &#39;312&#39;, &#39;326&#39;, &#39;333&#39;, &#39;343&#39;] . G.clear() . 3. Type of different networks . a. Weighted Graphs . Edge weight Consider that the edge that you are adding should contain additional information, such as the strength of the connection. This would be important, for example, when analyzing communication networks to check friendship/connectivity strength. You want to capture how many times they exchanged e-mails, calls, text messages, to indicate the strength of the connection. For this you will assign weights to the edge, values that can be the number of communications, or the fraction of communications, normalized. . I had used this type of graph in my analysis for Indian spices. In that case, the edge was assigned a weight corresponding to the number of times a pair of spice occured together in a recipe. . G.add_edge(&#39;Water&#39;,&#39;Soda&#39;, weight=10) . Ways to access edge property: . G.edges.data() . EdgeDataView([(&#39;Water&#39;, &#39;Soda&#39;, {&#39;weight&#39;: 10})]) . G[&#39;Soda&#39;][&#39;Water&#39;] . {&#39;weight&#39;: 10} . G[&#39;Water&#39;][&#39;Soda&#39;] . {&#39;weight&#39;: 10} . G[&#39;Water&#39;][&#39;Soda&#39;][&#39;weight&#39;] = -1 . G.edges.data() . EdgeDataView([(&#39;Water&#39;, &#39;Soda&#39;, {&#39;weight&#39;: -1})]) . b. Directed Graphs . Incorporate directionality in the edge. Instead of having just the edge showing the connection: A B encode a type of connection. If A is giving (food, resources, atoms, electrons) to B. In that case: A -&gt; B . G.nodes . NodeView((&#39;Water&#39;, &#39;Soda&#39;)) . dg = nx.to_directed(G) . dg.edges . OutEdgeView([(&#39;Water&#39;, &#39;Soda&#39;), (&#39;Soda&#39;, &#39;Water&#39;)]) . dg.get_edge_data(&#39;Water&#39;,&#39;Soda&#39;) . {&#39;weight&#39;: -1} . c. Multigraphs . NetworkX provides classes for graphs which allow multiple edges between any pair of nodes. The MultiGraph and MultiDiGraph classes allow you to add the same edge twice, possibly with different edge data. This can be powerful for some applications, but many algorithms are not well defined on such graphs. . MG = nx.MultiGraph() MG.add_weighted_edges_from([(1, 2, 3.0), (1, 2, 75), (2, 3, 5), (1, 2, 4.2)]) . MG.edges . MultiEdgeView([(1, 2, 0), (1, 2, 1), (1, 2, 2), (2, 3, 0)]) . MG.edges.data(&#39;weight&#39;) . MultiEdgeDataView([(1, 2, 3.0), (1, 2, 75), (1, 2, 4.2), (2, 3, 5)]) . MG.edges.data() . MultiEdgeDataView([(1, 2, {&#39;weight&#39;: 3.0}), (1, 2, {&#39;weight&#39;: 75}), (1, 2, {&#39;weight&#39;: 4.2}), (2, 3, {&#39;weight&#39;: 5})]) . MG[1][2] . AtlasView({0: {&#39;weight&#39;: 3.0}, 1: {&#39;weight&#39;: 75}, 2: {&#39;weight&#39;: 4.2}}) . d. Bipartite . Bipartite graphs B = (U, V, E) have two node sets U,V and edges in E that only connect nodes from opposite sets. It is common in the literature to use an spatial analogy referring to the two node sets as top and bottom nodes. . from networkx.algorithms import bipartite . bip = nx.Graph() . bip.add_nodes_from([&#39;apple&#39;, &#39;peach&#39;, &#39;watermelon&#39;, &#39;pear&#39;], bipartite=0) bip.add_nodes_from([&#39;Alice&#39;, &#39;Steve&#39;, &#39;Mary&#39;], bipartite=1) . bip.add_edges_from([(&#39;Alice&#39;, &#39;apple&#39;), (&#39;Alice&#39;, &#39;peach&#39;), (&#39;Steve&#39;, &#39;watermelon&#39;), (&#39;Mary&#39;, &#39;pear&#39;), (&#39;Mary&#39;, &#39;apple&#39;), (&#39;Mary&#39;, &#39;watermelon&#39;)]) . nx.draw(bip, with_labels=True) . Currently, NetworkX does not provide a bipartite graph visualization method to visually delimit the two sets of nodes. However, we can draw the left and right set of nodes and see how they connect to each other. Further, you can play around with coloring the nodes based on the &#39;bipartite&#39; attribute to further refine visually to which node set each node belongs to. . import scipy.sparse as sparse X, Y = bipartite.sets(bip) pos = dict() pos.update((n, (1, i*10)) for i, n in enumerate(X)) pos.update((n, (1.5, i*10)) for i, n in enumerate(Y)) nx.draw(bip, with_labels=True, pos=pos) . Bipartite graphs can be projected as two separate graphs G1 = (U, E1) and G2 = (V, E2). The edges will be different though. . We can create a network of fruits, where nodes will be fruits and the edges will between two fruits will be created if someone likes both fruits. Such, peach and apple will have one edge, as Alice likes both. Same for apple and pear, which are both liked by Mary. Likewise, we can create the second network as the network of individuals, where connections between them will be their preference for the same fruit. Here, we can create a connection/edge between Steve and Mary since both of them like watermelon. . 3. Network Models . Network models can be very useful for comparing their topology to the structural properties of our network built from real data. Different network models have very distinct structural characteristics, which defines their behavior in case of information flow on the network, attacks/failures on the nodes/edges, etc, and these properties have been extensively studied and are well documented. Knowing to which network model your graph corresponds to can provide valuable insights about its potential behavior under various circumstances. . There are a miriad of network models with different topological properties. Here we will try out some of the most useful ones (that frequently occur in real complex systems). . ba = nx.barabasi_albert_graph(10, 5) nx.draw_spectral(ba, node_size=200) . Barabasi-Alber Graph. A graph of N nodes is grown by attaching new nodes each with M edges that are preferentially attached to existing nodes with high degree. . er = nx.erdos_renyi_graph(50, 0.1) nx.draw_circular(er) . complete = nx.complete_graph(5) nx.draw(complete) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2020/11/08/network_analysis_basics.html",
            "relUrl": "/python/machine-learning/2020/11/08/network_analysis_basics.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Transfer learning walkthrough using Pytorch",
            "content": "What is transfer learning? . Transfer learning is a technique where a deep-learning model trained on another problem (which usually has lot of data and good accuracy for that task) is slightly modified to be used on a new problem. This is an important concept as building an entirely new model might not be take a long time or there might not be enough data for the training of that particular task. The idea is the weights/parameters of the model at the start of the layers have similar functionality and assist in better performance on the new task. Usually we freeze the weights training of the hidden layers an tweak the output layer slightly to account for the change in the task. . So for example, maybe you could have the neural network learn to recognize objects like cats and then use that knowledge or use part of that knowledge to help you do a better job reading x-ray scans. This is called transfer learning. Sometimes you can start with the weights and biases of a published netowrks as a starting point. . More details about Transfer Learning can be found on Stanford&#39;s CS231 CNN course here . In this example I use a pre-trained convolutional neural network model (ResNet-18) and modify ONLY the last layers of the model to use for our case. This model is trained on millions on images with 1000 image categories. . These two major transfer learning scenarios look as follows: . Finetuning the convnet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. . | ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. . | . This tutorial was adapted from PyTorch&#39;s official documentation (Link) . import time import os import copy import matplotlib.pyplot as plt import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torchvision from torchvision import datasets, models, transforms %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; # Plot matplotlib plots with white background: %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) device . device(type=&#39;cpu&#39;) . # As defined on the tutorial page mean = np.array([0.5, 0.5, 0.5]) std = np.array([0.25, 0.25, 0.25]) data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std) ]), &#39;val&#39;: transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean, std) ]), } . data_dir = &#39;data/hymenoptera_data/&#39; image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [&#39;train&#39;, &#39;val&#39;]} . dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in [&#39;train&#39;, &#39;val&#39;]} dataset_sizes = {x: len(image_datasets[x]) for x in [&#39;train&#39;, &#39;val&#39;]} . class_names = image_datasets[&#39;train&#39;].classes print(class_names) . [&#39;ants&#39;, &#39;bees&#39;] . def imshow(inp, title): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; fig, ax = plt.subplots(1,1, figsize=(10,10)) inp = inp.numpy().transpose((1, 2, 0)) inp = std * inp + mean inp = np.clip(inp, 0, 1) ax.imshow(inp) plt.title(title) plt.show() # Get a batch of training data inputs, classes = next(iter(dataloaders[&#39;train&#39;])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) . def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print(&#39;Epoch {}/{}&#39;.format(epoch, num_epochs - 1)) print(&#39;-&#39; * 10) # Each epoch has a training and validation phase for phase in [&#39;train&#39;, &#39;val&#39;]: if phase == &#39;train&#39;: model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # forward # track history if only in train with torch.set_grad_enabled(phase == &#39;train&#39;): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == &#39;train&#39;: optimizer.zero_grad() loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == &#39;train&#39;: scheduler.step() #Step in the scheduler epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(&#39;{} Loss: {:.4f} Acc: {:.4f}&#39;.format( phase, epoch_loss, epoch_acc)) # deep copy the model for which val_acc is better if phase == &#39;val&#39; and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print(&#39;Training complete in {:.0f}m {:.0f}s&#39;.format( time_elapsed // 60, time_elapsed % 60)) print(&#39;Best val Acc: {:4f}&#39;.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model . Method 1: Fine tuning of the model . Download a pretrained model, tweak the model architecture for our use-case, and (re)train on the new dataset . Download the model (ResNet18 in this case) | Change the output of the final layer -- in this case from 1000 output nodes to 2 since we&#39;re looking at only &#39;bee&#39; and &#39;ant&#39; | Re-train the model | Other models available from PyTorch can be viewed (here) . model = torchvision.models.resnet18(pretrained=True) print(model) . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True) ) . We&#39;re interested to only change the final layer named fc -- so we will look at that layers features, the attributes for each module in the model are stored as keys. . num_features = model.fc.in_features # Define a new linear layer as per our need -- 2 classes instead of 1000 as defined in the original model.fc = nn.Linear(num_features, len(class_names)) #Number of classes in the end # Send model to device model.to(device) . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=2, bias=True) ) . criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) #Scheduler to update the learning rate for the SGD &#39;&#39;&#39; After every 7 steps in the optimizer the learning rate will be multiplied by 0.1 &#39;&#39;&#39; step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.1) . model = train_model(model, criterion=criterion, optimizer=optimizer, scheduler=step_lr_scheduler, num_epochs=5) . Epoch 0/4 - train Loss: 1.0695 Acc: 0.5000 val Loss: 0.6179 Acc: 0.6340 Epoch 1/4 - train Loss: 0.7267 Acc: 0.5820 val Loss: 0.8217 Acc: 0.5948 Epoch 2/4 - train Loss: 0.6941 Acc: 0.6230 val Loss: 0.8131 Acc: 0.6667 Epoch 3/4 - train Loss: 0.6948 Acc: 0.5902 val Loss: 1.6474 Acc: 0.5882 Epoch 4/4 - train Loss: 0.6834 Acc: 0.6270 val Loss: 0.9145 Acc: 0.6275 Training complete in 5m 11s Best val Acc: 0.666667 . Method 2: Freeze penultimate weights . In this case ONLY the weights of the final layer are trained. This might reduce the accuracy but would greatly reduce the amount of time taken to fit the model since the number of weights to be optimized is greatly reduced. . model = torchvision.models.resnet18(pretrained=True) . for param in model.parameters(): param.requires_grad = False . num_features = model.fc.in_features model.fc = nn.Linear(num_features, 2) #Number of classes in the end model.to(device) . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=2, bias=True) ) . This block is same as before . criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.001) #Scheduler to update the learning rate for the SGD &#39;&#39;&#39; After every 7 steps in the optimizer the learning rate will be multiplied by 0.1 &#39;&#39;&#39; step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.1) . Model training . model = train_model(model, criterion=criterion, optimizer=optimizer, scheduler=step_lr_scheduler, num_epochs=5) . Epoch 0/4 - train Loss: 0.6937 Acc: 0.5615 val Loss: 0.5267 Acc: 0.7974 Epoch 1/4 - train Loss: 0.5915 Acc: 0.7008 val Loss: 0.4388 Acc: 0.8627 Epoch 2/4 - train Loss: 0.5249 Acc: 0.7623 val Loss: 0.3686 Acc: 0.9216 Epoch 3/4 - train Loss: 0.5079 Acc: 0.7664 val Loss: 0.3188 Acc: 0.9477 Epoch 4/4 - train Loss: 0.4556 Acc: 0.8115 val Loss: 0.3007 Acc: 0.9281 Training complete in 2m 18s Best val Acc: 0.947712 . Using fine-tuning and re-training all the weights for the new network take longer and may result in lower acceracy than having the weights fixed. When we keep the weights in earlier layers fixed, we save a lot of time in the model training aand the performance of the model is also better (provided the two tasks are quite similar). Given the network is deep, optimizing the weights for this network from scratch would have been difficult and time consuming. For such a case, transfer learning seems to be a good option. .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/machine-learning/2020/10/19/pytorch_transfer_learning_basics.html",
            "relUrl": "/python/data-analysis/machine-learning/2020/10/19/pytorch_transfer_learning_basics.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Get SMILES from PubChem using DASK",
            "content": "Dask implementation to acquire CanonicalSMILES from PubChem using the pubchem API. At the end of the notebook there is another dask based implementation of using RDKit to get InChIKey from the SMILES. While Dask is not necessary required in the case of InChIKeys it is a much more elegant implementation of dask.dataframes and map_partitions . import time import pubchempy as pcp from pubchempy import Compound, get_compounds import pandas as pd import numpy as np import re import copy . /depot/jgreeley/apps/envs/ml_torch/lib/python3.6/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError. warnings.warn(msg) . Get SMILES from Pubchem . Update:Parallelized using dask . df_100 = pd.read_csv(&#39;./DASK_SMILES/sample_chemical_names.csv&#39;, sep=&#39;,&#39;, header=0) . df_100.shape . (147, 1) . from dask.distributed import Client, progress import dask.dataframe as dd from dask import delayed, compute from dask.multiprocessing import get client = Client() client . Client . Scheduler: tcp://127.0.0.1:45859 | Dashboard: http://127.0.0.1:8787/status | . | Cluster . Workers: 4 | Cores: 8 | Memory: 39.85 GB | . | . def get_smile(cmpd_name): try: #delayed(f)(x, args=a) name = delayed(pcp.get_properties)([&#39;CanonicalSMILES&#39;], cmpd_name, &#39;name&#39;) time.sleep(5) smile = name[0][&#39;CanonicalSMILES&#39;] except: smile = &#39;X&#39; print(cta_name, smile) return smile def dask_smiles(df): df[&#39;CanonicalSMILES&#39;] = df[&#39;CTA&#39;].map(get_smile) return df #Map paritions works here -- but not with to_list() in the previous implementation . df_dask = dd.from_pandas(df_100, npartitions=10) . df_dask . Dask DataFrame Structure: CTA . npartitions=10 . 0 object | . 15 ... | . ... ... | . 135 ... | . 146 ... | . Dask Name: from_pandas, 10 tasks df_dask.visualize() %time ddf_out = df_dask.map_partitions(dask_smiles) . CPU times: user 567 ms, sys: 92.3 ms, total: 660 ms Wall time: 10 s . ddf_out.iloc[:,0] . Dask Series Structure: npartitions=10 0 object 15 ... ... 135 ... 146 ... Name: CTA, dtype: object Dask Name: getitem, 30 tasks . ddf_out.visualize() %time results = ddf_out.persist(scheduler=client).compute() . CPU times: user 9.42 s, sys: 1.27 s, total: 10.7 s Wall time: 2min 43s . type(results) . pandas.core.frame.DataFrame . results.loc[0] . CTA Cyclopropane CanonicalSMILES Delayed(&#39;getitem-e98dc8d7261c3d694a3c944735b3c... Name: 0, dtype: object . compute(results[&#39;CanonicalSMILES&#39;].iloc[0])[0] #Compute result for one entry . &#39;C1CC1&#39; . %time results[&#39;CanonicalSMILES&#39;] = [value[0] for value in results[&#39;CanonicalSMILES&#39;].map(compute)] . CPU times: user 3.73 s, sys: 443 ms, total: 4.17 s Wall time: 31.1 s . type(results) . pandas.core.frame.DataFrame . results[results[&#39;CanonicalSMILES&#39;] == &#39;X&#39;] . CTA CanonicalSMILES . results . CTA CanonicalSMILES . 0 Cyclopropane | C1CC1 | . 1 Ethylene | C=C | . 2 Methane | C | . 3 t-Butanol | CC(C)(C)O | . 4 ethane | CC | . ... ... | ... | . 142 Cyclohexane-1,3-dicarbaldehyde | C1CC(CC(C1)C=O)C=O | . 143 isobutene | CC(=C)C | . 144 propanal | CCC=O | . 145 methyl methacrylate | CC(=C)C(=O)OC | . 146 vinyl acetate | CC(=O)OC=C | . 147 rows × 2 columns . results.to_pickle(&quot;cta_smiles_table_100_less.pkl&quot;) ## Dask to get InChIKey . This implementation in my opinion is more elegant use of dask&#39;s apply command wrapper around conventional pandas apply. Also here we are defining the meta key for the variable since the code doesn&#39;t seem to recognise the type of entries we expect in the final output . More information about meta here: https://docs.dask.org/en/latest/dataframe-api.html . import rdkit from rdkit import Chem from rdkit.Chem import PandasTools from rdkit.Chem import Draw Chem.WrapLogs() lg = rdkit.RDLogger.logger() lg.setLevel(rdkit.RDLogger.CRITICAL) . def get_InChiKey(x): try: inchi_key = Chem.MolToInchiKey(Chem.MolFromSmiles(x)) except: inchi_key = &#39;X&#39; return inchi_key def dask_smiles(df): df[&#39;INCHI&#39;] = df[&#39;smiles&#39;].map(get_name) return df . results_dask = dd.from_pandas(results, npartitions=10) . inchi = results_dask[&#39;CanonicalSMILES&#39;].apply(lambda x: Chem.MolToInchiKey(Chem.MolFromSmiles(x)), meta=(&#39;inchi_key&#39;,str)) . inchi . Dask Series Structure: npartitions=10 0 object 15 ... ... 135 ... 146 ... Name: inchi_key, dtype: object Dask Name: apply, 30 tasks . inchi.visualize() inchi is a new Pandas series which has the delayed graphs for computing InChIKeys. We can compute it directly in the results dataframe as a new column. This is slightly different from the SMILES implementation above. . %time results[&#39;INCHI&#39;] = compute(inchi, scheduler = client)[0] . CPU times: user 125 ms, sys: 17.3 ms, total: 142 ms Wall time: 1.02 s . results . CTA CanonicalSMILES INCHI . 0 Cyclopropane | C1CC1 | LVZWSLJZHVFIQJ-UHFFFAOYSA-N | . 1 Ethylene | C=C | VGGSQFUCUMXWEO-UHFFFAOYSA-N | . 2 Methane | C | VNWKTOKETHGBQD-UHFFFAOYSA-N | . 3 t-Butanol | CC(C)(C)O | DKGAVHZHDRPRBM-UHFFFAOYSA-N | . 4 ethane | CC | OTMSDBZUPAUEDD-UHFFFAOYSA-N | . ... ... | ... | ... | . 142 Cyclohexane-1,3-dicarbaldehyde | C1CC(CC(C1)C=O)C=O | WHKHKMGAZGBKCK-UHFFFAOYSA-N | . 143 isobutene | CC(=C)C | VQTUBCCKSQIDNK-UHFFFAOYSA-N | . 144 propanal | CCC=O | NBBJYMSMWIIQGU-UHFFFAOYSA-N | . 145 methyl methacrylate | CC(=C)C(=O)OC | VVQNEPGJFQJSBK-UHFFFAOYSA-N | . 146 vinyl acetate | CC(=O)OC=C | XTXRWKRVRITETP-UHFFFAOYSA-N | . 147 rows × 3 columns .",
            "url": "http://pgg1610.github.io/blog_fastpages/chemical-science/python/data-analysis/2020/09/18/SMILES_from_pubchem.html",
            "relUrl": "/chemical-science/python/data-analysis/2020/09/18/SMILES_from_pubchem.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Analyze Bollywood movie ratings (1950-2020)",
            "content": "import os from requests import get import numpy as np import pandas as pd from bs4 import BeautifulSoup import time as time from tqdm.notebook import tqdm . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns sns.set(style=&quot;whitegrid&quot;) %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . Load Dataset . The data set for the movie was scrapped from IMDB using BeautifulSoup. A template for the code used for scrapping the data is shown in the cell below. . names, year, imdb_rating, metascore, num_votes = [], [], [], [], [] start_time = time.time() requests = 0 years_url = [str(i) for i in range(1950,2006)] page_iter = [0, 51, 101, 151, 201] for year_url in tqdm(years_url): for page_num in tqdm(page_iter): #URL to parse url = &#39;https://www.imdb.com/search/title/?title_type=feature,&amp;release_date={0},{0}&amp;countries=in&amp;languages=hi&amp;sort=num_votes,desc&amp;start={1}&amp;ref_=adv_prv&#39;.format(int(year_url), int(page_num)) response = get(url) #Sleep to carve out load time.sleep(np.random.randint(1,5)) #Estimate time elapsed per request requests += 1 elapsed_time = time.time() - start_time print(&#39;Request:{}; Frequency: {} requests/s&#39;.format(requests, requests/elapsed_time)) clear_output(wait = True) html_soup = BeautifulSoup(response.text, &#39;html.parser&#39;) movie_containers = html_soup.find_all(&#39;div&#39;, class_=&#39;lister-item mode-advanced&#39;) for i, container in enumerate(movie_containers): container_entry = movie_containers[i] movie_name = container_entry.h3.a.text names.append(movie_name) movie_year = container_entry.h3.find(&#39;span&#39;,class_=&#39;lister-item-year text-muted unbold&#39;).text.strip(&#39;()&#39;) year.append(movie_year) #print(movie_name, movie_year) try: movie_rating = float(container_entry.strong.text) imdb_rating.append(movie_rating) except AttributeError: imdb_rating.append(np.nan) try: movie_votes = float(&#39;&#39;.join(container_entry.find(&#39;span&#39;, attrs = {&#39;name&#39;:&#39;nv&#39;}).text.split(&#39;,&#39;))) num_votes.append(movie_votes) except (AttributeError, ValueError): num_votes.append(np.nan) try: movie_metascore = float(container_entry.find(&#39;span&#39;, class_=&#39;metascore&#39;).text.strip()) metascore.append(movie_metascore) except AttributeError: metascore.append(np.nan) print(&#39;Making dataframe for year {}&#39;.format(year_url)) df_movies = pd.DataFrame({&#39;name&#39;:names,&#39;year&#39;:year,&#39;rating&#39;:imdb_rating,&#39;metascore&#39;:metascore,&#39;num_votes&#39;:num_votes}) df_movies.to_csv(&#39;./temp_imdb_files/bollywood_data_{}.csv&#39;.format(year_url),sep=&#39;,&#39;,header=True, index=False) del df_movies . . df_movies = pd.read_csv(&#39;./IMDB-files/bollywood_movies_data_1950_2020_new.csv&#39;,sep=&#39;,&#39;, skipinitialspace=True) . df_movies.columns . Index([&#39;name&#39;, &#39;year&#39;, &#39;rating&#39;, &#39;metascore&#39;, &#39;num_votes&#39;], dtype=&#39;object&#39;) . df_movies.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 11876 entries, 0 to 11875 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 name 11876 non-null object 1 year 11875 non-null object 2 rating 7427 non-null float64 3 metascore 91 non-null float64 4 num_votes 7427 non-null float64 dtypes: float64(3), object(2) memory usage: 464.0+ KB . Cleaning the data . Since we are particularly interested in release year of the movies, we can sanitize that column first. To begin, we see what are different possible strings/elements in the year. . df_movies[&#39;year&#39;].unique() . array([&#39;1950&#39;, &#39;1951&#39;, &#39;I) (1951&#39;, &#39;1952&#39;, &#39;1957&#39;, &#39;II) (1952&#39;, &#39;1953&#39;, &#39;II) (1953&#39;, &#39;III) (1953&#39;, &#39;I) (1953&#39;, &#39;1954&#39;, &#39;I) (1954&#39;, &#39;III) (1954&#39;, &#39;1955&#39;, &#39;1956&#39;, &#39;II) (1957&#39;, &#39;1958&#39;, &#39;I) (1958&#39;, &#39;1959&#39;, &#39;II) (1959&#39;, &#39;1960&#39;, &#39;I) (1960&#39;, &#39;1961&#39;, &#39;1962&#39;, &#39;1963&#39;, &#39;I) (1964&#39;, &#39;1964&#39;, &#39;1965&#39;, &#39;1966&#39;, &#39;1967&#39;, &#39;1968&#39;, &#39;I) (1968&#39;, &#39;1969&#39;, &#39;I) (1969&#39;, &#39;1979&#39;, &#39;1970&#39;, &#39;II) (1970&#39;, &#39;1971&#39;, &#39;I) (1971&#39;, &#39;II) (1971&#39;, &#39;1972&#39;, &#39;II) (1972&#39;, &#39;1973&#39;, &#39;1974&#39;, &#39;II) (1974&#39;, &#39;1975&#39;, &#39;I) (1975&#39;, &#39;II) (1975&#39;, &#39;1976&#39;, &#39;1977&#39;, &#39;I) (1977&#39;, &#39;1978&#39;, &#39;II) (1978&#39;, &#39;I) (1979&#39;, &#39;II) (1979&#39;, &#39;1980&#39;, &#39;I) (1980&#39;, &#39;1981&#39;, &#39;1982&#39;, &#39;I) (1982&#39;, &#39;1983&#39;, &#39;I) (1983&#39;, &#39;II) (1983&#39;, &#39;1984&#39;, &#39;II) (1984&#39;, &#39;1985&#39;, &#39;I) (1985&#39;, &#39;1986&#39;, &#39;I) (1986&#39;, &#39;II) (1986&#39;, &#39;1987&#39;, &#39;I) (1987&#39;, &#39;1988&#39;, &#39;I) (1988&#39;, &#39;II) (1988&#39;, &#39;1989&#39;, &#39;I) (1989&#39;, &#39;1990&#39;, &#39;II) (1990&#39;, &#39;I) (1990&#39;, &#39;1991&#39;, &#39;I) (1991&#39;, &#39;1992&#39;, &#39;1993&#39;, &#39;I) (1992&#39;, &#39;II) (1992&#39;, &#39;I) (1993&#39;, &#39;II) (1993&#39;, &#39;1994&#39;, &#39;II) (1994&#39;, &#39;I) (1994&#39;, &#39;1995&#39;, &#39;1996&#39;, &#39;I) (1996&#39;, &#39;1997&#39;, &#39;I) (1997&#39;, &#39;1998&#39;, &#39;II) (1998&#39;, &#39;2005&#39;, &#39;1999&#39;, &#39;II) (1999&#39;, &#39;2000&#39;, &#39;II) (2000&#39;, &#39;I) (2000&#39;, &#39;2001&#39;, &#39;I) (2001&#39;, &#39;I) (2002&#39;, &#39;2002&#39;, &#39;2003&#39;, &#39;I) (2003&#39;, &#39;2004&#39;, &#39;2007&#39;, &#39;I) (2005&#39;, &#39;II) (2005&#39;, &#39;2006&#39;, &#39;I) (2006&#39;, &#39;II) (2006&#39;, &#39;I) (2007&#39;, &#39;III) (2007&#39;, &#39;2008&#39;, &#39;I) (2008&#39;, &#39;II) (2008&#39;, &#39;2009&#39;, &#39;I) (2009&#39;, &#39;2012&#39;, &#39;II) (2009&#39;, &#39;2010&#39;, &#39;I) (2010&#39;, &#39;II) (2010&#39;, &#39;IV) (2010&#39;, &#39;2011&#39;, &#39;I) (2011&#39;, &#39;II) (2011&#39;, &#39;IV) (2011&#39;, &#39;II) (2012&#39;, &#39;I) (2012&#39;, &#39;2013&#39;, &#39;I) (2013&#39;, &#39;II) (2013&#39;, &#39;V) (2013&#39;, &#39;2014&#39;, &#39;I) (2014&#39;, &#39;III) (2014&#39;, &#39;VIII) (2014&#39;, &#39;II) (2014&#39;, &#39;IV) (2014&#39;, &#39;2015&#39;, &#39;I) (2015&#39;, &#39;V) (2015&#39;, &#39;III) (2015&#39;, &#39;VI) (2015&#39;, &#39;II) (2015&#39;, &#39;IV) (2015&#39;, &#39;2016&#39;, &#39;I) (2016&#39;, &#39;III) (2016&#39;, &#39;XVII) (2016&#39;, &#39;IV) (2016&#39;, &#39;V) (2016&#39;, &#39;X) (2016&#39;, &#39;II) (2016&#39;, &#39;VII) (2016&#39;, &#39;VI) (2016&#39;, &#39;2017&#39;, &#39;I) (2017&#39;, &#39;II) (2017&#39;, &#39;III) (2017&#39;, &#39;IV) (2017&#39;, &#39;2018&#39;, &#39;III) (2018&#39;, &#39;I) (2018&#39;, &#39;II) (2018&#39;, &#39;2019&#39;, &#39;III) (2019&#39;, &#39;I) (2019&#39;, &#39;II) (2019&#39;, &#39;IV) (2019&#39;, &#39;2020&#39;, &#39;I) (2020&#39;, &#39;II) (2020&#39;, &#39;VI) (2020&#39;, nan], dtype=object) . Data pulled from the website has phantom characters alongside the dates. Hence this would need some cleaning from our end to ensure all the dates are in consistent format. . df_movies.shape . (11876, 5) . I am using strip to loop each date entry in the dataset and strip off any residual characters which coincide with the those mentioned in the filter. Another option is to use replace in pandas using regex filters . df_movies[&#39;year&#39;] = df_movies[&#39;year&#39;].astype(&#39;str&#39;) . df_movies[&#39;year&#39;]=[i.strip(&#39;IIII) XVII) ( ( TV Special TV Mov&#39;) for i in df_movies[&#39;year&#39;].tolist()] . Printing the data again to check for the date entries: . df_movies[&#39;year&#39;].unique() . array([&#39;1950&#39;, &#39;1951&#39;, &#39;1952&#39;, &#39;1957&#39;, &#39;1953&#39;, &#39;1954&#39;, &#39;1955&#39;, &#39;1956&#39;, &#39;1958&#39;, &#39;1959&#39;, &#39;1960&#39;, &#39;1961&#39;, &#39;1962&#39;, &#39;1963&#39;, &#39;1964&#39;, &#39;1965&#39;, &#39;1966&#39;, &#39;1967&#39;, &#39;1968&#39;, &#39;1969&#39;, &#39;1979&#39;, &#39;1970&#39;, &#39;1971&#39;, &#39;1972&#39;, &#39;1973&#39;, &#39;1974&#39;, &#39;1975&#39;, &#39;1976&#39;, &#39;1977&#39;, &#39;1978&#39;, &#39;1980&#39;, &#39;1981&#39;, &#39;1982&#39;, &#39;1983&#39;, &#39;1984&#39;, &#39;1985&#39;, &#39;1986&#39;, &#39;1987&#39;, &#39;1988&#39;, &#39;1989&#39;, &#39;1990&#39;, &#39;1991&#39;, &#39;1992&#39;, &#39;1993&#39;, &#39;1994&#39;, &#39;1995&#39;, &#39;1996&#39;, &#39;1997&#39;, &#39;1998&#39;, &#39;2005&#39;, &#39;1999&#39;, &#39;2000&#39;, &#39;2001&#39;, &#39;2002&#39;, &#39;2003&#39;, &#39;2004&#39;, &#39;2007&#39;, &#39;2006&#39;, &#39;2008&#39;, &#39;2009&#39;, &#39;2012&#39;, &#39;2010&#39;, &#39;2011&#39;, &#39;2013&#39;, &#39;2014&#39;, &#39;2015&#39;, &#39;2016&#39;, &#39;2017&#39;, &#39;2018&#39;, &#39;2019&#39;, &#39;2020&#39;, &#39;nan&#39;], dtype=object) . Consistency check for the dataframe shape to ensure no funny business . df_movies.shape . (11876, 5) . Filtering out movies . Since IMDb is a fairly recent rating portal there are lot of movies especially those realeased pre 1980s which have low votes. Also IMDb lists every possible movie that was released in Hindi language. To better focus on credible movies I would filter out movies with low votes . votes_filter = df_movies[&#39;num_votes&#39;] &gt; 50 #Filter out movies which have got less than 100 votes from IMDb users df_movies_filter_votes = df_movies.loc[votes_filter].reset_index(drop=True) #Reset the indices of the new dataframe and drop the old ones -- if not done a different column with old index is appended . df_movies_filter_votes.shape . (3912, 5) . Convert year data entry to pandas Datetime object for convenience . df_movies_filter_votes[&#39;year&#39;] = pd.to_datetime(df_movies_filter_votes[&#39;year&#39;],format=&#39;%Y&#39;).dt.year . Analyze annual movie releases . Defining a separate dataframe for doing per-year analysis . stat_list = [&#39;year&#39;, &#39;total_movies_year&#39;, &#39;highest_rated_movie&#39;, &#39;movie_rating&#39;,&#39;avg_num_votes&#39;, &#39;avg_movie_rating&#39;] annual_movie_stats = {keys:[] for keys in stat_list} for year_entry in df_movies_filter_votes[&#39;year&#39;].unique(): per_year_column = df_movies_filter_votes.loc[df_movies_filter_votes[&#39;year&#39;] == year_entry] try: movie_entry_with_max_ratings = df_movies_filter_votes.loc[per_year_column[&#39;rating&#39;].idxmax()] higest_movie_rating = movie_entry_with_max_ratings[&#39;rating&#39;] highest_rated_movie = movie_entry_with_max_ratings[&#39;name&#39;] avg_movie_rating = per_year_column[&#39;rating&#39;].mean() total_movies = len(per_year_column) avg_num_votes = per_year_column[&#39;num_votes&#39;].mean() except ValueError: higest_movie_rating = np.nan highest_rated_movie = np.nan total_movies = np.nan avg_movie_rating = np.nan annual_movie_stats[&#39;year&#39;].append(year_entry) annual_movie_stats[&#39;highest_rated_movie&#39;].append(highest_rated_movie) annual_movie_stats[&#39;movie_rating&#39;].append(higest_movie_rating) annual_movie_stats[&#39;avg_movie_rating&#39;].append(avg_movie_rating) annual_movie_stats[&#39;total_movies_year&#39;].append(total_movies) annual_movie_stats[&#39;avg_num_votes&#39;].append(avg_num_votes) . df_annual_movie_stats = pd.DataFrame(annual_movie_stats, columns=annual_movie_stats.keys()) . df_annual_movie_stats.sample(5) . year total_movies_year highest_rated_movie movie_rating avg_num_votes avg_movie_rating . 49 1999 | 67 | Sarfarosh | 8.1 | 2201.328358 | 5.583582 | . 3 1953 | 9 | Do Bigha Zamin | 8.4 | 293.000000 | 7.388889 | . 69 2019 | 141 | 99 Songs | 8.8 | 4041.056738 | 6.002128 | . 9 1959 | 11 | Kaagaz Ke Phool | 8.0 | 329.454545 | 7.172727 | . 34 1984 | 36 | Saaransh | 8.2 | 286.055556 | 6.427778 | . fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(30,20), sharex=True) year_list = [&quot;&#39;{}&quot;.format(str(value)[2:]) for value in df_annual_movie_stats.year.to_list()] sns.barplot(x=year_list, y=&#39;total_movies_year&#39;, color=&#39;k&#39;, alpha=0.8, data=df_annual_movie_stats, ax=ax1) ax1.set_ylabel(&#39;Average movies released&#39;) sns.scatterplot(year_list, &#39;avg_movie_rating&#39;, size=&#39;avg_num_votes&#39;, color=&#39;k&#39;, sizes=(40, 400), data=df_annual_movie_stats, ax=ax2); ax2.set_xlabel(&#39;Year&#39;) ax2.set_ylabel(&#39;Average movie rating&#39;) ax2.get_legend() for item in ax2.get_xticklabels(): item.set_rotation(45) plt.tight_layout() . /Users/pghaneka/miniconda3/envs/doodle/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . The two plots show the number of films released each year and the average IMDb rating for the movies released in that year. Now we might conclude that movies are getting selectively worse in spite of there being more movies being released, however the confidence in that statement is difficult to justify since the number of votes casted in these movies is an important parameter to keep in mind. . Sort the movies released as per decades . Define a new column here as per decade to condense the analysis . 10 * (df_annual_movie_stats[&#39;year&#39;]//10) . This line converts years to a decade entry . df_annual_movie_stats[&#39;decade&#39;] = 10 * (df_annual_movie_stats[&#39;year&#39;]//10) . df_annual_movie_stats_decade = df_annual_movie_stats.groupby([&#39;decade&#39;]).mean() . df_annual_movie_stats_decade.sample(5) . year total_movies_year movie_rating avg_num_votes avg_movie_rating . decade . 2000 2004.5 | 94.0 | 8.46 | 5160.443252 | 5.399690 | . 2010 2014.5 | 126.2 | 8.44 | 6305.900985 | 5.748150 | . 2020 2020.0 | 102.0 | 8.90 | 7485.009804 | 5.785294 | . 1960 1964.5 | 17.6 | 8.15 | 326.809100 | 7.104778 | . 1970 1974.5 | 30.4 | 8.18 | 820.688426 | 6.876870 | . df_annual_movie_stats_decade.index . Int64Index([1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020], dtype=&#39;int64&#39;, name=&#39;decade&#39;) . decade_list = [&quot;{}s&quot;.format(str(value)[2:]) for value in df_annual_movie_stats_decade.index.to_list()] . fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(15,10), sharex=True) sns.barplot(x=decade_list, y=&#39;total_movies_year&#39;, data=df_annual_movie_stats_decade, color=&#39;k&#39;, alpha=0.8, ax=ax1) ax1.set_ylabel(&#39;Average movies released annually&#39;) sns.scatterplot(decade_list, &#39;avg_movie_rating&#39;, size=&#39;avg_num_votes&#39;, sizes=(100, 400), data=df_annual_movie_stats_decade, ax=ax2); sns.despine() ax2.set_xlabel(&#39;Decade&#39;) ax2.set_ylabel(&#39;Average movie rating&#39;) ax2.get_legend().remove() plt.tight_layout() . /Users/pghaneka/miniconda3/envs/doodle/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/exploratory-data-analysis/web-scrapping/2020/08/23/IMDB_bollywood.html",
            "relUrl": "/python/exploratory-data-analysis/web-scrapping/2020/08/23/IMDB_bollywood.html",
            "date": " • Aug 23, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Classification-based algorithms walkthrough",
            "content": "What is classification? . Some times the data in a supervised learning task is more qualitative than quantitative such as high, medium, low; or the task has categorical outputs such as colors - red, blue, green, or type of fruits: orange or apple. In such cases we look at classification models for training a model predictor. Various types of classification tasks exists: . Type of classification: . 1. Binary classifier: . Distinguish between two classes - high, low, or cat and a dog. A simplest algorithm to train for such a task is logistic regression. Most of the off-the-shelf algorithms work directly on the such tasks -- SVM, Random Forests, Naive Bayes. . 2. Multi-class classifier: . Distinguish between more than two classes - digits classifier is an example of multi-class classification since for a given digit image the answer could any number from 0-9. It is still one class but multiple options exists. Certain algorithms like Random Forest, Naive Bayes are capable of handling multi-class classifier. Others like SVM and linear classifiers are strictly binary classifier. . There are ways you can convert a binary classifier: . One-versus-all: | . Train n classifier for n classes such that each classifier ONLY predicts whether that class is present or not. Eg: Train a classifier to predict if a digit is 2 or not. . One-versus-one: | . Pair-wise classifier, in this case models are trained in a binary fashion for as many pair there can be between n classes. This can become computationally expensive since for n classes: $n(n-1)/2$ classifiers are needed. . Main advantage in this approach is that size of training data is small as only pair-wise data is required. Such an approach is useful when models dont scale well with large data -- such as SVM or gaussian based . 3. Multi-label or Multi-output classifier: . Distinguish between more than class but also the answer is not just one value but a list of possiblities. When the output amongst the list is only Binary it is usually refered to as Multi-label classification. For example: . If we train a model to classify a digit image as: . 1. Is it smaller than 4? (1:yes; 0:no) 2. Is it odd number? (1:yes; 0:no) 3. Is it is greater than 7? (1:yes; 0:no) . Then the output would be a list - for 5: [0, 1, 0] ; 4: [0, 0, 0]; 3: [1, 1, 0] . K-Nearest neighbor is a type of classifier which supports such a classification. . Scikit-learn has a wonderful documentation on metrics to be used for different types of classification tasks (Link here) . From the Scikit-learn documentation: . Some metrics are essentially defined for binary classification tasks (e.g. f1_score, roc_auc_score). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled 1 (though this may be configurable through the pos_label parameter). In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the average parameter. . The code in this notebook is adapted from Aurélien Geron&#39;s hands-on machine learning tutorial on Classifications Github Link . import os import copy import numpy as np np.random.seed(42) . import matplotlib.pyplot as plt from matplotlib.pyplot import cm # High DPI rendering for mac %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . MNIST dataset . 70,000 small images of hand-written numbers. Each image has 784 features. Those features are split in 28x28 pixels and each feature is simply that pixel gray-scale intensity. Value for each pixel ranges from 0 to 255. . from sklearn.datasets import load_digits mnist = load_digits() print(mnist.data.shape) . (1797, 64) . from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings . def sort_by_target(mnist): reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1] reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1] #minist.data is a pandas DataFrame mnist_data_numpy = np.array(mnist.data.values) mnist_target_numpy = np.array(mnist.target.values) mnist_data_numpy[:60000] = mnist_data_numpy[reorder_train] mnist_target_numpy[:60000] = mnist_target_numpy[reorder_train] mnist_data_numpy[60000:] = mnist_data_numpy[reorder_test + 60000] mnist_target_numpy[60000:] = mnist_target_numpy[reorder_test + 60000] return mnist_data_numpy, mnist_target_numpy . X, y = sort_by_target(mnist) . random_idx = 62123 digit_image, digit_label = X[random_idx], y[random_idx] print(&#39;The {0} entry is a photo of {1}&#39;.format(random_idx, digit_label)) random_digit_image=digit_image.reshape(28,28) plt.imshow(random_digit_image, cmap=cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;); . The 62123 entry is a photo of 2 . def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] . Before training we shuffle the data to ensure all cross-validation folds to be similar. Moreover some classficiation algorithms are sensitive to the order of training instances, and they perform poorly if they get many similar instances in a row. . import numpy as np index_shuffle = np.random.permutation(60000) X_train, y_train = X_train[index_shuffle], y_train[index_shuffle] . Binary classification . Here we will build a single digit classifier -- for example looking at just 2. Hence in total there will be only 2 classes -- Those which are 2 and those which are not. . y_train_2 = (y_train == 2) #True for all 2s, False for all other digits y_test_2 = (y_test == 2) . Using Stochastic Gradient Descent classifier. Known to handle large datasets very well. . from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42) sgd_clf.fit(X_train, y_train_2) . SGDClassifier(max_iter=5, random_state=42, tol=-inf) . sgd_clf.predict([digit_image]) . array([ True]) . Performance metrics . Evaluating classifiers is often significantly challenging than the case for a regressor wherein we can use RMSE or MAE. Let&#39;s look at some usual metrics used to gauge the classifier performance. . 1. Accuracy using Cross-validation . It involves splitting your training data in K-folds. Training the model on K-1 folds and testing it on the left out fold. Scikit learn has in-built method to do so: cross_val_score(). We can implement our own version as well. . $$ % Accuracy = frac{Correct}{Total} * 100 $$ . from sklearn.model_selection import StratifiedKFold from sklearn.base import clone skfolds = StratifiedKFold(n_splits=3, shuffle=False) for train_index, test_index in skfolds.split(X_train,y_train_2): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_2[train_index] X_test_folds = X_train[test_index] y_test_folds = y_train_2[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred=clone_clf.predict(X_test_folds) n_correct = sum(y_pred == y_test_folds) print(n_correct/len(y_pred)) . 0.97365 0.96255 0.96165 . from sklearn.model_selection import cross_val_score cross_val_score(sgd_clf, X_train, y_train_2, cv=3, scoring=&#39;accuracy&#39;) . array([0.97365, 0.96255, 0.96165]) . Does this high accuracy tell us anything? . Is the sample space we are looking at uniform enough for this accuracy? . Maybe we have way less one-digit samples for training in the first place. . _count=0. for i in range(len(y_train)): if y_train[i] == 2: _count=_count+1. print(_count/len(y_train)*100) . 9.93 . So ~9% of the sample are actually 2. So even if we guess ALWAYS that image is not 2 we will be right 90% of the time! . The dumb classifier . To check whether classifier accuracy of ~95% is good enough so just a over-exagerration . from sklearn.base import BaseEstimator class Never2(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return(np.zeros((len(X),1),dtype=bool)) . never2 = Never2() cross_val_score(never2,X_train,y_train_2,cv=3,scoring=&#39;accuracy&#39;) . array([0.9017, 0.9001, 0.9003]) . This shows our data is skewed! . 2. Confusion Matrix . General idea is to count the number of times instances of Class A are classified as Class B. . Table that describes the performance of a classification model by grouping predictions into 4 categories. . True Positives: we correctly predicted they do have diabetes | True Negatives: we correctly predicted they don’t have diabetes | False Positives: we incorrectly predicted they do have diabetes (Type I error) | False Negatives: we incorrectly predicted they don’t have diabetes (Type II error) | . The ROWS in the matrix are the real class-labels i.e. the TRUTH values while COLUMNS are the predicted values. . | Actual class (observation) . | . Predicted class (expectation) . | tp (true positive) Correct result . | fp (false positive) Unexpected result . | . fn (false negative) Missing result . | tn (true negative) Correct absence of result . | . from sklearn.model_selection import cross_val_predict y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3) . from sklearn.metrics import confusion_matrix confusion_matrix(y_train_2, y_train_pred) . array([[53159, 883], [ 1160, 4798]]) . Each row in the confusion matrix represent actual class, while each column represents a predicted class. Following are the terms of the confusion matrix: . First row of this matrix is the non-2 images -- (0,0) instances were correctly classified as non 2 (True Negative) | (0,1) instances were wrongly classified as 2s (False Positive) | . | Second row considers the images of 5 -- (1,0) instances were wrongly classified as non 2s (False negatives) | (1,1) instances were correctly classified as 2s (True positives) | . | An ideal classifier would be a diagonal matrix with no false positives or false negatives . Precision . Think of this as precision of the model in estimating the binary class. FROM POV of the MODEL . It is the ratio of the total classification whether as True or Wrongly classified as True to True. That is, $$Precision = frac{TP}{TP+FP}$$ . This is looking at +ve classification and how many are really +ve and how many are wrongly shown as +ve. So Precision looks at the prediction of +ve results. . Recall . Think of this as comparing to the actual data in the model training. FROM POV of the REAL DATA . It is the ratio of total classification on the +ve samples from where they are classified correctly (TP) to wrongly classified as negative (FN). . $$Recall = frac{TP}{FN+TP}$$ . So Recall looks at the prediction of the +ve samples. . This is by just comparing the +ve samples in the binary classification. To check how many of them are correctly recalled as +ve. . F1 score . Harmonic mean of recall and precision. Higher the Precision and Recall, lower are the instances of FP and FN. So we want to have higher Recall and Precision both. . F1 favors classifiers with similar recall and precision. . from sklearn.metrics import precision_score, recall_score, f1_score print(&#39;Precision score: {}&#39;.format(precision_score(y_train_2, y_train_pred))) print(&#39;Recall score: {}&#39;.format(recall_score(y_train_2, y_train_pred))) print(&#39;F1 score: {}&#39;.format(f1_score(y_train_2, y_train_pred))) . Precision score: 0.8445696180249956 Recall score: 0.8053037932192011 F1 score: 0.8244694561388435 . Recall/Precision tradeoff . Unfortunately increasing precision reduces recall and vise-versa. However sometimes one of the qualities could be desirable in a model. . Recall looks at lowering the False Negatives so culling +ve cases. That could be detrimental in catching robberies. So we need classifiers with high recall and we are okay with low Precision wherein we would get False alarms. . Meanwhile, if we are censoring videos we need high Precision to ensure unsafe videos categorised as Safe ones. While we could be removing good videos by wrongly classifying them to be Unsafe (low recall). . More discussion here: Link . Decision functions evaluate a decision_score we can manually set the threshold for the score to whether that will accpted or rejected for the binary case. . Increasing threshold reduces recall, but increases precision. . Why? The more Precise you want to be i.e. more True Positive than False Positives -- the higher the threshold for passing the case of accepting the data as a given class. However doing so we are strict in what we define as a ideal class and can neglect samples which are positive but are not closest to ideal. Hence we do incorrectly mark them as Negative thus increasing the case of False Negaitives and hence lowering Recall. . y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) . from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_2, y_scores) . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;, linewidth=2) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;, linewidth=2) plt.xlabel(&quot;Threshold&quot;, fontsize=10) plt.legend(loc=&quot;best&quot;, fontsize=12) plt.ylim([0, 1]) plt.figure(figsize=(8, 4)) plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.xlim([-700000, 700000]) plt.show() . plt.figure(figsize=(8, 4)) plt.plot(recalls[:-1],precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;, linewidth=2) plt.ylabel(&quot;Precision&quot;, fontsize=16) plt.xlabel(&quot;Recall&quot;, fontsize=16) . Text(0.5, 0, &#39;Recall&#39;) . If someone says let&#39;s reach 99% PRECISION, we must ALWAYS ask at what RECALL? . Manually set the Recall/Precision using threshold . y_scores = sgd_clf.decision_function([digit_image]) print(y_scores) y_pred_thresh = sgd_clf.predict([digit_image]) print(y_pred_thresh) #Setting threshold higher than the y_score threshold = y_scores + 1.0 y_pred_thresh = (y_scores &gt; threshold) print(y_pred_thresh) . [247991.40436599] [ True] [False] . y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) y_train_pred_90 = (y_scores &gt; 200000) print(&#39;Precision score: {}&#39;.format(precision_score(y_train_2, y_train_pred_90))) print(&#39;Recall score: {}&#39;.format(recall_score(y_train_2, y_train_pred_90))) print(&#39;F1 score: {}&#39;.format(f1_score(y_train_2, y_train_pred_90))) . Precision score: 0.9637028700056275 Recall score: 0.5748573346760658 F1 score: 0.720142977291842 . We have made classifier with an arbitrary Precision score: 97% However doing so we reduced the Recall. . The ROC curve . Another common tool used for binary classifiers apart from Precision/Recall. Instead of plotting precision vs recall we plot True Positive Rate (TPR) i.e. Recall against False Positive Rate (FPR). FPR is the ratio of negative instances that are incorrectly classified as positive. . ROC plots sensitivity vs 1-specificty . from sklearn.metrics import roc_curve #Decision scores for all instnces in the training set -- y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) fpr, tpr, thresholds = roc_curve(y_train_2, y_scores) def plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([0, 1, 0, 1]) plt.xlabel(&#39;False Positive Rate&#39;, fontsize=16) plt.ylabel(&#39;True Positive Rate&#39;, fontsize=16) plt.figure(figsize=(8, 6)) plot_roc_curve(fpr, tpr) plt.show() . from sklearn.metrics import roc_auc_score roc_auc_score(y_train_2, y_scores) . 0.9651158581307573 . PR curve when we care of precision -- getting False +ve and not so much of getting False -ve. We are okay with losing some +ve cases but for sure do not want to neglect any -ve ones. . Random forest classifier . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=10, random_state=42) y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_2, cv=3, method=&quot;predict_proba&quot;) . y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_2, y_scores_forest) . plt.figure(figsize=(8, 6)) plt.plot(fpr, tpr, &quot;b:&quot;, linewidth=2, label=&quot;SGD&quot;) plot_roc_curve(fpr_forest, tpr_forest, &quot;Random Forest&quot;) plt.legend(loc=&quot;lower right&quot;, fontsize=16) plt.show() . Multiclass classification . Multiclass classifiers are able to label and distinguish between more than two classes. Some algorithms such as Random Forest and Näive Bayes are capable of handling this directly. Having said that, Naive Baye&#39;s has shortcomming of considering class conditional independence and having discrete entries in the input. . OvA (One-versus-all classifiers): Herein, we would train n binary classifiers for n type of labels and see which n-th classifier has highest decision score. . | OvO (One-versus-one strategy): Binary classifier for every pair. So for n labels we will have n(n-1)/2 classifiers. . | . Error analysis . X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] index_shuffle = np.random.permutation(60000) X_train, y_train = X_train[index_shuffle], y_train[index_shuffle] sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42) sgd_clf.fit(X_train, y_train) y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3) y_scores = cross_val_predict(sgd_clf, X_train, y_train, cv=3, method=&quot;decision_function&quot;) conf_mx = confusion_matrix(y_train, y_train_pred) . plt.matshow(conf_mx, cmap=plt.cm.gray) plt.show() . Plotting heat-map for the errors in the classification . row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums #Diagonals are filled to be zero to concentrate only at the errors np.fill_diagonal(norm_conf_mx, 0) plt.matshow(norm_conf_mx, cmap=plt.cm.gray) plt.show() . ROWS in the confusion matrix are the REAL labels. COLUMNS in the confusion matrix are the PREDICTED values. It can seen that in the case of row 3 and column 5: . 5 is most of the times confused with 3 and 8 | 9 is confused with 4 and 7 | . def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = cm.binary, **options) plt.axis(&quot;off&quot;) . cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)] . plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5) plt.show() . Given above are two sets of &#39;3&#39; and &#39;5&#39; -- the boxes to the left are 3 and 5 classified as 3. Top left are the images of 3 classified as 3 while Bottom left are the images of 5 classified as 3. It can seen that some imags of 5 quite poor and the algorithm (which is linear in this case) will have difficulty predicting it. . Multi-label classifier . Determine a label such that it is a list for a every digit answering two questions: . Is this number odd? 1: Yes, 0: No | Is this number greater than 7? 1: Yes, 0: No | Creating new y_label for model . is_odd = (y % 2 == 0).astype(int) is_greater_7 = (y &gt; 7).astype(int) y_multilabel = np.c_[is_odd, is_greater_7] . from sklearn.neighbors import KNeighborsClassifier knn_clf = KNeighborsClassifier() . X_train, X_test, y_ml_train, y_ml_test = X[:60000], X[60000:], y_multilabel[:60000], y_multilabel[60000:] index_shuffle = np.random.permutation(60000) X_train, y_ml_train = X_train[index_shuffle], y_ml_train[index_shuffle] . knn_clf.fit(X_train, y_ml_train) . KNeighborsClassifier() . knn_clf.predict([X_train[12]]) . array([[0, 0]]) . y_ml_train[12] . array([0, 0]) . y_knn_ml_pred= cross_val_predict(knn_clf, X_train, y_ml_train, cv=3) . # another option is the &#39;weighted&#39; f1_score(y_ml_train, y_knn_ml_pred, average=&#39;macro&#39;) . 0.9719059410724892 .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2020/08/12/MNIST_Scikit_learn-Classification.html",
            "relUrl": "/python/machine-learning/2020/08/12/MNIST_Scikit_learn-Classification.html",
            "date": " • Aug 12, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "S&P 500 analysis using beautifulsoup and pandas",
            "content": "To extract stock information . To extract stock information we will use yfinance module which is a convenient way to download data from Yahoo Finance. The official API for Yahoo Finance was decommissioned some time back. More details about this module can be found here. . from requests import get import numpy as np import pandas as pd from bs4 import BeautifulSoup import time as time from tqdm import tqdm import yfinance as yf from IPython.core.display import clear_output . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns sns.set(style=&quot;whitegrid&quot;) sns.color_palette(&quot;husl&quot;) %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 30, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . 1. Generate list of S&amp;P 500 companies . Parse wikipedia to generate a list . wiki_url = &#39;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies&#39; response = get(wiki_url) html_soup = BeautifulSoup(response.text, &#39;html.parser&#39;) tab = html_soup.find(&quot;table&quot;,{&quot;class&quot;:&quot;wikitable sortable&quot;}) . column_headings = [entry.text.strip() for entry in tab.findAll(&#39;th&#39;)] print(column_headings) . [&#39;Symbol&#39;, &#39;Security&#39;, &#39;SEC filings&#39;, &#39;GICS Sector&#39;, &#39;GICS Sub-Industry&#39;, &#39;Headquarters Location&#39;, &#39;Date first added&#39;, &#39;CIK&#39;, &#39;Founded&#39;] . SP_500_dict = {keys:[] for keys in column_headings} . for i, name in enumerate(SP_500_dict.keys()): print(i, name) . 0 Symbol 1 Security 2 SEC filings 3 GICS Sector 4 GICS Sub-Industry 5 Headquarters Location 6 Date first added 7 CIK 8 Founded . Populate each row entry as per company data . for row_entry in tab.findAll(&#39;tr&#39;)[1:]: row_elements = row_entry.findAll(&#39;td&#39;) for key, _elements in zip(SP_500_dict.keys(), row_elements): SP_500_dict[key].append(_elements.text.strip()) . SP_500_df = pd.DataFrame(SP_500_dict, columns=SP_500_dict.keys()) . SP_500_df . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 0 MMM | 3M Company | reports | Industrials | Industrial Conglomerates | St. Paul, Minnesota | 1976-08-09 | 0000066740 | 1902 | . 1 ABT | Abbott Laboratories | reports | Health Care | Health Care Equipment | North Chicago, Illinois | 1964-03-31 | 0000001800 | 1888 | . 2 ABBV | AbbVie Inc. | reports | Health Care | Pharmaceuticals | North Chicago, Illinois | 2012-12-31 | 0001551152 | 2013 (1888) | . 3 ABMD | Abiomed | reports | Health Care | Health Care Equipment | Danvers, Massachusetts | 2018-05-31 | 0000815094 | 1981 | . 4 ACN | Accenture | reports | Information Technology | IT Consulting &amp; Other Services | Dublin, Ireland | 2011-07-06 | 0001467373 | 1989 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 500 YUM | Yum! Brands Inc | reports | Consumer Discretionary | Restaurants | Louisville, Kentucky | 1997-10-06 | 0001041061 | 1997 | . 501 ZBRA | Zebra Technologies | reports | Information Technology | Electronic Equipment &amp; Instruments | Lincolnshire, Illinois | 2019-12-23 | 0000877212 | 1969 | . 502 ZBH | Zimmer Biomet | reports | Health Care | Health Care Equipment | Warsaw, Indiana | 2001-08-07 | 0001136869 | 1927 | . 503 ZION | Zions Bancorp | reports | Financials | Regional Banks | Salt Lake City, Utah | 2001-06-22 | 0000109380 | 1873 | . 504 ZTS | Zoetis | reports | Health Care | Pharmaceuticals | Parsippany, New Jersey | 2013-06-21 | 0001555280 | 1952 | . 505 rows × 9 columns . SP_500_df[&#39;GICS Sector&#39;].value_counts() . Information Technology 75 Industrials 74 Financials 65 Consumer Discretionary 63 Health Care 62 Consumer Staples 32 Real Estate 29 Materials 28 Utilities 28 Communication Services 26 Energy 23 Name: GICS Sector, dtype: int64 . Visualize distribution of the companies as per sectors . fig, ax = plt.subplots(1,1, figsize=(10,10)) SP_500_df[&#39;GICS Sector&#39;].value_counts().plot.pie(y=&#39;GICS Sector&#39;, autopct=&#39;%1.1f%%&#39;, fontsize=20, ax = ax, colormap=&#39;tab20&#39;) plt.axis(&#39;off&#39;) . (-1.25, 1.25, -1.25, 1.25) . SP_500_df.loc[ SP_500_df[&#39;GICS Sector&#39;] == &#39;Energy&#39;] . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 44 APA | APA Corporation | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1997-07-28 | 0000006769 | 1954 | . 59 BKR | Baker Hughes Co | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 2017-07-07 | 0001701605 | 2017 | . 80 COG | Cabot Oil &amp; Gas | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 2008-06-23 | 0000858470 | 1989 | . 101 CVX | Chevron Corp. | reports | Energy | Integrated Oil &amp; Gas | San Ramon, California | 1957-03-04 | 0000093410 | 1879 | . 121 COP | ConocoPhillips | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1957-03-04 | 0001163165 | 2002 | . 140 DVN | Devon Energy | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Oklahoma City, Oklahoma | 2000-08-30 | 0001090012 | 1971 | . 142 FANG | Diamondback Energy | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Midland, Texas | 2018-12-03 | 0001539838 | 2007 | . 169 EOG | EOG Resources | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 2000-11-02 | 0000821189 | 1999 | . 183 XOM | Exxon Mobil Corp. | reports | Energy | Integrated Oil &amp; Gas | Irving, Texas | 1957-03-04 | 0000034088 | 1999 | . 219 HAL | Halliburton Co. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 1957-03-04 | 0000045012 | 1919 | . 227 HES | Hess Corporation | reports | Energy | Integrated Oil &amp; Gas | New York, New York | 1984-05-31 | 0000004447 | 1919 | . 230 HFC | HollyFrontier Corp | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Dallas, Texas | 2018-06-18 | 0000048039 | 1947 | . 274 KMI | Kinder Morgan | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Houston, Texas | 2012-05-25 | 0001506307 | 1997 | . 298 MRO | Marathon Oil Corp. | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1991-05-01 | 0000101778 | 1887 | . 299 MPC | Marathon Petroleum | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Findlay, Ohio | 2011-07-01 | 0001510295 | 2009 (1887) | . 345 NOV | NOV Inc. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 2005-03-14 | 0001021860 | 1841 | . 352 OXY | Occidental Petroleum | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1982-12-31 | 0000797468 | 1920 | . 355 OKE | Oneok | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Tulsa, Oklahoma | 2010-03-15 | 0001039684 | 1906 | . 372 PSX | Phillips 66 | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Houston, Texas | 2012-05-01 | 0001534701 | 2012 (1917) | . 374 PXD | Pioneer Natural Resources | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Irving, Texas | 2008-09-24 | 0001038357 | 1997 | . 411 SLB | Schlumberger Ltd. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Curaçao, Kingdom of the Netherlands | 1965-03-31 | 0000087347 | 1926 | . 466 VLO | Valero Energy | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | San Antonio, Texas | | 0001035002 | 1980 | . 494 WMB | Williams Companies | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Tulsa, Oklahoma | 1975-03-31 | 0000107263 | 1908 | . We can parse these tables and search companies based on the sector . SP_500_df.loc[ SP_500_df[&#39;GICS Sector&#39;] == &#39;Information Technology&#39;] . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 4 ACN | Accenture | reports | Information Technology | IT Consulting &amp; Other Services | Dublin, Ireland | 2011-07-06 | 0001467373 | 1989 | . 6 ADBE | Adobe Inc. | reports | Information Technology | Application Software | San Jose, California | 1997-05-05 | 0000796343 | 1982 | . 7 AMD | Advanced Micro Devices | reports | Information Technology | Semiconductors | Santa Clara, California | 2017-03-20 | 0000002488 | 1969 | . 13 AKAM | Akamai Technologies | reports | Information Technology | Internet Services &amp; Infrastructure | Cambridge, Massachusetts | 2007-07-12 | 0001086222 | 1998 | . 38 APH | Amphenol Corp | reports | Information Technology | Electronic Components | Wallingford, Connecticut | 2008-09-30 | 0000820313 | 1932 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 475 V | Visa Inc. | reports | Information Technology | Data Processing &amp; Outsourced Services | San Francisco, California | 2009-12-21 | 0001403161 | 1958 | . 489 WDC | Western Digital | reports | Information Technology | Technology Hardware, Storage &amp; Peripherals | San Jose, California | 2009-07-01 | 0000106040 | 1970 | . 490 WU | Western Union Co | reports | Information Technology | Data Processing &amp; Outsourced Services | Englewood, Colorado | 2006-09-29 | 0001365135 | 1851 | . 498 XLNX | Xilinx | reports | Information Technology | Semiconductors | San Jose, California | 1999-11-08 | 0000743988 | 1984 | . 501 ZBRA | Zebra Technologies | reports | Information Technology | Electronic Equipment &amp; Instruments | Lincolnshire, Illinois | 2019-12-23 | 0000877212 | 1969 | . 75 rows × 9 columns . Get total number of Shares . We will use yfinance to extact Tickr information for each SP500 company and use pandas datareader . yf_tickr = yf.Ticker(&#39;ADBE&#39;) yf_tickr.info[&#39;sharesOutstanding&#39;] #info has good summary info for the stock . import yfinance as yf . START_DATE = &quot;2020-01-01&quot; END_DATE = &quot;2020-07-26&quot; . yf_tickr = yf.Ticker(&#39;TSLA&#39;) . _shares_outstanding = yf_tickr.info[&#39;sharesOutstanding&#39;] _previous_close = yf_tickr.info[&#39;previousClose&#39;] print(&#39;Outstanding shares: {}&#39;.format(_shares_outstanding)) print(&#39;Market Cap: {} Million USD&#39;.format((_shares_outstanding * _previous_close)/10**6)) . Outstanding shares: 959854016 Market Cap: 676447.51923584 Million USD . df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, interval=&quot;1wk&quot;, actions=False) df_tckr[&#39;Market_Cap&#39;] = df_tckr[&#39;Open&#39;] * _shares_outstanding df_tckr[&#39;YTD&#39;] = (df_tckr[&#39;Open&#39;] - df_tckr[&#39;Open&#39;][0]) * 100 / df_tckr[&#39;Open&#39;][0] . fig, ax = plt.subplots(1,1, figsize=(10,8)) df_tckr.plot(use_index=True, y=&quot;YTD&quot;,ax=ax, linewidth=4, grid=False, label=&#39;TSLA&#39;) ax.set_xlabel(&#39;Date&#39;) ax.set_ylabel(&#39;% YTD change (Weekly basis)&#39;) . Text(0, 0.5, &#39;% YTD change (Weekly basis)&#39;) . Extend this to plotting for multiple companies . import time as time def plot_market_cap(tickr_list, START_DATE, END_DATE): total_data = {} for tickr in tickr_list: total_data[tickr] = {} print(&#39;Looking at: {}&#39;.format(tickr)) yf_tickr = yf.Ticker(tickr) #try: # _shares_outstanding = yf_tickr.info[&#39;sharesOutstanding&#39;] #except(IndexError): # print(&#39;Shares outstanding not found&#39;) # _shares_outstanding = None df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, actions=False) df_tckr[&#39;YTD&#39;] = (df_tckr[&#39;Open&#39;] - df_tckr[&#39;Open&#39;][0]) * 100 / df_tckr[&#39;Open&#39;][0] total_data[tickr][&#39;hist&#39;] = df_tckr #total_data[tickr][&#39;shares&#39;] = _shares_outstanding time.sleep(np.random.randint(10)) return total_data . tickr_list = [&#39;AAPL&#39;, &#39;TSLA&#39;,&#39;FB&#39;,&#39;DAL&#39;,&#39;XOM&#39;] data = plot_market_cap(tickr_list, START_DATE, END_DATE) . Looking at: AAPL Looking at: TSLA Looking at: FB Looking at: DAL Looking at: XOM . company_name = [SP_500_df[SP_500_df[&#39;Symbol&#39;].str.contains(i)][&#39;Security&#39;].values[0] for i in tickr_list] . company_name . [&#39;Apple Inc.&#39;, &#39;Tesla, Inc.&#39;, &#39;Facebook, Inc.&#39;, &#39;Delta Air Lines Inc.&#39;, &#39;Exxon Mobil Corp.&#39;] . print(len(data[&#39;AAPL&#39;][&#39;hist&#39;][&#39;YTD&#39;])) . 142 . ytd_stat = pd.DataFrame() for tickr in tickr_list: ytd_stat[tickr] = data[tickr][&#39;hist&#39;][&#39;YTD&#39;].values ytd_stat[&#39;Date&#39;] = data[&#39;AAPL&#39;][&#39;hist&#39;].index . ytd_stat . AAPL TSLA FB DAL XOM Date . 0 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2020-01-02 | . 1 0.307187 | 3.769137 | 0.222494 | -2.426609 | 1.566061 | 2020-01-03 | . 2 -0.827016 | 3.762073 | -0.024185 | -3.292044 | 0.113891 | 2020-01-06 | . 3 1.215244 | 8.692576 | 2.935916 | -1.730873 | 0.370157 | 2020-01-07 | . 4 0.310568 | 11.590101 | 3.022975 | -2.002382 | -0.185078 | 2020-01-08 | . ... ... | ... | ... | ... | ... | ... | . 137 30.850611 | 257.835096 | 16.111244 | -53.866312 | -36.389268 | 2020-07-20 | . 138 34.589490 | 286.320361 | 19.090690 | -54.720640 | -36.477584 | 2020-07-21 | . 139 31.223803 | 276.678424 | 16.207978 | -55.181977 | -35.005460 | 2020-07-22 | . 140 31.637726 | 295.512370 | 15.903267 | -55.574967 | -36.109559 | 2020-07-23 | . 141 23.481414 | 233.571249 | 11.337365 | -54.754814 | -35.402933 | 2020-07-24 | . 142 rows × 6 columns . Final plot for returns . fig, ax = plt.subplots(1,1,figsize=(15,10)) for i, tickr in enumerate(tickr_list): ax.plot(ytd_stat[&#39;Date&#39;], ytd_stat[tickr], linewidth=5.0, label=company_name[i]) ax.set_ylabel(&#39;YTD %Return 2020&#39;) ax.set_xlabel(&#39;Date&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f9c0a4365e0&gt; .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/exploratory-data-analysis/data-visualization/web-scrapping/2020/08/01/SP_500.html",
            "relUrl": "/python/exploratory-data-analysis/data-visualization/web-scrapping/2020/08/01/SP_500.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Plotting surface in matplotlib",
            "content": "This is adapted from the following Tutorial: Link . import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; . fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) x = np.array([[1, 3], [2, 4]]) #Array format: [[a,b],[c,d]] -- a b are in row; c d are in row y = np.array([[5, 6], [7, 8]]) z = np.array([[9, 12], [10, 11]]) ax.plot_surface(x, y, z) ax.set(xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, zlabel=&#39;z&#39;) fig.tight_layout() . Meshgrid . Mesh is important to create a surface since just looking at the x, y vector by themselves what you would look at is the diagonal of the matrix formed by combination of all the possible x values with y values. For the given x and y vector, every entry in x vector can have the entire y vector as a possible point. So it is important to generate an array which captures all these possible pairing. . So using mesh-grid if x-vector is of dimensions M and y-vector is of dimensions N -- the final resulting matrix is NxM dimensions where every $n^{th}$ entry in y all the entries of x are added. Finally the ouput is given as x coordinate of that matrix and y coordinate of that matrix. . Example: . $X$ : $ begin{bmatrix} x_{1} &amp; x_{2} &amp; x_{3} end{bmatrix}$ | $Y$ : $ begin{bmatrix} y_{1} &amp; y_{2} end{bmatrix}$ | . Then resulting mesh would be: $$ X-Y-Mesh = begin{bmatrix} x_{1}y_{1} &amp; x_{2}y_{1} &amp; x_{3}y_{1} x_{1}y_{2} &amp; x_{2}y_{2} &amp; x_{3}y_{2} end{bmatrix}$$ . $$ X-path = begin{bmatrix} x_{1} &amp; x_{2} &amp; x_{3} x_{1} &amp; x_{2} &amp; x_{3} end{bmatrix}$$ . $$ X-path = begin{bmatrix} y_{1} &amp; y_{1} &amp; y_{1} y_{2} &amp; y_{2} &amp; y_{2} end{bmatrix}$$ . x_axis_range = np.arange(-2,2.1,1) y_axis_range = np.arange(-4,4.1,1) #Make the meshgrid for the x and y (x,y) = np.meshgrid(x_axis_range, y_axis_range, sparse=True) . z = x + y . fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) ax.plot_surface(x, y, z) fig.tight_layout() . Plotting this 2D function: $$ z = e^{- sqrt {x^2 + y^2}}cos(4x)cos(4y) $$ using the surface . import matplotlib.cm as cm x_axis_bound = np.linspace(-1.8,1.8,100) y_axis_bound = np.linspace(-1.8,1.8,100) (x,y) = np.meshgrid(x_axis_bound, y_axis_bound, sparse=True) def f(x,y): return np.exp(-np.sqrt( x**2 + y**2 )) * np.cos(4*x) * np.cos(4*y) Z = f(x,y) fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) ax.plot_surface(x, y, Z, cmap=cm.hot) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) fig.tight_layout() .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-visualization/machine-learning/2020/07/18/creating_meshes.html",
            "relUrl": "/python/data-visualization/machine-learning/2020/07/18/creating_meshes.html",
            "date": " • Jul 18, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Activation functions",
            "content": "Function that activates the particular neuron or node if the value across a particular threshold. These functions add the necessary non-linearity in the ANNs. Each perceptron is, in reality (and traditionally), a logistic regression unit. When N units are stacked on top of each other we get a basic single layer perceptron which serves as the basis of Artificial neural network. . Click here for Google&#39;s ML glossary definition . There are different types of activation function and each has its benefits and faults. One of the consideration is the ease in evaluation of the gradient. It should be easy but also help in the final learning process by translating the necessary abstraction and non-linearity across the network. Some of the activation functions are primarily used to model the output of the ANN. Traditionally for a classification task, we would use a sigmoid activation function for a binary classification to predict a binary output (yes/no). In the case of multi-class classification that activation is replaced by softmax activation to estimate the &#39;probability&#39; across different classes. . Some of the traditionally used Activation functions: . Sigmoid activaton function | tanh (hyperbolic tangent) activaton function | ReLU activaton function | Leaky ReLU activaton function | Softplus function | Softmax function | . import numpy as np import matplotlib.pyplot as plt %config InlineBackend.figure_format = &#39;retina&#39; import seaborn as sns sns.set_palette(&quot;deep&quot;) . ## Baseline reference . z = np.linspace(-10,10,100) . Sigmoid activation function . def sigmoid(z): return 1/(1+np.exp(-z)) # derivative of Sigmoid Function def dsigmoid(a): return a*(1-a) # returns a derivative od sigmoid function if a=sigmoid then a&#39;=a(1-a) . plt.plot(z, sigmoid(z), label = r&#39;$sigmoid$&#39;) plt.plot(z, dsigmoid(sigmoid(z)), label = r&#39;$ frac{ partial (sigmoid)}{ partial z}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.show() . import torch x = torch.tensor(z, requires_grad=True) print(x.requires_grad) b = torch.sigmoid(x) . True . x . tensor([-10.0000, -9.7980, -9.5960, -9.3939, -9.1919, -8.9899, -8.7879, -8.5859, -8.3838, -8.1818, -7.9798, -7.7778, -7.5758, -7.3737, -7.1717, -6.9697, -6.7677, -6.5657, -6.3636, -6.1616, -5.9596, -5.7576, -5.5556, -5.3535, -5.1515, -4.9495, -4.7475, -4.5455, -4.3434, -4.1414, -3.9394, -3.7374, -3.5354, -3.3333, -3.1313, -2.9293, -2.7273, -2.5253, -2.3232, -2.1212, -1.9192, -1.7172, -1.5152, -1.3131, -1.1111, -0.9091, -0.7071, -0.5051, -0.3030, -0.1010, 0.1010, 0.3030, 0.5051, 0.7071, 0.9091, 1.1111, 1.3131, 1.5152, 1.7172, 1.9192, 2.1212, 2.3232, 2.5253, 2.7273, 2.9293, 3.1313, 3.3333, 3.5354, 3.7374, 3.9394, 4.1414, 4.3434, 4.5455, 4.7475, 4.9495, 5.1515, 5.3535, 5.5556, 5.7576, 5.9596, 6.1616, 6.3636, 6.5657, 6.7677, 6.9697, 7.1717, 7.3737, 7.5758, 7.7778, 7.9798, 8.1818, 8.3838, 8.5859, 8.7879, 8.9899, 9.1919, 9.3939, 9.5960, 9.7980, 10.0000], dtype=torch.float64, requires_grad=True) . b.backward(torch.ones(x.shape)) . x.grad . tensor([4.5396e-05, 5.5558e-05, 6.7994e-05, 8.3213e-05, 1.0184e-04, 1.2463e-04, 1.5252e-04, 1.8666e-04, 2.2843e-04, 2.7954e-04, 3.4207e-04, 4.1859e-04, 5.1221e-04, 6.2673e-04, 7.6682e-04, 9.3817e-04, 1.1477e-03, 1.4039e-03, 1.7172e-03, 2.1000e-03, 2.5677e-03, 3.1389e-03, 3.8362e-03, 4.6869e-03, 5.7241e-03, 6.9876e-03, 8.5250e-03, 1.0394e-02, 1.2661e-02, 1.5407e-02, 1.8724e-02, 2.2721e-02, 2.7521e-02, 3.3259e-02, 4.0084e-02, 4.8151e-02, 5.7615e-02, 6.8615e-02, 8.1257e-02, 9.5592e-02, 1.1158e-01, 1.2906e-01, 1.4771e-01, 1.6703e-01, 1.8633e-01, 2.0471e-01, 2.2118e-01, 2.3471e-01, 2.4435e-01, 2.4936e-01, 2.4936e-01, 2.4435e-01, 2.3471e-01, 2.2118e-01, 2.0471e-01, 1.8633e-01, 1.6703e-01, 1.4771e-01, 1.2906e-01, 1.1158e-01, 9.5592e-02, 8.1257e-02, 6.8615e-02, 5.7615e-02, 4.8151e-02, 4.0084e-02, 3.3259e-02, 2.7521e-02, 2.2721e-02, 1.8724e-02, 1.5407e-02, 1.2661e-02, 1.0394e-02, 8.5250e-03, 6.9876e-03, 5.7241e-03, 4.6869e-03, 3.8362e-03, 3.1389e-03, 2.5677e-03, 2.1000e-03, 1.7172e-03, 1.4039e-03, 1.1477e-03, 9.3817e-04, 7.6682e-04, 6.2673e-04, 5.1221e-04, 4.1859e-04, 3.4207e-04, 2.7954e-04, 2.2843e-04, 1.8666e-04, 1.5252e-04, 1.2463e-04, 1.0184e-04, 8.3213e-05, 6.7994e-05, 5.5558e-05, 4.5396e-05], dtype=torch.float64) . plt.plot(x.data.numpy(), b.data.numpy(), label = r&#39;$sigmoid$&#39;) plt.plot(x.data.numpy(), x.grad.data.numpy(), label = r&#39;$ frac{ partial (sigmoid)}{ partial z}$&#39;) plt.legend(fontsize = 12) . &lt;matplotlib.legend.Legend at 0x7f8b5f3ece48&gt; . np.unique(np.round((x.grad.data.numpy() - dsigmoid(sigmoid(z))),4)) . array([0.]) . Hyperbolic tangent activation function . def tanh(z): return np.tanh(z) # derivative of tanh def dtanh(a): return 1-np.power(a,2) . plt.plot(z, tanh(z),&#39;b&#39;, label = &#39;tanh&#39;) plt.plot(z, dtanh(tanh(z)),&#39;r&#39;, label=r&#39;$ frac{dtanh}{dz}$&#39;) plt.legend(fontsize = 12) plt.show() . ReLU (Rectified Linear Unit) Activation function . def ReLU(z): return np.maximum(0,z) # derivative of ReLu def dReLU(a): return 1*(a&gt;0) . plt.plot(z, ReLU(z),&#39;b&#39;, label =&#39;ReLU&#39;) plt.plot(z, dReLU(ReLU(z)),&#39;r&#39;, label=r&#39;$ frac{dReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.ylim(0,4) plt.xlim(-4,4) plt.show() . Leaky ReLU Activation function . def LeakyReLU(z): return np.maximum(0.01*z,z) # derivative of ReLu def dLeakyReLU(a): return 0.01*(a&gt;0) . plt.plot(z, LeakyReLU(z),&#39;b&#39;, label = &#39;LeakyReLU&#39;) plt.plot(z, dLeakyReLU(LeakyReLU(z)),&#39;r&#39;, label=r&#39;$ frac{dLeakyReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.ylim(0,4) plt.xlim(-4,4) plt.show() . Comparison of derivative for activation functions . plt.plot(z, dsigmoid(sigmoid(z)),label = r&#39;$ frac{dsigmoid}{dz}$&#39; ) plt.plot(z, dtanh(tanh(z)), label = r&#39;$ frac{dtanh}{dz}$&#39;) plt.plot(z, dReLU(ReLU(z)), label=r&#39;$ frac{dReLU}{dz}$&#39;) plt.plot(z, dLeakyReLU(LeakyReLU(z)), label=r&#39;$ frac{dLeakyReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.title(&#39;Derivatives of activation functions&#39;) plt.show() .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/pytorch/2020/04/22/activation_functions.html",
            "relUrl": "/python/machine-learning/pytorch/2020/04/22/activation_functions.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Convolutional neural network example",
            "content": "This tutorial is adopted from Python-Engineer&#39;s Pytorch Tutorial | Video . Good reading links: . CMU&#39;s CS231 Course | . Convolutional neural network is used to train CIFAR-10 dataset. It is implemented in PyTorch . What does it consists of? . The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. . The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. . What&#39;s a CNN? . Same as NN but are optimized for image analysis. Before training the weights and biases in the full-connected layer the training data is &#39;screened and filtered&#39; to tease out relevant features of each image by passing each image through a prescribed filter and &#39;convolutions&#39;. Think of it like passing a colored lens or fancy ink to selectively look at edges, contrasts, shapes in the image. . Finally that a projection of that image is made by &#39;pooling&#39; which is a way of down-sampling the resulting convolution as a new data-point. . Schematic CNN architecture . import torch import torch.nn as nn #NeuralNetwork module import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt import numpy as np # Device configuration device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) dataset_dir = &#39;data/&#39; # Hyper-parameters num_epochs = 5 batch_size = 4 learning_rate = 0.001 . Loading the data . Dataset has PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1] . # We transform them to Tensors of normalized range [-1, 1] transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class #Importing the training set for the CIFAR10 dataset train_dataset = torchvision.datasets.CIFAR10(root=dataset_dir, train=True, download=True, transform=transform) #Importing the testing set for the CIFAR10 dataset test_dataset = torchvision.datasets.CIFAR10(root=dataset_dir, train=False, download=True, transform=transform) . Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz Extracting data/cifar-10-python.tar.gz to data/ Files already downloaded and verified . def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() . train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) . dataiter = iter(train_loader) images, labels = dataiter.next() # show images imshow(torchvision.utils.make_grid(images)) print(&#39;Images of: {}&#39;.format([classes[i] for i in labels])) print(&#39;Size of the image array for a given batch: {}&#39;.format(images.shape)) . Images of: [&#39;truck&#39;, &#39;horse&#39;, &#39;deer&#39;, &#39;ship&#39;] Size of the image array for a given batch: torch.Size([4, 3, 32, 32]) . Testing the convolutions . Before implementing the CNN for the image recognition let&#39;s see what the convolutions and the pooling layers do the images . Convolution is the first layer to extract features from an input image. It preserves the relationship between pixels by learning images features using small squares of input data. It&#39;s a matrix operation that takes two inputs -- image matrix and a filter/kernel . Two main hyper-parameters for the pooling layers: . Stride -- controls how filters &#39;slides&#39; on the input volume. Stride is normally set in a way so that the output volume is an integer and not a fraction. Increase the stride if you want receptive fields to overlap less and want smaller spatial dimensions . | Padding -- Image matrix multiplies kernel or filter matrix . | 3 x 3 Output matrix . Calculating the output size of the image after convolutions: . To calculate the output size of the image after convolution layer: . $$O = frac{W - F + 2P}{S} + 1$$ . where O is the output height/length, W is the input height/length, F is the filter size, P is the padding, and S is the stride. . Pooling layers . Pooling layers section would reduce the number of parameters when the images are too large. Spatial pooling also called subsampling or downsampling which reduces the dimensionality of each map but retains important information. Spatial pooling can be of different types: . Max Pooling | Average Pooling | Sum Pooling | Max pooling takes the largest element from the rectified feature map. Taking the largest element could also take the average pooling. Sum of all elements in the feature map call as sum pooling. . Max pooling scheme . conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0) pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) #Take max of the 2x2 array and shift by 2 conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0) . print(images.shape) x = conv1(images) print(x.shape) . torch.Size([4, 3, 32, 32]) torch.Size([4, 6, 28, 28]) . x = pool(x) print(x.shape) . torch.Size([4, 6, 14, 14]) . x = conv2(x) print(x.shape) . torch.Size([4, 16, 10, 10]) . x = pool(x) print(x.shape) . torch.Size([4, 16, 5, 5]) . Building the CNN class . class ConvNet(nn.Module): &#39;&#39;&#39; Inherit from the nn.Module all the necessary routines super() is in the business of delegating method calls to some class in the instance’s ancestor tree. Conv1 = First convolution 3 color channels (RGB) to 6 output, filter size=5 pool = Max pool layer of 2x2 and stride of 2 ie. we shift 2 pixel to the right after each pooling operations Conv2 = Second convolution layer with 6 input channel and 16 output channel, filter size of 5 Full connected layer FC1 = Flatten output of the final convolution + pooling (16 * 5 * 5) to 120 dim array FC2 = 120 to 84 FC3 = 84 to no of hidden equal to that of class labels Forward operation -- images --&gt; conv --&gt; relu --&gt; pool --&gt; conv2 --&gt; relu --&gt; pool Flatten pooled output --&gt; FC (w/ relu) --&gt; FC (relu) --&gt; FC --&gt; output &#39;&#39;&#39; def __init__(self): super(ConvNet, self).__init__() #Here we built the architecture for the CNN #First conv1 function instantiation self.conv1 = nn.Conv2d(3,6,5) #General purpose pooling self.pool = nn.MaxPool2d(2,2) #Second conv2 function instantiation self.conv2 = nn.Conv2d(6,16,5) #1st NN layer self.fc1 = nn.Linear(16*5*5,120) #2nd NN layer self.fc2 = nn.Linear(120,84) #Final output layer self.fc3 = nn.Linear(84,10) def forward(self, x): #Two pooling operations x = self.pool(F.relu(self.conv1(x))) # -&gt; n, 6, 14, 14 x = self.pool(F.relu(self.conv2(x))) # -&gt; n, 16, 5, 5 #Flatten the output from pooling/convoltions x = x.view(-1, 16 * 5 * 5) # -&gt; n, 400 x = F.relu(self.fc1(x)) # -&gt; n, 120 x = F.relu(self.fc2(x)) # -&gt; n, 84 x = self.fc3(x) # -&gt; n, 10 return x . Defining the training criterion and optmizer . model = ConvNet().to(device) #For multiclass classification -- crossentropy loss criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) . Train the CNN on the test dataset . n_total_steps = len(train_loader) for epoch in range(num_epochs): for i, (images,labels) in enumerate(train_loader): # origin shape: [4, 3, 32, 32] = 4, 3, 1024 # input_layer: 3 input channels, 6 output channels, 5 kernel size images = images.to(device) labels = labels.to(device) #Forward pass outputs = model(images) loss = criterion(outputs, labels) #Backward prop and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 2000 == 0: print (f&#39;Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}&#39;) print(&#39;Finished Training&#39;) PATH = &#39;./cnn.pth&#39; torch.save(model.state_dict(), PATH) . with torch.no_grad(): #We dont need backward propogation here n_correct = 0 n_samples = 0 n_class_correct = [0 for i in range(10)] n_class_samples = [0 for i in range(10)] for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) # max returns (value ,index) _, predicted = torch.max(outputs, 1) n_samples += labels.size(0) n_correct += (predicted == labels).sum().item() for i in range(batch_size): label = labels[i] pred = predicted[i] if (label == pred): n_class_correct[label] += 1 n_class_samples[label] += 1 acc = 100.0 * n_correct / n_samples print(f&#39;Accuracy of the network: {acc} %&#39;) for i in range(10): acc = 100.0 * n_class_correct[i] / n_class_samples[i] print(f&#39;Accuracy of {classes[i]}: {acc} %&#39;) . Accuracy of the network: 48.28 % Accuracy of plane: 55.7 % Accuracy of car: 62.1 % Accuracy of bird: 26.1 % Accuracy of cat: 34.2 % Accuracy of deer: 31.0 % Accuracy of dog: 33.3 % Accuracy of frog: 75.1 % Accuracy of horse: 50.0 % Accuracy of ship: 58.7 % Accuracy of truck: 56.6 % .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/machine-learning/2020/03/17/cnn_pytorch_tutorial.html",
            "relUrl": "/python/data-analysis/machine-learning/2020/03/17/cnn_pytorch_tutorial.html",
            "date": " • Mar 17, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Implement Support Vector Machines in scikit-learn",
            "content": "This tutorial is borrowed from Jake VanderPlas&#39;s example of SVM in his notebook: Python Data Science Handbook . Motivation for Support Vector Machines . We want to find a line/curve (in 2D) or a manifold (in n-D) that divides the class from each other. This is a type of Discriminative Classification | Consider a simple case of classification task, in which the two classes of points are well separated | . import os import numpy as np import matplotlib.pyplot as plt from scipy import stats random_state = 42 . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns; sns.set() %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 30, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=200, centers=2, random_state=random_state, cluster_std=1.5) . print(&#39;X is a {} array with x-y coordinates of the cluster points n&#39;.format(np.shape(X))) print(X[:3]) print(&#39; n&#39;) print(&#39;y is a {} array with a classification of the points to which cluster they belong to n&#39;.format(np.shape(y))) print(y[:3]) print(&#39; n&#39;) plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap=&#39;autumn&#39;); . X is a (200, 2) array with x-y coordinates of the cluster points [[2.24823735 1.07410715] [5.12395668 0.73232327] [4.6766441 2.72016712]] y is a (200,) array with a classification of the points to which cluster they belong to [1 1 1] . For two-dimensional data, as observed in this case, a linear discriminative classifier would attempt to draw a straight line separating the two data-sets and thereby creating a model for (binary) classification. For the 2D data like the shown above, this task could be done by hand. But there is more than one line that can divide this data in two halves! . x_fit = np.linspace(min(X[:,0]),max(X[:,0])) fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) # Random point in the 2D plain ax.plot([-2],[4],&#39;x&#39;,color=&#39;blue&#39;, markeredgewidth=2, markersize=10) # Plot various 2D planes separating the two &#39;blobs&#39; for m,b in [(3.5,5),(2,5),(0.9,5)]: plt.plot(x_fit, x_fit*m+b, &#39;-k&#39;) plt.xlim(min(X[:,0]),max(X[:,0])) plt.ylim(min(X[:,1]),max(X[:,1])) . (-0.9549620153430207, 13.094539878082749) . What&#39;s a better methodology to determine the cutting plane? Something like k-nearest neighbor clustering wherein you find the plane with best separation from the two clusters based on some distance metric. However, k-nearest neighbors is based on non-parametric method -- needing to be estimated everytime a new data point is introduced. . What if I want something which is learned and then used as a function every other time a new datum is to be classified. This is where support vector machines (SVM) are useful. . Support Vector Machines: . Rather than simply drawing a zero-width line between classes, we can draw round each line a margin of some width, up to the nearest point. . x_fit = np.linspace(min(X[:,0]),max(X[:,0])) plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) plt.plot([-2],[4],&#39;x&#39;,color=&#39;blue&#39;, markeredgewidth=2, markersize=10) for m, b, d in [(3.5,5,0.33),(2,5,0.55),(0.9,5,0.8)]: y_fit = x_fit*m + b plt.plot(x_fit, y_fit, &#39;-k&#39;) plt.fill_between(x_fit, y_fit-d, y_fit+d, edgecolor=&#39;none&#39;, color=&#39;#AAAAAA&#39;, alpha=0.4) plt.xlim(min(X[:,0]),max(X[:,0])) plt.ylim(min(X[:,1]),max(X[:,1])) . (-0.9549620153430207, 13.094539878082749) . In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator. . Fitting a support vector machine model . Using Scikit-learn&#39;s SVM module to train a classifier on the above data. We will use a linear-kernel and set C parameters to a very large value. . from sklearn.svm import SVC model = SVC(kernel=&#39;linear&#39;,C=1E10) model.fit(X,y) . SVC(C=10000000000.0, kernel=&#39;linear&#39;) . To better appreciate the SVM classification logic, we use a convenience function to visualize the decision boundary as made by the SVM module. Code is adopted from Jake&#39;s tutorial. . def plot_svc_decision_function(model, ax=None, plot_support=True, list_vectors=False): &quot;&quot;&quot;Plot the decision function for a 2D SVC&quot;&quot;&quot; if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() # create grid to evaluate model x = np.linspace(xlim[0], xlim[1], 30) y = np.linspace(ylim[0], ylim[1], 30) Y, X = np.meshgrid(y, x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) # plot decision boundary and margins ax.contour(X, Y, P, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) # plot support vectors if plot_support: ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;,linestyle=&#39;--&#39;); if list_vectors: print(model.support_vectors_) ax.set_xlim(xlim) ax.set_ylim(ylim) . plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(model) . This is the dividing line that maximizes the margin between the two sets of points. . Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure. . These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. . In Scikit-Learn, the identity of these points are stored in the supportvectors attribute of the classifier. . model.support_vectors_ . array([[-0.40500616, 6.91150953], [ 2.65952903, 4.72035783], [ 2.07017704, 4.00397825]]) . A key to this classifier&#39;s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! . Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin. . We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset: . def plot_svm(N=10, ax=None): X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.60) X = X[:N] y = y[:N] model = SVC(kernel=&#39;linear&#39;, C=1E10) model.fit(X, y) ax = ax or plt.gca() ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plot_svc_decision_function(model, ax) fig, ax = plt.subplots(1, 2, figsize=(16, 6)) fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1) for axi, N in zip(ax, [60, 120]): plot_svm(N, axi) axi.set_title(&#39;N = {0}&#39;.format(N)) . In spite of increasing the training points, once the margins and the corresponding support vectors are identified the model does not change. This is one of the strengths of this algorithm . from ipywidgets import interact, fixed interact(plot_svm, N=[10, 50, 100, 150, 200], ax=fixed(None)); . Beyond linear kernels: Kernel SVM . Kernels are helpful in projecting data into higher dimensional feature space. This can be useful in simplest case to fit non-linear data using linear regression models. Similarly in the case of SVM: Projecting the data into higher dimensions through either polynomial or gaussian kernels we can fit non-linear relationships to a linear classifier . Let&#39;s look at a data-set which is not linearly separated: . from sklearn.datasets import make_circles X, y = make_circles(200, factor=0.1, noise=0.1) clf = SVC(kernel=&#39;linear&#39;).fit(X,y) plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(clf, plot_support=False) . There is not straight forward way to separate this data however we can project the data into higher dimensions based on its properties in the current dimensional space and get more information about its spread. One way of doing so is computing a radial basis function centered at the middle lump . r = np.exp(-(np.sum((X)**2,axis=1))) from mpl_toolkits import mplot3d def plot_3D(elev=30, azim=30, X=X, y=y): ax = plt.subplot(projection=&#39;3d&#39;) ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap=&#39;autumn&#39;) ax.view_init(elev=elev, azim=azim) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_zlabel(&#39;r&#39;) interact(plot_3D, elev=[-90, -45, -30, 30, 45, 60, 90], azip=(-180, 180), X=fixed(X), y=fixed(y)); . Projecting the data in an additonal dimensions we can see can having a plane at r=0.7 could give us good separation. . Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. . In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use. . One strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a kernel transformation, as it is based on a similarity relationship (or kernel) between each pair of points. . A potential problem with this strategy—projecting N points into N dimensions—is that it might become very computationally intensive as N grows large. However, because of a neat little procedure known as the kernel trick, a fit on kernel-transformed data can be done implicitly—that is, without ever building the full N-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful. . In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the kernel model hyperparameter: . clf = SVC(kernel=&#39;rbf&#39;, C=1E6) clf.fit(X, y) . SVC(C=1000000.0) . plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(clf) plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=300, lw=1, facecolors=&#39;none&#39;); . Softer margins . X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.2) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;); . To handle this case, the SVM implementation has a bit of a fudge-factor which &quot;softens&quot; the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as C. . For very large C, the margin is hard, and points cannot lie in it. For smaller C, the margin is softer, and can grow to encompass some points. . The plot shown below gives a visual picture of how a changing C parameter affects the final fit, via the softening of the margin: . X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8) fig, ax = plt.subplots(1, 3, figsize=(16, 6)) for axi, C in zip(ax, [1E10, 10.0, 0.1]): model = SVC(kernel=&#39;linear&#39;, C=C).fit(X, y) axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(model, axi) axi.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;,linestyle=&#39;--&#39;) axi.set_title(&#39;C = {0:.1f}&#39;.format(C), size=14) . Example: Facial Recognition . As an example of support vector machines in action, let&#39;s take a look at the facial recognition problem. . We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. . A fetcher for the dataset is built into Scikit-Learn: . from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people(min_faces_per_person=60) print(faces.target_names) print(faces.images.shape) . [&#39;Ariel Sharon&#39; &#39;Colin Powell&#39; &#39;Donald Rumsfeld&#39; &#39;George W Bush&#39; &#39;Gerhard Schroeder&#39; &#39;Hugo Chavez&#39; &#39;Junichiro Koizumi&#39; &#39;Tony Blair&#39;] (1348, 62, 47) . fig, ax = plt.subplots(3,5,figsize=(20,20)) for i,axi in enumerate(ax.flat): axi.imshow(faces.images[i],cmap=&#39;bone&#39;) axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]]) . Each image contains [62×47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features. . Here we will use a principal component analysis to extract 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline: . from sklearn.svm import SVC from sklearn.decomposition import PCA from sklearn.pipeline import make_pipeline pca = PCA(n_components=150, whiten=True, svd_solver=&#39;randomized&#39;, random_state=42) svc = SVC(kernel=&#39;rbf&#39;, class_weight=&#39;balanced&#39;) model = make_pipeline(pca,svc) . from sklearn.model_selection import train_test_split, GridSearchCV Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42) . Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model: . param_grid = {&#39;svc__C&#39;: [0.001, 0.1, 1, 5, 10, 50], &#39;svc__gamma&#39;: [0.0001, 0.0005, 0.001, 0.005]} grid = GridSearchCV(model, param_grid, cv=5) %time grid.fit(Xtrain, ytrain) print(grid.best_params_) . CPU times: user 2min 26s, sys: 35.1 s, total: 3min 1s Wall time: 33 s {&#39;svc__C&#39;: 10, &#39;svc__gamma&#39;: 0.001} . model = grid.best_estimator_ yfit = model.predict(Xtest) . fig, ax = plt.subplots(4, 6,figsize=(20,20)) for i, axi in enumerate(ax.flat): axi.imshow(Xtest[i].reshape(62, 47), cmap=&#39;bone&#39;) axi.set(xticks=[], yticks=[]) axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color=&#39;black&#39; if yfit[i] == ytest[i] else &#39;red&#39;) fig.suptitle(&#39;Predicted Names; Incorrect Labels in Red&#39;); . from sklearn.metrics import classification_report print(classification_report(ytest, yfit, target_names=faces.target_names)) . precision recall f1-score support Ariel Sharon 0.65 0.73 0.69 15 Colin Powell 0.80 0.87 0.83 68 Donald Rumsfeld 0.74 0.84 0.79 31 George W Bush 0.92 0.83 0.88 126 Gerhard Schroeder 0.86 0.83 0.84 23 Hugo Chavez 0.93 0.70 0.80 20 Junichiro Koizumi 0.92 1.00 0.96 12 Tony Blair 0.85 0.95 0.90 42 accuracy 0.85 337 macro avg 0.83 0.84 0.84 337 weighted avg 0.86 0.85 0.85 337 . from sklearn.metrics import confusion_matrix mat = confusion_matrix(ytest, yfit) fig, ax = plt.subplots(1,1, figsize=(10,10)) sns.heatmap(mat.T, square=True, annot=True, fmt=&#39;d&#39;, cbar=False, xticklabels=faces.target_names, yticklabels=faces.target_names, ax=ax) plt.xlabel(&#39;true label&#39;) plt.ylabel(&#39;predicted label&#39;); .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2020/02/19/SVM_example.html",
            "relUrl": "/python/machine-learning/2020/02/19/SVM_example.html",
            "date": " • Feb 19, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "http://pgg1610.github.io/blog_fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Vectorisation in python using numpy",
            "content": "import numpy as np import time a = np.random.randint(10E6,size=(50,1000)) print(np.shape(a)) w = np.random.randint(100,size=(50,1)) print(np.shape(w)) . (50, 1000) (50, 1) . t_start = time.time() z = np.dot(w.T,a).T t_stop = time.time() print(&#39;Time take: {} ms&#39;.format(1000*(t_stop-t_start))) #Non vectorized version z_for = [] t_start = time.time() for j in range(np.shape(a)[1]): _count = 0.0 for i in range(np.shape(a)[0]): _count+=w[i,0]*a[i,j] z_for.append(_count) t_stop = time.time() print(&#39;Time take for for-loop: {} ms&#39;.format(1000*(t_stop-t_start))) #Check the output print(&#39;Check sum: {}&#39;.format(np.sum(np.asarray(z_for).reshape(np.shape(z))-z))) . Time take: 0.3979206085205078 ms Time take for for-loop: 33.74624252319336 ms Check sum: 0.0 . #If I want to have expoenential of different values in the array a = np.random.randint(10,size=(10,2)) #With for loops: import math exp_a = np.zeros(np.shape(a)) for j in range(np.shape(a)[1]): for i in range(np.shape(a)[0]): exp_a[i,j] = math.exp(a[i,j]) . exp_a_numpy = np.exp(a) #Vector already setup -- element-wise exponential #Other vectorized functions: # np.log(x) # np.abs(x) # np.maximum(x,0) -- computes element-wise maximum comparing to 0 # x**2 for numpy array # 1/x for numpy array . exp_a_numpy - exp_a . array([[0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.]]) . food_cal = np.array([[56.0,0.0,4.4,68.0], [1.2, 104, 52, 8.], [1.8, 135.,99., 0.9]]) . carb = np.array([food_cal[0,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) protein = np.array([food_cal[1,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) fat = np.array([food_cal[2,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) cal = np.array([carb,protein,fat]) print(cal) . [[94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] . cal = food_cal.sum(axis=0) #AXIS = 0 is sum vertically -- along column #AXIS = 1 is sum horizontally -- along row print(cal) . [ 59. 239. 155.4 76.9] . #Here the cal is BROADCASTING from 1,4 to 4,4 percentage = 100*food_cal/cal.reshape(1,4) print(percentage) . [[94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] . #Example 1 A = np.linspace(1,5,5) print(A.shape) B = A+10. print(A, B, B.shape) # Here 10. was broadcasted into 5x1 vector . (5,) [1. 2. 3. 4. 5.] [11. 12. 13. 14. 15.] (5,) . A = np.array([[1,2,3], [4,5,6]]) print(A.shape) B = np.array([100,200,300]) print(B.shape) C = A + B print(C.shape) print(A,B) print(C) # Here B was broadcasted from (3,) to 2x3! . (2, 3) (3,) (2, 3) [[1 2 3] [4 5 6]] [100 200 300] [[101 202 303] [104 205 306]] . General principle . (m,n) matrix with (+, -, *, /) with (1,n) or (m,1) lead of copying it to (m,n) before conducting computing. . Good practices and tips . import numpy as np a = np.random.randn(5) print(a) . [ 0.68281763 -1.3579685 0.99577659 0.31269709 0.595569 ] . print(a.shape) . (5,) . Here a is a array of rank 1. It is neither a row or a column vector. So this has some non-intuitive effects . print(a.T) . [ 0.68281763 -1.3579685 0.99577659 0.31269709 0.595569 ] . print(np.dot(a,a.T)) . 3.7543713020122427 . So it is recommended for consistency to NOT use data-structures have rank 1 like the one above but instead instantiate the array as the fixed array of known size . ALWAYS COMMIT TO MAKING DEFINED ROW AND COLUMN VECTORS . a1 = np.random.randn(5,1) print(a1) print(a1.shape) . [[-0.7474656 ] [-0.75790159] [ 0.30984002] [ 0.18874051] [-0.80470167]] (5, 1) . print(a1.T) . [[-0.7474656 -0.75790159 0.30984002 0.18874051 -0.80470167]] . Here there are two Square Brackets compared to the previous transport of a suggesting in the case of a1 it is well-defined 1x5 row vector . print(np.dot(a1,a1.T)) #Outer product . [[ 0.55870482 0.56650536 -0.23159476 -0.14107704 0.60148682] [ 0.56650536 0.57441482 -0.23482825 -0.14304673 0.60988468] [-0.23159476 -0.23482825 0.09600084 0.05847936 -0.24932878] [-0.14107704 -0.14304673 0.05847936 0.03562298 -0.1518798 ] [ 0.60148682 0.60988468 -0.24932878 -0.1518798 0.64754478]] . assert(a1.shape==(5,1)) #Assertion statement to check the known size a = a.reshape((5,1)) print(a.shape) . (5, 1) . A = np.random.randn(4,3) . print(A) . [[ 0.22469294 0.78832742 -1.13148285] [-0.04070683 -0.74061401 -1.59838506] [ 0.12821164 0.72892812 0.4912876 ] [ 0.09323584 1.66090848 1.87905216]] . np.sum(A,axis=1,keepdims=True).shape . (4, 1) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2019/10/11/Vectorisation_and_TF_example.html",
            "relUrl": "/python/machine-learning/2019/10/11/Vectorisation_and_TF_example.html",
            "date": " • Oct 11, 2019"
        }
        
    
  
    
        ,"post27": {
            "title": "Lambda, Filter, and Map functions in Python",
            "content": "Lambda . Lambda is an important function/operator to create anonymous in-line functions in pyhton. . Basic syntax: . lamda arguments: expression . def func(x,y): return(x+y) func(2,3) . 5 . add=lambda x,y:x+y add(2,3) . 5 . lambda functions can be used in place of a iterator when sorting -- this allows for selecting the column or the varible according to which sorting has to be done . import numpy as np np.random.seed(42) a1=np.random.choice(10,5,replace=False) file_list=[] for i in a1: file_list.append(&#39;{0}-{1}&#39;.format(&#39;Filename&#39;,i)) . print(file_list) . [&#39;Filename-8&#39;, &#39;Filename-1&#39;, &#39;Filename-5&#39;, &#39;Filename-0&#39;, &#39;Filename-7&#39;] . file_list=sorted(file_list, key=lambda x:x.split(&#39;-&#39;)[-1]) . file_list . [&#39;Filename-0&#39;, &#39;Filename-1&#39;, &#39;Filename-5&#39;, &#39;Filename-7&#39;, &#39;Filename-8&#39;] . Map . When you want to have multiple outputs for the functions but do not want to write a for all explicitly you can use map function for pseeding things up . def square(x): return(x**2) print(a1) ans=[square(i) for i in a1] print(ans) . [8 1 5 0 7] [64, 1, 25, 0, 49] . map(square,a1) . &lt;map at 0x7f85058e5eb8&gt; . list(map(square,a1)) . [64, 1, 25, 0, 49] . Combining the two: . a=np.random.choice(10,5,replace=False) b=np.random.choice(50,5,replace=False) result=map(lambda x,y:x*y,a,b) print(a) print(b) print(np.asarray(list(result))) . [0 1 8 5 3] [36 16 4 9 45] [ 0 16 32 45 135] .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/2019/10/04/Lambda_Map.html",
            "relUrl": "/python/2019/10/04/Lambda_Map.html",
            "date": " • Oct 4, 2019"
        }
        
    
  
    
        ,"post28": {
            "title": "Principal component analysis",
            "content": "Principal component analysis is one of the oldest tools to be implemented for analyzing feature sets in the data. It is fundamentally a dimensionality reduction technique targeted to reduce noise, feature reduction/extraction and engineering. It can also allow for providing &#39;new&#39; features where are not necessarily correlated ensuring indenpedent treatment on the model. . General idea: . PCA introduces a new set of variables (called principal components, PCs) by linear combination of the original variables in the data, standardized to zero mean and unit variance (see Figure 12.8 for a toy example in two dimensions). The PCs are chosen such that they are uncorrelated, and they are ordered such that the first component captures the largest possible amount of variation in the data, and subsequent components capture increasingly less. Usually, key features in the data can be seen from only the first two or three PCs. . Example: If we are trying to understand the effect of weight, age, and height in humans, the weight of the subject is an correlated variable to other two. Height is, in some way, related to weight and that is in a way related to age of the person. Hence understanding effect of one variable on the output without the effect on another is difficult if not impossible. Here, we can use PCA to project the age and weight in a new 2-D space where now the height can be related to THESE two variables independently. Now the drawback is that we do not necessarily know what do these two variables means. For understanding the inherent logic of the variables there are techniques like vari-max rotation used to recapture the projection that MIGHT be used to get the new variables. . When should you use PCA? . Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration? | Do you want to ensure your variables are independent of one another? | Are you comfortable making your independent variables less interpretable? | . Useful Resources: . Jake VanderPlas&#39;s Python Data Science Handbook Chapter . | Tutorial on Principal Component Analysis by Jonathan Shlens (Google Research) . | . Currently, PCA, when categorizing it from ML-terminology standpoint, is considered as a dimensionality reduction and a fast-flexible unsupervised learning method. Let&#39;s look at simplified example: . Two dimensional data-set | import numpy as np import matplotlib.pyplot as plt import seaborn as sns; sns.set() %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; # Plot matplotlib plots with white background: %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} . rng = np.random.RandomState(42) x1=rng.randn(2,200) #Normally distributed 200 entries with 2 rows factor=rng.rand(2,2) #factor to multiply the entries . X = np.dot(factor, x1).T plt.scatter(X[:,0],X[:,1]) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;) plt.axis(&#39;equal&#39;); . In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn&#39;s PCA estimator, we can compute this as follows: . from sklearn.decomposition import PCA pca=PCA(n_components=2, random_state=42) pca.fit(X) . PCA(n_components=2, random_state=42) . print(pca.components_) . [[ 0.41224135 0.91107468] [ 0.91107468 -0.41224135]] . print(pca.explained_variance_) . [0.86789943 0.11361735] . PCA analysis learns some quantities in the data. To visualize the &#39;Principal components&#39; we can look at the Components which are the directions of the vector and Explained Variance is the square-length magnitude of the vector. . def draw_vector(v0, v1, ax=None): ax = ax or plt.gca() arrowprops=dict(arrowstyle=&#39;-&gt;&#39;, linewidth=2.5, color=&#39;k&#39;, shrinkA=0, shrinkB=0) ax.annotate(&#39;&#39;,v1,v0,arrowprops=arrowprops) plt.scatter(X[:,0], X[:,1], alpha=0.6) for length, vector in zip(pca.explained_variance_, pca.components_): v = vector * 4. * np.sqrt(length) #vector enhanced by a factor of 5 and the sqrt(lenght) print(v) draw_vector(pca.mean_, pca.mean_+v) #Pre PCA dataset mean plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;) plt.axis(&#39;equal&#39;); . [1.53619465 3.3950695 ] [ 1.22839009 -0.55581963] . The vectors above represent the principal axes of the data. Length of the vector is how imporatant are they. That is given by how much variance is explained by that axes. The projection of each data point onto the principal axes are the &quot;principal components&quot; of the data. If we plot the original data and the data being transformed such that the principal components are now the unit axes (through translation, rotation, and scaling of the data) we will get something like this . fig, ax = plt.subplots(1, 2, figsize=(16, 6)) # plot data ax[0].scatter(X[:, 0], X[:, 1], alpha=0.6) for length, vector in zip(pca.explained_variance_, pca.components_): v = vector * 3 * np.sqrt(length) draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0]) ax[0].axis(&#39;equal&#39;); ax[0].set(xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, title=&#39;input&#39;) # plot principal components X_pca = pca.transform(X) ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6) draw_vector([0, 0], [0, 3], ax=ax[1]) draw_vector([0, 0], [3, 0], ax=ax[1]) ax[1].axis(&#39;equal&#39;) ax[1].set(xlabel=&#39;Component 1&#39;, ylabel=&#39;Component 2&#39;, title=&#39;principal components&#39;, xlim=(-5, 5), ylim=(-3, 3.1)); . Dimensionality reduction . Through this analysis we can prune out certain components in the data which are not contributing to explaining the variance in the data. While the TRUE meaning the variable is convoluted (linear combination of original variable) in the analysis we can appreciate the dimensionality reduction. . pca = PCA(n_components=1, random_state=42) pca.fit(X) #Original data X_pca = pca.transform(X) print(&quot;original shape: &quot;, X.shape) print(&quot;transformed shape:&quot;, X_pca.shape) . original shape: (200, 2) transformed shape: (200, 1) . The transformed data has just one-dimension. To visualize this effect we can perform inverse transform of this reduced data and plot with original data . X_reduced = pca.inverse_transform(X_pca) print(X_reduced.shape) plt.scatter(X[:, 0], X[:, 1], alpha=0.4) plt.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.8) plt.axis(&#39;equal&#39;); . (200, 2) . The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much &quot;information&quot; is discarded in this reduction of dimensionality. . This reduced-dimension dataset is in some senses &quot;good enough&quot; to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved. . Going to higher dimensions . The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data. We can appreciate it more for classifying the feature sets used to predict the handwriten digits . from sklearn.datasets import load_digits digits = load_digits() digits.data.shape . (1797, 64) . The data consists of 8×8 pixel images, meaning that they are 64-dimensional. To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two . pca = PCA(n_components=2) # project from 64 to 2 dimensions projected = pca.fit_transform(digits.data) print(digits.data.shape) print(projected.shape) . (1797, 64) (1797, 2) . We can now plot the dataset on the transformed space along the two components . plt.figure(figsize=(10,10)) plt.scatter(projected[:, 0], projected[:, 1], c=digits.target, edgecolor=&#39;none&#39;, alpha=0.5, cmap=plt.cm.get_cmap(&#39;Spectral&#39;, 10)) plt.xlabel(&#39;component 1&#39;) plt.ylabel(&#39;component 2&#39;) plt.colorbar(); . Initially the data set for each image was a 64 dimensional entry. We now project that 64 dimensional data on a two component principal axes. The PCA routine has found the optimal stretch and rotation in the 64 dimensional space that allows us to see the layout of the digits in two dimensions. This was done in unsupervised manner. . How does it all work? . Simple illustration to show the inner working of the PCA analysis. The fundamental goal of PCA is to identify the most meaningful basis to re-express the data-set. These basis are linearly independent from each other. . Is there another basis, which is a linear combination of the original basis, the best re-expresses the data? . Important features/considerations:1. Linearity: The underlying idea of PCA is to find another basis for representing the data. This makes PCA is a change of basis problem. . Variance: To identify which direction to project the data on, signal-to-noise ratio calculated by variance is assumed to model the interesting nature. Hence principal components with larger variance represent the interesting structure. . | Orthogonality: The principal components are orthonormal basis vectors. This allows PCA to provide an intuitive simplification . | Covariance matrix is a symmetric matrix that measures the degree of pair-wise linear relationship in the data. . The diagonal entries estimate the variance of the variable | while the off-diagonal entries estimate the covariance between a given pair of variables. | . Ideally, for the case to reduce dimensions and correlations the resulting covariance for the data from change of basis should have off-diagonal elements as 0 and only diagonal elements which are ordered magnitude-wise. . In practice computing PCA of dataset following steps: . Recast the data as zero mean dataset | Compute eigenvectors for the covariance matrix for the dataset -- these are the principal components of the data | Those eigenvectors would diagonalize the covariance matrix of the original dataset | The diagonal entries of the new covariance matrix will give the variance along each principal component | The diagonalised matrix from the above transformation is the covariance matrix for the projected data-set. This is made of the eigenvalues of the covariance matrix of original data . begin{align*} C_{Y}&amp;= frac{YY^{T}}{n} &amp;= frac{(PX)(PX)^{T}}{n} &amp;= frac{PXP^{T}X^{T}}{n} &amp;= frac{P(XX)^{T}P^{T}}{n} &amp;=PC_{X}P^{T} end{align*}Here P is the eigenvector of Cov(X) matrix . Let&#39;s use the first example as a basis for explanation: . rng = np.random.RandomState(42) x1=rng.randn(2,200) #Normally distributed 200 entries with 2 rows factor=rng.rand(2,2) #factor to multiply the entries X = np.dot(factor, x1) plt.scatter(X[0,:],X[1,:]) plt.xlabel(&#39;X1&#39;) plt.ylabel(&#39;X2&#39;) plt.axis(&#39;equal&#39;); . X_center = np.empty(shape=X.shape) X_center[0,:]=X[0,:]-np.mean(X[0,:]) X_center[1,:]=X[1,:]-np.mean(X[1,:]) . cov_X = np.dot(X,X.T)/(X_center.shape[1]-1) print(cov_X) . [[0.24184557 0.28376997] [0.28376997 0.74491787]] . eigen_values, eigen_vectors = np.linalg.eig(cov_X) #eigen_values[i] is eigenvalue of eigen_vector[:,i] print(eigen_vectors) . [[-0.91195569 -0.4102887 ] [ 0.4102887 -0.91195569]] . values_vectors = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))] . values_vectors = sorted(values_vectors, key=lambda x:x[0], reverse=True) print(values_vectors) . [(0.8725859273634107, array([-0.4102887 , -0.91195569])), (0.11417751236536822, array([-0.91195569, 0.4102887 ]))] . fig, ax_new = plt.subplots(1, 2, figsize=(16, 6)) # plot data ax_new[0].scatter(X[0, :], X[1, :], alpha=0.6) ax_new[0].axis(&#39;equal&#39;); ax_new[0].set(xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, title=&#39;input&#39;) # plot principal components X_transform = np.dot(eigen_vectors.T,X) ax_new[1].scatter(X_transform[0, :], X_transform[1, :], alpha=0.6) ax_new[1].axis(&#39;equal&#39;) ax_new[1].set(xlabel=&#39;component 1&#39;, ylabel=&#39;component 2&#39;, title=&#39;principal components&#39;, xlim=(-5, 5), ylim=(-3, 3.1)) . [Text(0.5, 0, &#39;component 1&#39;), Text(0, 0.5, &#39;component 2&#39;), Text(0.5, 1.0, &#39;principal components&#39;), (-5.0, 5.0), (-3.0, 3.1)] . pca=PCA(n_components=2, random_state=42) pca.fit(X.T) pca_results = [(np.abs(pca.explained_variance_[i]), pca.components_[:,i]) for i in range(len(pca.explained_variance_))] pca_results = sorted(pca_results, key=lambda x:x[0], reverse=True) print(pca_results) . [(0.867899431633577, array([0.41224135, 0.91107468])), (0.11361735469514019, array([ 0.91107468, -0.41224135]))] . cov_Y = np.dot(eigen_vectors.T,np.dot(cov_X,eigen_vectors)) . np.around(cov_Y,4) . array([[0.1142, 0. ], [0. , 0.8726]]) . PCA on Iris dataset . The Scikit-learn package has some datasets and sample images. One of them is the Iris dataset. . The Iris dataset consists of measurements of sepals and petals of 3 different plant species: . Iris setosa | Iris versicolor | Iris virginica | . from sklearn import datasets import pandas as pd iris_data = datasets.load_iris() X = iris_data.data y = iris_data.target iris_data_df = pd.DataFrame(X, columns = iris_data.feature_names) X = iris_data_df - iris_data_df.mean() . print(X.shape, y.shape, iris_data.feature_names) . (150, 4) (150,) [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . pca_2D = PCA(n_components=2) pca_2D.fit(X) X_new = pca_2D.transform(X) X_new.shape data_total = np.vstack((X_new.T, y.T)).T pca_df = pd.DataFrame(data_total, columns=[&quot;PC1&quot;, &quot;PC2&quot;,&quot;y&quot;]) name_dict = {&#39;0&#39;:&#39;Setosa&#39;,&#39;1&#39;:&#39;Versicolour&#39;, &#39;2&#39;:&#39;Virginica&#39;} pca_df[&#39;flowers&#39;] = [name_dict[str(i)] for i in y] . pca_df . PC1 PC2 y flowers . 0 -2.684126 | 0.319397 | 0.0 | Setosa | . 1 -2.714142 | -0.177001 | 0.0 | Setosa | . 2 -2.888991 | -0.144949 | 0.0 | Setosa | . 3 -2.745343 | -0.318299 | 0.0 | Setosa | . 4 -2.728717 | 0.326755 | 0.0 | Setosa | . ... ... | ... | ... | ... | . 145 1.944110 | 0.187532 | 2.0 | Virginica | . 146 1.527167 | -0.375317 | 2.0 | Virginica | . 147 1.764346 | 0.078859 | 2.0 | Virginica | . 148 1.900942 | 0.116628 | 2.0 | Virginica | . 149 1.390189 | -0.282661 | 2.0 | Virginica | . 150 rows × 4 columns . import seaborn as sns sns.set(style=&quot;white&quot;, color_codes=True) g = sns.scatterplot(x=&#39;PC1&#39;, y=&#39;PC2&#39;, hue=&#39;flowers&#39;, edgecolor=None, alpha=1.0, data=pca_df) plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) plt.tight_layout(); #plt.savefig(&#39;PCA_iris.png&#39;,dpi=300); . pca_var = PCA(n_components=4, random_state=42).fit(X) fig, ax = plt.subplots(figsize=(8,6)) ax.tick_params(labelsize=20) ax.set_xlabel(&#39;Num of components&#39;, fontsize=20) ax.set_ylabel(&#39;Cumulative variance&#39;, fontsize=20) ax.set_title(&#39;PCA cumulative variance&#39;, fontsize=20) num_range = np.arange(1,len(pca_var.explained_variance_ratio_)+1) pca_exp = np.cumsum(pca_var.explained_variance_ratio_) ax.plot(num_range, pca_exp,&#39;-o&#39;,markersize=&#39;10&#39;) ax.axhline(linewidth=&#39;3.5&#39;,linestyle=&#39;--&#39;,color=&#39;r&#39;,y=1) plt.xticks(num_range) plt.tight_layout(); . Loadings . PCA loadings are the coefficients of the linear combination of the original variables from which the principal components (PCs) are constructed. . pca_df . PC1 PC2 y flowers . 0 -2.684126 | 0.319397 | 0.0 | Setosa | . 1 -2.714142 | -0.177001 | 0.0 | Setosa | . 2 -2.888991 | -0.144949 | 0.0 | Setosa | . 3 -2.745343 | -0.318299 | 0.0 | Setosa | . 4 -2.728717 | 0.326755 | 0.0 | Setosa | . ... ... | ... | ... | ... | . 145 1.944110 | 0.187532 | 2.0 | Virginica | . 146 1.527167 | -0.375317 | 2.0 | Virginica | . 147 1.764346 | 0.078859 | 2.0 | Virginica | . 148 1.900942 | 0.116628 | 2.0 | Virginica | . 149 1.390189 | -0.282661 | 2.0 | Virginica | . 150 rows × 4 columns . pca_2D.components_ . array([[ 0.36138659, -0.08452251, 0.85667061, 0.3582892 ], [ 0.65658877, 0.73016143, -0.17337266, -0.07548102]]) . iris_data.feature_names . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . loadings_data = pd.DataFrame(pca_2D.components_.T, columns=[&#39;PC1&#39;,&#39;PC2&#39;], index=iris_data.feature_names) loadings_data . PC1 PC2 . sepal length (cm) 0.361387 | 0.656589 | . sepal width (cm) -0.084523 | 0.730161 | . petal length (cm) 0.856671 | -0.173373 | . petal width (cm) 0.358289 | -0.075481 | . import matplotlib as mpl def loading_plot(coeff, labels): n = coeff.shape[0] for i in range(n): plt.arrow(0, 0, coeff[i,0], coeff[i,1], head_width = 0.05, color=&#39;k&#39;, head_length = 0.02,alpha = 0.5) plt.text(coeff[i,0]* 0.8 * i, coeff[i,1] * 1.2, labels[i],ha = &#39;center&#39;, va = &#39;center&#39;, fontsize=20) cmap = plt.cm.viridis #norm = mpl.colors.Normalize(vmin=-10, vmax=10) #plt.scatter(x=pca_df[&#39;PC1&#39;], y=pca_df[&#39;PC2&#39;], color=cmap(norm(pca_df[&#39;y&#39;].values)), alpha=1.0) plt.scatter(x=pca_df[&#39;PC1&#39;], y=pca_df[&#39;PC2&#39;], c=pca_df[&#39;y&#39;].values, cmap=&quot;viridis&quot;) plt.xlabel(&#39;PC1&#39;) plt.ylabel(&#39;PC2&#39;) plt.grid() . fig, ax = plt.subplots(figsize = (10,10)) loading_plot(pca_2D.components_.T, iris_data.feature_names) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/2019/09/07/PCA_tutorial.html",
            "relUrl": "/python/data-analysis/2019/09/07/PCA_tutorial.html",
            "date": " • Sep 7, 2019"
        }
        
    
  
    
        ,"post29": {
            "title": "Neural network implementation in PyTorch",
            "content": "This notebook is inspired by the Andrew Ng&#39;s amazing Coursera course on Deep learning. The dataset we will be using the train the model on is the MNIST dataset which one of the default datasets in PyTorch. . import numpy as np import torch import torch.nn as nn import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt import matplotlib as mpl %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} . device = &#39;cpu&#39; #torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) . cpu . train_dataset = torchvision.datasets.MNIST(root=&#39;data/&#39;, train=True, transform=torchvision.transforms.ToTensor(), download=True) val_dataset = torchvision.datasets.MNIST(root=&#39;data/&#39;, train=False, transform=torchvision.transforms.ToTensor(), download=True) input_tensor, label = train_dataset[0] print(&#39;MNIST dataset with {} train data and {} test data&#39;.format(len(train_dataset), len(val_dataset))) print(&#39;Type of data in dataset: {} AND {}&#39;.format(type(input_tensor), type(label))) print(&#39;Input tensor image dimensions: {}&#39;.format(input_tensor.shape)) . MNIST dataset with 60000 train data and 10000 test data Type of data in dataset: &lt;class &#39;torch.Tensor&#39;&gt; AND &lt;class &#39;int&#39;&gt; Input tensor image dimensions: torch.Size([1, 28, 28]) . input_size = 784 # Image input for the digits - 28 x 28 x 1 (W-H-C) -- flattened in the end before being fed in the NN num_hidden_layers = 1 hidden_layer_size = 50 num_classes = 10 num_epochs = 50 batch_size = 64 learning_rate = 10e-4 . from torch.utils.data import Dataset, DataLoader train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True, num_workers = 2) test_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, num_workers = 2) #Take a look at one batch examples = iter(train_loader) samples, labels = examples.next() print(samples.shape, labels.shape) #Plotting first 4 digits in the dataset: for i in range(4): plt.subplot(2, 2, i+1) plt.imshow(samples[i][0], cmap=mpl.cm.binary, interpolation=&quot;nearest&quot;) plt.title(&#39;Digit:{}&#39;.format(labels[i])) plt.axis(&quot;off&quot;); . torch.Size([64, 1, 28, 28]) torch.Size([64]) . Above, we have defined a batch-size of 100 for the training dataset with the samples as seen here to be of size = 100 x 1 x 28 x 28 . class NeuralNet(nn.Module): def __init__(self, input_size, num_hidden_layers, hidden_layer_size, num_classes): super(NeuralNet, self).__init__() self.L1 = nn.Linear(in_features = input_size, out_features = hidden_layer_size) self.relu = nn.ReLU() self.num_hidden_layers = num_hidden_layers if (self.num_hidden_layers-1) &gt; 1: self.L_hidden = nn.ModuleList( [nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size) for _ in range(num_hidden_layers-1)] ) self.relu_hidden = nn.ModuleList( [nn.ReLU() for _ in range(num_hidden_layers-1)] ) else: self.L2 = nn.Linear(in_features = hidden_layer_size, out_features = hidden_layer_size) self.L_out = nn.Linear(in_features = hidden_layer_size, out_features = num_classes) def forward(self, x): out = self.relu(self.L1(x)) if (self.num_hidden_layers-1) &gt; 1: for L_hidden, relu_hidden in zip(self.L_hidden, self.relu_hidden): out = relu_hidden(L_hidden(out)) else: out = self.relu(self.L2(out)) out = self.L_out(out) #No softmax or cross-entropy activation just the output from linear transformation return out . model = NeuralNet(input_size=input_size, num_hidden_layers=num_hidden_layers, hidden_layer_size=hidden_layer_size, num_classes=num_classes) . model . NeuralNet( (L1): Linear(in_features=784, out_features=50, bias=True) (relu): ReLU() (L2): Linear(in_features=50, out_features=50, bias=True) (L_out): Linear(in_features=50, out_features=10, bias=True) ) . CrossEntropyLoss in Pytorch implementes Softmax activation and NLLLoss in one class. . criterion = nn.CrossEntropyLoss() #This is implement softmax activation for us so it is not implemented in the model optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #Training loop total_batches = len(train_loader) losses = [] epochs = [] for epoch in range(num_epochs): for i, (image_tensors, labels) in enumerate(train_loader): running_loss = 0 batch_count = 0 #image tensor = 100, 1, 28, 28 --&gt; 100, 784 input needed image_input_to_NN = image_tensors.view(-1,28*28).to(device) labels = labels.to(device) #Forward pass outputs = model(image_input_to_NN) loss = criterion(outputs, labels) running_loss += loss.item() batch_count += 1 #Backward optimizer.zero_grad() #Detach and flush the gradients loss.backward() #Backward gradients evaluation optimizer.step() #To update the weights/parameters in the NN if (epoch) % 10 == 0 and (i+1) % 500 == 0: print(f&#39;epoch {epoch+1} / {num_epochs}, batch {i+1}/{total_batches}, loss = {loss.item():.4f}&#39;) loss_per_epoch = running_loss / batch_count epochs.append(epoch) losses.append(loss_per_epoch) . epoch 1 / 50, batch 500/938, loss = 0.2568 epoch 11 / 50, batch 500/938, loss = 0.0431 epoch 21 / 50, batch 500/938, loss = 0.0141 epoch 31 / 50, batch 500/938, loss = 0.0032 epoch 41 / 50, batch 500/938, loss = 0.0518 . fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(epochs, losses) plt.title(&#39;Loss Curve (Training)&#39;) ax.set_xlabel(&#39;Epochs&#39;) ax.set_ylabel(&#39;Loss Value&#39;) . Text(0, 0.5, &#39;Loss Value&#39;) . with torch.no_grad(): n_correct = 0 n_samples = 0 for images, labels in test_loader: images = images.view(-1, 28*28).to(device) labels = labels.to(device) outputs = model(images) _, predictions = torch.max(outputs, 1) n_samples += labels.shape[0] n_correct += (predictions == labels).sum().item() #For each correction prediction we add the correct samples acc = 100 * n_correct / n_samples print(f&#39;Accuracy = {acc:.2f}%&#39;) . Accuracy = 97.54% .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/machine-learning/2019/08/13/PyTorch_MNIST_classification.html",
            "relUrl": "/python/data-analysis/machine-learning/2019/08/13/PyTorch_MNIST_classification.html",
            "date": " • Aug 13, 2019"
        }
        
    
  
    
        ,"post30": {
            "title": "End-to-end Machine Learning Project",
            "content": "This project is adapted from Aurelien Geron&#39;s ML book (Github link) . The aim to predict median house values in Californian districts, given a number of features from these districts. . Main steps we will go through: . Formulate the problem | Get the data | Discover and visualize data / Data exploration to gain insight | Prep data for ML algorithm testing | Select model and train it | Fine-tuning the model | Step 1: Formulate the problem . Prediction of district&#39;s median housing price given all other metrics. A supervised learning task is where we are given &#39;labelled&#39; data for training purpose. Regression model to predict a continuous variable i.e. district median housing price. Given multiple features, this is a multi-class regression type problem. Univariate regression since a single output is estimated. . Step 2: Get the data . import os import pandas as pd import numpy as np . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns sns.set(style=&quot;whitegrid&quot;) sns.color_palette(&quot;husl&quot;) %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 30, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . housing = pd.read_csv(&#39;./data/housing.csv&#39;) . housing.sample(7) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 10769 -117.89 | 33.66 | 33.0 | 3595.0 | 785.0 | 1621.0 | 732.0 | 4.1372 | 265200.0 | &lt;1H OCEAN | . 4165 -118.20 | 34.11 | 36.0 | 1441.0 | 534.0 | 1809.0 | 500.0 | 2.1793 | 185700.0 | &lt;1H OCEAN | . 3685 -118.37 | 34.21 | 36.0 | 2080.0 | 455.0 | 1939.0 | 484.0 | 4.2875 | 176600.0 | &lt;1H OCEAN | . 8040 -118.15 | 33.84 | 37.0 | 1508.0 | 252.0 | 635.0 | 241.0 | 3.7500 | 221300.0 | &lt;1H OCEAN | . 11836 -120.98 | 39.08 | 20.0 | 4570.0 | 906.0 | 2125.0 | 815.0 | 3.0403 | 148000.0 | INLAND | . 1525 -122.07 | 37.89 | 28.0 | 3410.0 | 746.0 | 1428.0 | 670.0 | 4.3864 | 266800.0 | NEAR BAY | . 18180 -122.03 | 37.37 | 9.0 | 2966.0 | 770.0 | 1430.0 | 740.0 | 3.0047 | 256000.0 | &lt;1H OCEAN | . Each row presents one district. Each of these districts has 10 attributes (features). . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . One thing to notice in this dataset is the number of total_bedroom entries is different from other entries. This suggests there are some missing entries or null in the dataset. . housing[housing.total_bedrooms.isnull()] . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 290 -122.16 | 37.77 | 47.0 | 1256.0 | NaN | 570.0 | 218.0 | 4.3750 | 161900.0 | NEAR BAY | . 341 -122.17 | 37.75 | 38.0 | 992.0 | NaN | 732.0 | 259.0 | 1.6196 | 85100.0 | NEAR BAY | . 538 -122.28 | 37.78 | 29.0 | 5154.0 | NaN | 3741.0 | 1273.0 | 2.5762 | 173400.0 | NEAR BAY | . 563 -122.24 | 37.75 | 45.0 | 891.0 | NaN | 384.0 | 146.0 | 4.9489 | 247100.0 | NEAR BAY | . 696 -122.10 | 37.69 | 41.0 | 746.0 | NaN | 387.0 | 161.0 | 3.9063 | 178400.0 | NEAR BAY | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20267 -119.19 | 34.20 | 18.0 | 3620.0 | NaN | 3171.0 | 779.0 | 3.3409 | 220500.0 | NEAR OCEAN | . 20268 -119.18 | 34.19 | 19.0 | 2393.0 | NaN | 1938.0 | 762.0 | 1.6953 | 167400.0 | NEAR OCEAN | . 20372 -118.88 | 34.17 | 15.0 | 4260.0 | NaN | 1701.0 | 669.0 | 5.1033 | 410700.0 | &lt;1H OCEAN | . 20460 -118.75 | 34.29 | 17.0 | 5512.0 | NaN | 2734.0 | 814.0 | 6.6073 | 258100.0 | &lt;1H OCEAN | . 20484 -118.72 | 34.28 | 17.0 | 3051.0 | NaN | 1705.0 | 495.0 | 5.7376 | 218600.0 | &lt;1H OCEAN | . 207 rows × 10 columns . For categorical entries (here, ocean_proximity entries) we can find out the entries and their number using the value_counts(). We can do this for any entry we wish but makes more sense for categorical entries. . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 . housing.describe().round(2) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . count 20640.00 | 20640.00 | 20640.00 | 20640.00 | 20433.00 | 20640.00 | 20640.00 | 20640.00 | 20640.00 | . mean -119.57 | 35.63 | 28.64 | 2635.76 | 537.87 | 1425.48 | 499.54 | 3.87 | 206855.82 | . std 2.00 | 2.14 | 12.59 | 2181.62 | 421.39 | 1132.46 | 382.33 | 1.90 | 115395.62 | . min -124.35 | 32.54 | 1.00 | 2.00 | 1.00 | 3.00 | 1.00 | 0.50 | 14999.00 | . 25% -121.80 | 33.93 | 18.00 | 1447.75 | 296.00 | 787.00 | 280.00 | 2.56 | 119600.00 | . 50% -118.49 | 34.26 | 29.00 | 2127.00 | 435.00 | 1166.00 | 409.00 | 3.53 | 179700.00 | . 75% -118.01 | 37.71 | 37.00 | 3148.00 | 647.00 | 1725.00 | 605.00 | 4.74 | 264725.00 | . max -114.31 | 41.95 | 52.00 | 39320.00 | 6445.00 | 35682.00 | 6082.00 | 15.00 | 500001.00 | . Describe is powerful subroutine since that allows us to check the stat summary of numerical attributes . The 25%-50%-75% entries for each column show corresponding percentiles. It indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of observation have median income below 2.56, 50% observations have median income below 3.53, and 75% observations have median income below 4.74. 25% --&gt; 1st Quartile, 50% --&gt; Median, 75% --&gt; 3rd Quartile . housing.hist(bins=50,figsize=(20,20)); . Few observations from the Histogram plots, again remember each row is an entry for an ENTIRE district: . NOTICE:From the dataset&#39;s source disclaimer: The housing_median_value, housing_median_age, median_income_value are capped at an arbitrary value. . From latitute and longitude plots there seems to be lots of district in four particular locations (34,37 -- latitude) and (-120,-118 -- longitude). We cannot comment on the exact location but only one on these pairs giving most data. | We see a tighter distribution for total_rooms, total_bedrooms, and population but spread for house_value and an intresting spike at its end. | Small spike at the end of median_income plot suggests presence of small group of affluent families but interestingly that spike does not correlate with the spike in the house_value (More high-end property entries than more &quot;rich&quot; people in a district) | Finally, the dataset is tail-heavy that is they extend further to the right from the median which might make modeling using some ML algorithm a bit chanellenging. Few entries should be scaled such that the distribution is more normal. . Create a test-set . This ensures that this is the data on which training, testing occurs and we do not try overfitting to account for all the variance in the data. Typical 20% of data-points are randomly chosen. . def split_train_test(data,test_ratio): shuffled_indices=np.random.permutation(len(data)) test_set_size=int(len(data)*test_ratio) test_indices=shuffled_indices[:test_set_size] train_indices=shuffled_indices[test_set_size:] return(data.iloc[train_indices],data.iloc[test_indices]) . #shuffled indices risking the possibility of the algo seeing the entire dataset! np.random.seed(42) #Random seed set to 42 for no particular reason #but just cause its the answer to the Ultimate Question of Life, The Universe, and Everything train_set, test_set = split_train_test(housing, 0.2) print(len(train_set), &quot;train +&quot;, len(test_set), &quot;test&quot;) . 16512 train + 4128 test . Better way is to have an instance identifier (like id) for each entry to distingusih each entry and see if its sampled or not. . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set] . The dataset currently doesnt have inherent id. We could use the row index as id. Or we could use an ad-hoc unique identifier as an interim id. . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) #HOUSING DATA WITH ID AS COMBO OF LAT AND LONG. housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) #SCIKIT-LEARN IMPLEMENTATION #from sklearn.model_selection import train_test_split #train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) . test_set.head() . index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 59 59 | -122.29 | 37.82 | 2.0 | 158.0 | 43.0 | 94.0 | 57.0 | 2.5625 | 60000.0 | NEAR BAY | -122252.18 | . 60 60 | -122.29 | 37.83 | 52.0 | 1121.0 | 211.0 | 554.0 | 187.0 | 3.3929 | 75700.0 | NEAR BAY | -122252.17 | . 61 61 | -122.29 | 37.82 | 49.0 | 135.0 | 29.0 | 86.0 | 23.0 | 6.1183 | 75000.0 | NEAR BAY | -122252.18 | . 62 62 | -122.29 | 37.81 | 50.0 | 760.0 | 190.0 | 377.0 | 122.0 | 0.9011 | 86100.0 | NEAR BAY | -122252.19 | . 67 67 | -122.29 | 37.80 | 52.0 | 1027.0 | 244.0 | 492.0 | 147.0 | 2.6094 | 81300.0 | NEAR BAY | -122252.20 | . However the sampling we have considered here or the one used in Scikit-learn is random sampling by default. This is fine for large dataset however for smaller dataset it is utmost important that the sampled data is representative of the main population data or else we will introduce sampling bias. . This is an important bias that could be introduced without prior knowledge and could be overlooked at multiple occassion leading to wrong conclusions. To ensure the sampled dataset is representative of the population set we use stratified sampling (pseudo-random sampling). To make the stratified sampling tractable we first divide the main data into multiple &#39;stratas&#39; based on an variable which we feel is an feature that should be replicated in our test set. The sample is divided into strata and right number of instances are chosen from each strata. We must not have too many stratas and the each strate must have appropriate number of instances. . For the case of property pricing in the district, median_income variable is chosen as the variable whose distribution in the main population and the randomly chosen test sample is same. This attribute is an important attribute to predict the final median housing price. So we can think of converting the continuous variable of median_variable into categorical variable -- that is stratas. . Stratified sampling using median income . housing[&quot;median_income&quot;].hist() . &lt;AxesSubplot:&gt; . From the median_income histogram it is seen that most of the entries are clustered in the range of 2-5 (arbitrary units). We can then use this information to make stratas around these instances. Cut routine in the pandas is used for this purpose. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], #bins around 2-5 income bracket labels=[1, 2, 3, 4, 5]) housing[&quot;income_cat&quot;].value_counts() . 3 7236 2 6581 4 3639 5 2362 1 822 Name: income_cat, dtype: int64 . housing[&quot;income_cat&quot;].hist() . &lt;AxesSubplot:&gt; . Now with the population categorised into various median income groups we can use stratified sampling routine (as implemented in scikit-learn) to make our test-set. As an additional proof let&#39;s compare this to a randomly sampled test_case. We will redo the random sampling we did prviously but with the new population with categorised median_income. . from sklearn.model_selection import StratifiedShuffleSplit, train_test_split split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] #Using random sampling rand_train_set, rand_test_set = train_test_split(housing, test_size=0.2, random_state=42) . Let&#39;s check the distribution of the income_cat variable in the strat_test, random_test, and the main sample. . housing[&quot;income_cat&quot;].value_counts()/len(housing[&quot;income_cat&quot;]) . 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 . rand_test_set[&quot;income_cat&quot;].value_counts()/len(rand_test_set[&quot;income_cat&quot;]) . 3 0.358527 2 0.324370 4 0.167393 5 0.109496 1 0.040213 Name: income_cat, dtype: float64 . strat_test_set[&quot;income_cat&quot;].value_counts()/len(strat_test_set[&quot;income_cat&quot;]) . 3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64 . def income_cat_proportions(data): return data[&quot;income_cat&quot;].value_counts() / len(data) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(housing), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(rand_test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 . compare_props . Overall Stratified Random Rand. %error Strat. %error . 1 0.039826 | 0.039729 | 0.040213 | 0.973236 | -0.243309 | . 2 0.318847 | 0.318798 | 0.324370 | 1.732260 | -0.015195 | . 3 0.350581 | 0.350533 | 0.358527 | 2.266446 | -0.013820 | . 4 0.176308 | 0.176357 | 0.167393 | -5.056334 | 0.027480 | . 5 0.114438 | 0.114583 | 0.109496 | -4.318374 | 0.127011 | . Now, we can remove the income_cat column . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Preliminary visualization of the data . Let&#39;s now dive a bit deeper into the data visualization and analysis. Before we do so, copy the strat_train_set as that would be the data-set we would be playing around and make sure the main data-set is not touched. . housing=strat_train_set.copy() housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 16512 entries, 17606 to 15775 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 16512 non-null float64 1 latitude 16512 non-null float64 2 housing_median_age 16512 non-null float64 3 total_rooms 16512 non-null float64 4 total_bedrooms 16354 non-null float64 5 population 16512 non-null float64 6 households 16512 non-null float64 7 median_income 16512 non-null float64 8 median_house_value 16512 non-null float64 9 ocean_proximity 16512 non-null object dtypes: float64(9), object(1) memory usage: 1.4+ MB . Geographical visualization . Let&#39;s now plot the data entries in the housing data as per the latitude and longitude. . housing.plot(kind=&#39;scatter&#39;,x=&#39;longitude&#39;,y=&#39;latitude&#39;); . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points. . &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . This look&#39;s like California however, we cannot infer anything more out of this. Let&#39;s play around a little bit more... . Playing with the alpha value in the plotting routine allows us to see the frequency of THAT datapoint in the plot | housing.plot(kind=&#39;scatter&#39;,x=&#39;longitude&#39;,y=&#39;latitude&#39;,alpha=0.1); . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points. . &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . From here, we can see the high density of listings in the Bay area and LA also around Sacramento and Fresco. . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(8,5), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, sharex=False) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82c1692f70&gt; . This is more interesting! We have now plotted the data with more information. Each data-point has two additional set of info apart of frequency of occurence. First being the color of the point is the median_house_value entry (option c). The radius of the data-point is the population of that district (option s). It can be seen that the housing prices are very much related to the location. The ones closer to the bay area are more expensive but need not be densely populated. . Looking for simple correlations . In addition to looking at the plot of housing price, we can check for simpler correaltions. Pearson&#39;s correlation matrix is something which is in-built in pandas and can be directly used. It checks for correlation between every pair of feature provided in the data-set. It estimates the covariance of the two features and estimates whether the correlation is inverse, direct, or none. . corr_matrix=housing.corr() corr_matrix.style.background_gradient(cmap=&#39;coolwarm&#39;).set_precision(2) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . longitude 1.00 | -0.92 | -0.11 | 0.05 | 0.08 | 0.11 | 0.06 | -0.02 | -0.05 | . latitude -0.92 | 1.00 | 0.01 | -0.04 | -0.07 | -0.12 | -0.08 | -0.08 | -0.14 | . housing_median_age -0.11 | 0.01 | 1.00 | -0.36 | -0.33 | -0.30 | -0.31 | -0.11 | 0.11 | . total_rooms 0.05 | -0.04 | -0.36 | 1.00 | 0.93 | 0.86 | 0.92 | 0.20 | 0.14 | . total_bedrooms 0.08 | -0.07 | -0.33 | 0.93 | 1.00 | 0.88 | 0.98 | -0.01 | 0.05 | . population 0.11 | -0.12 | -0.30 | 0.86 | 0.88 | 1.00 | 0.90 | 0.00 | -0.03 | . households 0.06 | -0.08 | -0.31 | 0.92 | 0.98 | 0.90 | 1.00 | 0.01 | 0.06 | . median_income -0.02 | -0.08 | -0.11 | 0.20 | -0.01 | 0.00 | 0.01 | 1.00 | 0.69 | . median_house_value -0.05 | -0.14 | 0.11 | 0.14 | 0.05 | -0.03 | 0.06 | 0.69 | 1.00 | . corr_matrix[&#39;median_house_value&#39;].sort_values(ascending=True) . latitude -0.142724 longitude -0.047432 population -0.026920 total_bedrooms 0.047689 households 0.064506 housing_median_age 0.114110 total_rooms 0.135097 median_income 0.687160 median_house_value 1.000000 Name: median_house_value, dtype: float64 . The correlation matrix suggests the amount of correlation between a pair of variables. When close to 1 it means a strong +ve correlation whereas, -1 means an inverse correlation. Looking at the correlation between median_house_values and other variables, we can see that there&#39;s some correlation with median_income (0.68 -- so +ve), and with the latitude (-0.14 -- so an inverse relation). . Another to check this relation is to plot scatter plots for each pair of variables in the dataset. Below we plot this for few potential/interesting variables . from pandas.plotting import scatter_matrix attributes = [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;] scatter_axes = scatter_matrix(housing[attributes], figsize=(12, 8)); #y labels [plt.setp(item.yaxis.get_label(), &#39;size&#39;, 10) for item in scatter_axes.ravel()]; #x labels [plt.setp(item.xaxis.get_label(), &#39;size&#39;, 10) for item in scatter_axes.ravel()]; . The diagonal entries show the histogram for each variable. We saw this previously for some variables. The most promising variable from this analysis seems to be the median_income. . housing.plot(kind=&quot;scatter&quot;, x=&quot;median_income&quot;, y=&quot;median_house_value&quot;, alpha=0.1); . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points. . Plotting it shows the stronger correlation with the target variable i.e. median_house_value however we can see horizontal lines (especially at USD 500k, 450k 350k) these could be due to some stratifying done in the dataset implicitly. It would be better to remove those to ensure our model does not spuriously fit for those since they are some of the quirks in the data. . Experimenting with attributes . Before we began proposing models for the data. We can play around with the variables and try different combinations of them to see if we get better trends. Let&#39;s look at a few. First, the total_room and/or total_bedroom variable could be changed to average_bedroom_per_house to better for bedrooms rather than looking for total bedroom in that district we would be looking at avg_bedroom per district and similarly we would do it for rooms. . housing[&#39;avg_bedroom&#39;]=housing[&#39;total_bedrooms&#39;]/housing[&#39;households&#39;] #Average room per house-holds in the district housing[&#39;avg_room&#39;]=housing[&#39;total_rooms&#39;]/housing[&#39;households&#39;] #Average bedrooms per rooms in a given district housing[&#39;bedroom_per_room&#39;]=housing[&#39;total_bedrooms&#39;]/housing[&#39;total_rooms&#39;] #Average population per household in a given district housing[&#39;population_per_household&#39;]=housing[&#39;population&#39;]/housing[&#39;households&#39;] #Average room per population in a given district housing[&#39;room_per_popoulation&#39;]=housing[&#39;total_rooms&#39;]/housing[&#39;population&#39;] #Average room per population in a given district housing[&#39;room_per_popoulation&#39;]=housing[&#39;total_rooms&#39;]/housing[&#39;population&#39;] . corr_matrix=housing.corr() corr_matrix.style.background_gradient(cmap=&#39;coolwarm&#39;).set_precision(2) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value avg_bedroom avg_room bedroom_per_room population_per_household room_per_popoulation . longitude 1.00 | -0.92 | -0.11 | 0.05 | 0.08 | 0.11 | 0.06 | -0.02 | -0.05 | 0.01 | -0.03 | 0.10 | -0.00 | -0.07 | . latitude -0.92 | 1.00 | 0.01 | -0.04 | -0.07 | -0.12 | -0.08 | -0.08 | -0.14 | 0.07 | 0.11 | -0.12 | 0.01 | 0.14 | . housing_median_age -0.11 | 0.01 | 1.00 | -0.36 | -0.33 | -0.30 | -0.31 | -0.11 | 0.11 | -0.08 | -0.15 | 0.14 | 0.02 | -0.10 | . total_rooms 0.05 | -0.04 | -0.36 | 1.00 | 0.93 | 0.86 | 0.92 | 0.20 | 0.14 | 0.03 | 0.13 | -0.19 | -0.02 | 0.12 | . total_bedrooms 0.08 | -0.07 | -0.33 | 0.93 | 1.00 | 0.88 | 0.98 | -0.01 | 0.05 | 0.04 | 0.00 | 0.09 | -0.03 | 0.05 | . population 0.11 | -0.12 | -0.30 | 0.86 | 0.88 | 1.00 | 0.90 | 0.00 | -0.03 | -0.07 | -0.07 | 0.04 | 0.08 | -0.14 | . households 0.06 | -0.08 | -0.31 | 0.92 | 0.98 | 0.90 | 1.00 | 0.01 | 0.06 | -0.06 | -0.08 | 0.07 | -0.03 | -0.04 | . median_income -0.02 | -0.08 | -0.11 | 0.20 | -0.01 | 0.00 | 0.01 | 1.00 | 0.69 | -0.06 | 0.31 | -0.62 | 0.02 | 0.23 | . median_house_value -0.05 | -0.14 | 0.11 | 0.14 | 0.05 | -0.03 | 0.06 | 0.69 | 1.00 | -0.04 | 0.15 | -0.26 | -0.02 | 0.20 | . avg_bedroom 0.01 | 0.07 | -0.08 | 0.03 | 0.04 | -0.07 | -0.06 | -0.06 | -0.04 | 1.00 | 0.86 | 0.05 | -0.01 | 0.84 | . avg_room -0.03 | 0.11 | -0.15 | 0.13 | 0.00 | -0.07 | -0.08 | 0.31 | 0.15 | 0.86 | 1.00 | -0.40 | -0.01 | 0.90 | . bedroom_per_room 0.10 | -0.12 | 0.14 | -0.19 | 0.09 | 0.04 | 0.07 | -0.62 | -0.26 | 0.05 | -0.40 | 1.00 | 0.00 | -0.26 | . population_per_household -0.00 | 0.01 | 0.02 | -0.02 | -0.03 | 0.08 | -0.03 | 0.02 | -0.02 | -0.01 | -0.01 | 0.00 | 1.00 | -0.05 | . room_per_popoulation -0.07 | 0.14 | -0.10 | 0.12 | 0.05 | -0.14 | -0.04 | 0.23 | 0.20 | 0.84 | 0.90 | -0.26 | -0.05 | 1.00 | . corr_matrix[&#39;median_house_value&#39;].sort_values(ascending=True) . bedroom_per_room -0.259984 latitude -0.142724 longitude -0.047432 avg_bedroom -0.043343 population -0.026920 population_per_household -0.021985 total_bedrooms 0.047689 households 0.064506 housing_median_age 0.114110 total_rooms 0.135097 avg_room 0.146285 room_per_popoulation 0.199429 median_income 0.687160 median_house_value 1.000000 Name: median_house_value, dtype: float64 . This is interesting! We see that bedroom_per_room is another potential descriptor with negative corelation moreover we get room_per_population and avg_room to be decent new descriptors for the median_house_value. Not bad for a simple math manipulation to better represent the data. This is a crucial step and where domain knowledge and intuition would come handy. . Data cleaning and prepping . Separate the predictors and the target values | Write functions to conduct various data transformations ensuring consistency and ease | Make sure the data is devoid of any NaN values since that would raise warning and errors. We have three strategies we can implement here: a. Get rid of those points (districts) entirely b. Get rid of whole attribute c. Set missing values to either zero or one of the averages (mean, median, or mode) | In our case, total bedrooms had some missing values. . # Option a: housing.dropna(subset=[&quot;total_bedrooms&quot;]) # Option b: housing.drop(&quot;total_bedrooms&quot;, axis=1) # Option c: median = housing[&quot;total_bedrooms&quot;].median() housing[&quot;total_bedrooms&quot;].fillna(median, inplace=True) # option 3 . Before we do any of this let&#39;s first separate the predictor and target_values . housing = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) # drop labels for training set housing_labels = strat_train_set[&quot;median_house_value&quot;].copy() . sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head() sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | NaN | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | NaN | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | NaN | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | NaN | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | NaN | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . sample_incomplete_rows.dropna(subset=[&quot;total_bedrooms&quot;]) # option 1 . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . sample_incomplete_rows.drop(&quot;total_bedrooms&quot;, axis=1) #option 2 . longitude latitude housing_median_age total_rooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . median = housing[&quot;total_bedrooms&quot;].median() sample_incomplete_rows[&quot;total_bedrooms&quot;].fillna(median, inplace=True) # option 3 sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 433.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 433.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 433.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 433.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 433.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . Scikit-learn imputer class . This is a handy class to take of missing values. First, we create an instance of that class with specifying what is to be replaced and what strategy is used. Before doing so, we need to make srue the entire data-set has ONLY numerical entries and Imputer will evaluate the given average for all the dataset and store it in the statistics_ instance . What the imputer will do is, . Evaluate an specified type of average. | For a given numerical data-set look for NaN or Null entires in a given attribute and replace it with the computed avearge for that attribute | try: from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+ except ImportError: from sklearn.preprocessing import Imputer as SimpleImputer imputer = SimpleImputer(strategy=&quot;median&quot;) #We define the strategy here housing_num = housing.drop(&#39;ocean_proximity&#39;, axis=1) # alternatively: housing_num = housing.select_dtypes(include=[np.number]) . imputer.fit(housing_num) . SimpleImputer(strategy=&#39;median&#39;) . imputer.statistics_ . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . housing_num.median().values . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . We can now use this as a training variables set for our model . X = imputer.transform(housing_num) . We convert the Pandas dataframe entries to a numpy array which is transformed with appropriately replacing the missing entries with median. . print(type(X), type(housing_num)) . &lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; . print(np.shape(X), housing_num.shape) &#39;&#39;&#39; If we need the data-frame back housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index) &#39;&#39;&#39; . (16512, 8) (16512, 8) . &#39; nIf we need the data-frame back nhousing_tr = pd.DataFrame(X, columns=housing_num.columns, n index=housing.index) n&#39; . Handling Text and Categorical Attribute . Now let&#39;s preprocess the categorical input feature, ocean_proximity: . housing_cat = housing[[&#39;ocean_proximity&#39;]] type(housing_cat) . pandas.core.frame.DataFrame . Converting the categorical entries to integers . try: from sklearn.preprocessing import OrdinalEncoder except ImportError: from future_encoders import OrdinalEncoder # Scikit-Learn &lt; 0.20 ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) housing_cat_encoded[:10] . array([[0.], [0.], [4.], [1.], [0.], [1.], [0.], [1.], [0.], [0.]]) . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 7276 INLAND 5263 NEAR OCEAN 2124 NEAR BAY 1847 ISLAND 2 Name: ocean_proximity, dtype: int64 . Now, housing_cat_encoded has converted the categorical entries to purely numerical values for each category . ordinal_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . Now, this is helpful with the string categories getting converted to numerical categories. However, there is still an small issue. The categorical numbering may introduce some bias in the final model. ML algorithms will assume that two nearby values are more similar than two distant values. . In the above case, &#39;&lt;1H OCEAN&#39; and &#39;INLAND&#39;have category values as 0 and 1 however &#39;&lt;1H OCEAN&#39; is more closer to &#39;NEAR OCEAN&#39; with category value 4. . To fix this issue, one solution is to create one binary attribute per category. This is called One-hot encoding as ONLY one of the attribute in the vector is 1 (hot) and others are 0 (cold). . Scikit-learn provides a OneHotEncoder encoder to convert integer categorical values to one-hot vectors. . try: from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn &lt; 0.20 from sklearn.preprocessing import OneHotEncoder except ImportError: from future_encoders import OneHotEncoder # Scikit-Learn &lt; 0.20 cat_encoder = OneHotEncoder() #1-Hot encoded vector for the housing training data-set housing_cat_1hot = cat_encoder.fit_transform(housing_cat) type(housing_cat_1hot) . scipy.sparse.csr.csr_matrix . A sparse array is declared in this case which has the position of the non-zero value and not necessarily the entire numpy matrix. This is helpful in the cases where there are too many categories and also many datapoints. For examples, if we have 4 categories and 1000 datapoints the final one-hot matrix would be 1000x4 size. Most of that would be full of 0s, with only one 1 per row for a particular category. . The housing_cat_1hot can be converted to numpy array by using the housing_cat_1hot.toarray() . Alternatively, you can set sparse=False when creating the OneHotEncoder . cat_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . Let&#39;s create a custom transformer to add extra attributes: . housing.columns . Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;ocean_proximity&#39;], dtype=&#39;object&#39;) . from sklearn.base import BaseEstimator, TransformerMixin # get the right column indices: safer than hard-coding indices rooms_ix, bedrooms_ix, population_ix, household_ix = [ list(housing.columns).index(col) for col in (&quot;total_rooms&quot;, &quot;total_bedrooms&quot;, &quot;population&quot;, &quot;households&quot;)] #Here we convert the housing.columns to list and #then find the index for the entry which matches the string in the loop class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, household_ix] population_per_household = X[:, population_ix] / X[:, household_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True) housing_extra_attribs = attr_adder.transform(housing.values) . From the above class, there&#39;s only one hyperparameter in the class. add_bedrooms_per_room is the only option and is set True by default. Let&#39;s check the new feature space by converting it to a Pandas Dataframe . housing_extra_attribs = pd.DataFrame( housing_extra_attribs, columns=list(housing.columns)+[&quot;bedrooms_per_room&quot;,&quot;rooms_per_household&quot;, &quot;population_per_household&quot;], index=housing.index) housing_extra_attribs.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity bedrooms_per_room rooms_per_household population_per_household . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | &lt;1H OCEAN | 4.625369 | 2.094395 | 0.223852 | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | &lt;1H OCEAN | 6.00885 | 2.707965 | 0.159057 | . 14650 -117.2 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | NEAR OCEAN | 4.225108 | 2.025974 | 0.241291 | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | INLAND | 5.232295 | 4.135977 | 0.200866 | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | &lt;1H OCEAN | 4.50581 | 3.047847 | 0.231341 | . Feature scaling . Feaature scaling is an important transformation needed to be applied to the data. With some exceptions, ML algorithms dont perform well when the input numerical entries have very different scales. Eg: One variable has range 0-1 but other variable has range 1-1000. This is the case in our data-base where the total number of rooms range from 6 to 39,320 while the objective variable i.e. median income only ranges from 0-15. Two common ways of scaling: . Min-max scaling (also called Normalization) Values are shifted such that they are normalized. They are rescaled in the range of 0-1. We do this by subtracting the min value and dividing by the range in the data begin{equation} x_{i} = frac{X_{i}-min}{max-min} end{equation} . | Standardization This is when the mean of the dataset is subtracted from each entry so that the data has mean as 0 and then divided by the standard deviation so that the resulting distribution has a unit variance. Unlike min-max scaling, standardization does not bound to a particular range like 0-1. However, standardisation is much less affected by outliers. If a particular values is extremely high or low that could affect the other inputs in the case of min-max scaling. However that effect is reduced in the case of standardization given it does not directly account for the range in the scaling but the mean and variance. begin{equation} x_{i} = frac{X_{i}- mu}{ sigma} end{equation} . NOTE: It is important that these scaling operations are performed on the training data only and not on the full dataset . | Transformation pipelines . Pipeline class in scikit-learn can help with sequences of transformations. Now let&#39;s build a pipeline for preprocessing the numerical attributes (note that we could use CombinedAttributesAdder() instead of FunctionTransformer(...) if we preferred): . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), #Fill in missing values using the median of each entry (&#39;attribs_adder&#39;, CombinedAttributesAdder(add_bedrooms_per_room=True)), #Add additional columns entrys (&#39;std_scaler&#39;, StandardScaler()), #Feature scaling -- using standardisation here ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . housing_num_tr . array([[-1.15604281, 0.77194962, 0.74333089, ..., -0.31205452, -0.08649871, 0.15531753], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0.21768338, -0.03353391, -0.83628902], [ 1.18684903, -1.34218285, 0.18664186, ..., -0.46531516, -0.09240499, 0.4222004 ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0.3469342 , -0.03055414, -0.52177644], [ 0.78221312, -0.85106801, 0.18664186, ..., 0.02499488, 0.06150916, -0.30340741], [-1.43579109, 0.99645926, 1.85670895, ..., -0.22852947, -0.09586294, 0.10180567]]) . This Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last step/estimator must be the trasnformers (like feature scaling). . try: from sklearn.compose import ColumnTransformer except ImportError: from future_encoders import ColumnTransformer # Scikit-Learn &lt; 0.20 . Now let&#39;s join all these components into a big pipeline that will preprocess both the numerical and the categorical features (again, we could use CombinedAttributesAdder() instead of FunctionTransformer(...) if we preferred): . num_attribs = list(housing_num) cat_attribs = [&quot;ocean_proximity&quot;] full_pipeline = ColumnTransformer([ (&quot;num&quot;, num_pipeline, num_attribs), (&quot;cat&quot;, OneHotEncoder(), cat_attribs), ]) housing_prepared = full_pipeline.fit_transform(housing) . housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . housing_prepared.shape . (16512, 16) . Step 3: Select and train a model . Finally. . Training and evaluation on the training set . Given the prioir transformations, the features are scaled, categories are converted to one-hot vectors, and the missing variables are taken account of. Let&#39;s train a Linear Regression model first . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) . LinearRegression() . trial_input = housing.iloc[:5] #First 5 entries trial_label = housing_labels.iloc[:5] #First 5 labels corresponding to the entries prep_trial_input = full_pipeline.transform(trial_input) #Transforming the entries to suit the trained input print(&#39;Predictions:&#39;,lin_reg.predict(prep_trial_input)) . Predictions: [210644.60459286 317768.80697211 210956.43331178 59218.98886849 189747.55849879] . print(&#39;Labels:&#39;,list(trial_label)) . Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] . from sklearn.metrics import mean_squared_error housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) lin_rmse . 68628.19819848923 . housing_labels.describe() . count 16512.000000 mean 206990.920724 std 115703.014830 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 . As seen the RMSE is 68628 which is better than nothing but still it is quite high when the range of the median_house_values range from 15000 to 500000 . from sklearn.metrics import mean_absolute_error lin_mae = mean_absolute_error(housing_labels, housing_predictions) lin_mae . 49439.89599001897 . Here the model is underfitting the data since the RMSE is so high. . When this happens either the features do not provide enough information to make good predictions or the model is not powerful enough. . Let&#39;s try using a more complex model, DecisionTreeRegressor which is capable of finding non-linear relationships in the data . Decision Tree Regressor . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(random_state=42) tree_reg.fit(housing_prepared, housing_labels) . DecisionTreeRegressor(random_state=42) . housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse . 0.0 . LOL! . This model is badly overfitting the data! Let&#39;s proof this hypothesis using cross-validation schemes. We dont want to touch the test set, JUST WORK WITH THE TRAINING SET. Only touch the test set when the model we are using is good enough. We will use the part of the training set for training and other part for model validation. . Fine-tune the model . Cross-validation . Cross-validation is a method of getting reliable estimate of model performance using only the training data 10-fold cross-validation breaking training data in 10 equal parts creating 10 miniature test/train splits. Out of the 10 folds, train data on 9 and test on 10th. Do this 10 times each time holding out different fold. . Scikit-learn cross-validation feature expects a utility function (greater the better) rather than a cost function (lower the better), so to score the functions we use opposite of MSE, which is why we again compute -scores before calculating the square root . from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) tree_rmse_scores = np.sqrt(-scores) . def display_scores(scores): print(&quot;Scores:&quot;, scores) print(&quot;Mean:&quot;, scores.mean()) print(&quot;Standard deviation:&quot;, scores.std()) display_scores(tree_rmse_scores) . Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027] Mean: 71407.68766037929 Standard deviation: 2439.4345041191004 . Cross-validation not only allows us to get an estimate of the performance of the model but also the measure of how precise this estimate is (i.e. standard deviation). The Decision tree has high std-dev. This information could not be obtained with just one validation set. However, caveat is that cross-validation comes at the cost of training the model several times, so it is not always possible. . Let&#39;s compute the same score for LinearRegresson model. . lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores) . Scores: [66782.73843989 66960.118071 70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067] Mean: 69052.46136345083 Standard deviation: 2731.674001798342 . Here it can be seen that DecisionTree model performs much worse than the LinearRegression model. . Random Forrest Regressor . Random forrest works by employing many decision trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning and it is often great way to push ML algorithms even further. . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=10, random_state=42) forest_reg.fit(housing_prepared, housing_labels) . RandomForestRegressor(n_estimators=10, random_state=42) . Note:we specify n_estimators=10 to avoid a warning about the fact that the default value is going to change to 100 in Scikit-Learn 0.22. . housing_predictions = forest_reg.predict(housing_prepared) forest_mse = mean_squared_error(housing_labels, housing_predictions) forest_rmse = np.sqrt(forest_mse) forest_rmse . 21933.31414779769 . from sklearn.model_selection import cross_val_score forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores) display_scores(forest_rmse_scores) . Scores: [51646.44545909 48940.60114882 53050.86323649 54408.98730149 50922.14870785 56482.50703987 51864.52025526 49760.85037653 55434.21627933 53326.10093303] Mean: 52583.72407377466 Standard deviation: 2298.353351147122 . Random forest regressor looks better than DecisionTree and LinearRegression. The RMSE is still quite high for production quality code and could be due to overfitting. We can try other algorithms before spending time on a particular algorithm tweaking the hyperparameters. The goal is to shortlist 2-3 methods that are promising then fine-tune the model. Before we move ahead we can take a look at one more ML algorithm which is commonly employed for such supervised learning cases: Support Vector Regression . from sklearn.svm import SVR svm_reg = SVR(kernel=&quot;linear&quot;) svm_reg.fit(housing_prepared, housing_labels) housing_predictions = svm_reg.predict(housing_prepared) svm_mse = mean_squared_error(housing_labels, housing_predictions) svm_rmse = np.sqrt(svm_mse) svm_rmse . 111094.6308539982 . Step 4: Fine-tune the model . Once we settle for an algorithm we can fine-tune them efficiently using some of the in-built scikit-learn routines. . Grid Search . GridSearchCV is a faster way of tweaking the hyper-parameters for a given algorithm. It needs the hyper-parameters you want to experiment with, what values to try out, and it will evaluate possible combination of hyperparameters values using cross-validation. We can do that step for RandomForrestRegressor which we found to have lowesst RMSE of the three methods we tried . from sklearn.model_selection import GridSearchCV param_grid = [ # try 12 (3×4) combinations of hyperparameters {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, # then try 6 (2×3) combinations with bootstrap set as False {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor(random_state=42) # train across 5 folds, that&#39;s a total of (12+6)*5=90 rounds of training grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) grid_search.fit(housing_prepared, housing_labels) . GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), param_grid=[{&#39;max_features&#39;: [2, 4, 6, 8], &#39;n_estimators&#39;: [3, 10, 30]}, {&#39;bootstrap&#39;: [False], &#39;max_features&#39;: [2, 3, 4], &#39;n_estimators&#39;: [3, 10]}], return_train_score=True, scoring=&#39;neg_mean_squared_error&#39;) . The param_grid tells Scikit-learn to: . First evaluate 3x4 combinations of n_estimators and max_features with bootstrap True which is the default. | Then with bootstrap set as False we look for 2x3 combinations of n_estimators and max_featurs for the random forest | Finally, both these models are trained 5 times for the cross validation purposes in a 5-fold cross-validation fashion. | Total of (12+6)x5=90 round of training are conducted. Finally when it is done we get the best model parameters which give lowest RMSE. . grid_search.best_params_ . {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} . grid_search.best_estimator_ . RandomForestRegressor(max_features=8, n_estimators=30, random_state=42) . cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(np.sqrt(-mean_score), params) . 63669.11631261028 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55627.099719926795 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 53384.57275149205 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60965.950449450494 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52741.04704299915 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50377.40461678399 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58663.93866579625 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52006.19873526564 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 50146.51167415009 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 57869.25276169646 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 51711.127883959234 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 49682.273345071546 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 62895.06951262424 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54658.176157539405 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 59470.40652318466 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52724.9822587892 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 57490.5691951261 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 51009.495668875716 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} . pd.DataFrame(grid_search.cv_results_) . mean_fit_time std_fit_time mean_score_time std_score_time param_max_features param_n_estimators param_bootstrap params split0_test_score split1_test_score ... mean_test_score std_test_score rank_test_score split0_train_score split1_train_score split2_train_score split3_train_score split4_train_score mean_train_score std_train_score . 0 0.053401 | 0.002499 | 0.003312 | 0.000173 | 2 | 3 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} | -3.837622e+09 | -4.147108e+09 | ... | -4.053756e+09 | 1.519591e+08 | 18 | -1.064113e+09 | -1.105142e+09 | -1.116550e+09 | -1.112342e+09 | -1.129650e+09 | -1.105559e+09 | 2.220402e+07 | . 1 0.182488 | 0.002683 | 0.010123 | 0.001727 | 2 | 10 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} | -3.047771e+09 | -3.254861e+09 | ... | -3.094374e+09 | 1.327062e+08 | 11 | -5.927175e+08 | -5.870952e+08 | -5.776964e+08 | -5.716332e+08 | -5.802501e+08 | -5.818785e+08 | 7.345821e+06 | . 2 0.503301 | 0.010947 | 0.024532 | 0.000476 | 2 | 30 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} | -2.689185e+09 | -3.021086e+09 | ... | -2.849913e+09 | 1.626875e+08 | 9 | -4.381089e+08 | -4.391272e+08 | -4.371702e+08 | -4.376955e+08 | -4.452654e+08 | -4.394734e+08 | 2.966320e+06 | . 3 0.081569 | 0.001284 | 0.003247 | 0.000154 | 4 | 3 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} | -3.730181e+09 | -3.786886e+09 | ... | -3.716847e+09 | 1.631510e+08 | 16 | -9.865163e+08 | -1.012565e+09 | -9.169425e+08 | -1.037400e+09 | -9.707739e+08 | -9.848396e+08 | 4.084607e+07 | . 4 0.269809 | 0.005145 | 0.008730 | 0.000133 | 4 | 10 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} | -2.666283e+09 | -2.784511e+09 | ... | -2.781618e+09 | 1.268607e+08 | 8 | -5.097115e+08 | -5.162820e+08 | -4.962893e+08 | -5.436192e+08 | -5.160297e+08 | -5.163863e+08 | 1.542862e+07 | . 5 0.800619 | 0.007793 | 0.024065 | 0.000543 | 4 | 30 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} | -2.387153e+09 | -2.588448e+09 | ... | -2.537883e+09 | 1.214614e+08 | 3 | -3.838835e+08 | -3.880268e+08 | -3.790867e+08 | -4.040957e+08 | -3.845520e+08 | -3.879289e+08 | 8.571233e+06 | . 6 0.113473 | 0.004140 | 0.003330 | 0.000178 | 6 | 3 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} | -3.119657e+09 | -3.586319e+09 | ... | -3.441458e+09 | 1.893056e+08 | 14 | -9.245343e+08 | -8.886939e+08 | -9.353135e+08 | -9.009801e+08 | -8.624664e+08 | -9.023976e+08 | 2.591445e+07 | . 7 0.378872 | 0.014358 | 0.009405 | 0.001258 | 6 | 10 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} | -2.549663e+09 | -2.782039e+09 | ... | -2.704645e+09 | 1.471569e+08 | 6 | -4.980344e+08 | -5.045869e+08 | -4.994664e+08 | -4.990325e+08 | -5.055542e+08 | -5.013349e+08 | 3.100456e+06 | . 8 1.222785 | 0.030201 | 0.027815 | 0.001110 | 6 | 30 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} | -2.370010e+09 | -2.583638e+09 | ... | -2.514673e+09 | 1.285080e+08 | 2 | -3.838538e+08 | -3.804711e+08 | -3.805218e+08 | -3.856095e+08 | -3.901917e+08 | -3.841296e+08 | 3.617057e+06 | . 9 0.146751 | 0.002996 | 0.003320 | 0.000174 | 8 | 3 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} | -3.353504e+09 | -3.348552e+09 | ... | -3.348850e+09 | 1.241939e+08 | 13 | -9.228123e+08 | -8.553031e+08 | -8.603321e+08 | -8.881964e+08 | -9.151287e+08 | -8.883545e+08 | 2.750227e+07 | . 10 0.510138 | 0.016372 | 0.009396 | 0.000904 | 8 | 10 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} | -2.571970e+09 | -2.718994e+09 | ... | -2.674041e+09 | 1.392777e+08 | 5 | -4.932416e+08 | -4.815238e+08 | -4.730979e+08 | -5.155367e+08 | -4.985555e+08 | -4.923911e+08 | 1.459294e+07 | . 11 1.518905 | 0.056501 | 0.025125 | 0.001086 | 8 | 30 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} | -2.357390e+09 | -2.546640e+09 | ... | -2.468328e+09 | 1.091662e+08 | 1 | -3.841658e+08 | -3.744500e+08 | -3.773239e+08 | -3.882250e+08 | -3.810005e+08 | -3.810330e+08 | 4.871017e+06 | . 12 0.077979 | 0.001837 | 0.003983 | 0.000390 | 2 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -3.785816e+09 | -4.166012e+09 | ... | -3.955790e+09 | 1.900964e+08 | 17 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 13 0.259085 | 0.003267 | 0.010411 | 0.000311 | 2 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -2.810721e+09 | -3.107789e+09 | ... | -2.987516e+09 | 1.539234e+08 | 10 | -6.056477e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -2.967449e+00 | -6.056027e-01 | 1.181156e+00 | . 14 0.104454 | 0.004167 | 0.003841 | 0.000315 | 3 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -3.618324e+09 | -3.441527e+09 | ... | -3.536729e+09 | 7.795057e+07 | 15 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -6.072840e+01 | -1.214568e+01 | 2.429136e+01 | . 15 0.345978 | 0.004147 | 0.010372 | 0.000444 | 3 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -2.757999e+09 | -2.851737e+09 | ... | -2.779924e+09 | 6.286720e+07 | 7 | -2.089484e+01 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -5.465556e+00 | -5.272080e+00 | 8.093117e+00 | . 16 0.135484 | 0.015839 | 0.004140 | 0.000585 | 4 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -3.134040e+09 | -3.559375e+09 | ... | -3.305166e+09 | 1.879165e+08 | 12 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 17 0.453605 | 0.013427 | 0.010725 | 0.000358 | 4 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -2.525578e+09 | -2.710011e+09 | ... | -2.601969e+09 | 1.088048e+08 | 4 | -0.000000e+00 | -1.514119e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -3.028238e-03 | 6.056477e-03 | . 18 rows × 23 columns . Randomised search . Grid search approach is fine when we are exploring relatively few combinations. But when hyperparameter space is large it is often preferrable to use RandomizedSearchCV instead. Here instead of doing all the possible combinationes of hyperparameters, it evaluates a given number of random combinations by selecting a random value for each hyper parameter at every iteration. . Ensemble search . Combine models that perform best. The group or &#39;ensemble&#39; will often perform better than the best individual model just like RandomForest peforms better than Decision Trees especially if we have individual models make different types of errors. . Step 5: Analyze the Best Models and their Errors . RandomForestRegressor can indicate the relative importance of each attribute for making the accurate predictions. . feature_importances = grid_search.best_estimator_.feature_importances_ feature_importances . array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]) . extra_attribs = [&quot;rooms_per_hhold&quot;, &quot;pop_per_hhold&quot;, &quot;bedrooms_per_room&quot;] cat_encoder = full_pipeline.named_transformers_[&quot;cat&quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True) . [(0.36615898061813423, &#39;median_income&#39;), (0.16478099356159054, &#39;INLAND&#39;), (0.10879295677551575, &#39;pop_per_hhold&#39;), (0.07334423551601243, &#39;longitude&#39;), (0.06290907048262032, &#39;latitude&#39;), (0.056419179181954014, &#39;rooms_per_hhold&#39;), (0.053351077347675815, &#39;bedrooms_per_room&#39;), (0.04114379847872964, &#39;housing_median_age&#39;), (0.014874280890402769, &#39;population&#39;), (0.014672685420543239, &#39;total_rooms&#39;), (0.014257599323407808, &#39;households&#39;), (0.014106483453584104, &#39;total_bedrooms&#39;), (0.010311488326303788, &#39;&lt;1H OCEAN&#39;), (0.0028564746373201584, &#39;NEAR OCEAN&#39;), (0.0019604155994780706, &#39;NEAR BAY&#39;), (6.0280386727366e-05, &#39;ISLAND&#39;)] . Step 6: Evaluate the model on the Test Set . final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(&quot;median_house_value&quot;, axis=1) y_test = strat_test_set[&quot;median_house_value&quot;].copy() X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) . fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(y_test, final_predictions, s=100) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.plot(lims, lims, &#39;k--&#39;, linewidth=2.0, alpha=0.75, zorder=0) ax.set_aspect(&#39;equal&#39;) ax.set_xlim(lims) ax.set_ylim(lims) ax.set_xlabel(&#39;ML Prediction&#39;) ax.set_ylabel(&#39;Actual Value&#39;) . Text(0, 0.5, &#39;Actual Value&#39;) . final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) print(final_rmse) . 47730.22690385927 . We can compute a 95% confidence interval for the test RMSE: . from scipy import stats confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 mean = squared_errors.mean() m = len(squared_errors) np.sqrt(stats.t.interval(confidence, m - 1, loc=np.mean(squared_errors), scale=stats.sem(squared_errors))) . array([45685.10470776, 49691.25001878]) . Alternatively, we could use a z-scores rather than t-scores: . zscore = stats.norm.ppf((1 + confidence) / 2) zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m) np.sqrt(mean - zmargin), np.sqrt(mean + zmargin) . (45685.717918136594, 49690.68623889426) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/exploratory-data-analysis/machine-learning/2019/08/02/end2endML_housing.html",
            "relUrl": "/python/exploratory-data-analysis/machine-learning/2019/08/02/end2endML_housing.html",
            "date": " • Aug 2, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hello, I am Pushkar Ghanekar. . This blog is a compilation of tinkerings, learnings, and helpful tips I have accumulated in the area of data science and machine learning, with some flavor of chemical science sprinkled sporadically. The posts are meant to be a reference for my future self and whoever wishes to start dabbling in the data science and ML space. . This is my first-ever venture in the world of blogging and is definitely a work in progress. Would love hear what you think! . You can check out my personal website here. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "http://pgg1610.github.io/blog_fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://pgg1610.github.io/blog_fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}