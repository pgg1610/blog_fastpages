{
  
    
        "post0": {
            "title": "Implement NN from scratch for binary data classification",
            "content": "In this notebook I build a simple neural network, having a single hidden layer. Next, I compare this model for its classification accuracy to a boilerplate logistic regression. . Implement a 2-class classification neural network with a single hidden layer | Use units with a non-linear activation function, such as tanh | Compute the cross entropy loss | Implement forward and backward propagation | . This code was adapted from Andrew Ng&#39;s Deep Learning Specialization course on Coursera . 1 - Packages . Let&#39;s first import all the packages that you will need during this assignment. . numpy is the fundamental package for scientific computing with Python. | sklearn provides simple and efficient tools for data mining and data analysis. | matplotlib is a library for plotting graphs in Python. | testCases provides some test examples to assess the correctness of your functions | planar_utils provide various useful functions used in this assignment | . import numpy as np import matplotlib.pyplot as plt import sklearn import sklearn.datasets as datasets import sklearn.linear_model import copy as copy %config InlineBackend.figure_format = &#39;retina&#39; np.random.seed(42) # set a seed so that the results are consistent . 2 - Dataset . X, Y = datasets.make_moons(n_samples=1000, noise=.05) Y = Y.reshape(-1,1) print(X.shape, Y.shape) Code to make spirals is adapted from: http://cs231n.github.io/neural-networks-case-study/ . N = 400 # number of points per class D = 2 # dimensionality K = 2 # number of spokes X = np.zeros((N*K,D)) # data matrix (each row = single example) Y = np.zeros(N*K, dtype=&#39;int&#39;) # class labels for j in range(K): ix = range(N*j,N*(j+1)) r = np.linspace(0, 1, N) # radius t = np.linspace(j*4.2, (j+1)*4.2, N) + np.random.randn(N)*0.2 # theta X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] Y[ix] = (0 if j % 2 == 0 else 1) X = copy.deepcopy(X.T) Y = copy.deepcopy(Y.reshape(-1,1).T) . fig, ax = plt.subplots(1,1, figsize=(8,8)) # lets visualize the data: ax.scatter(X[0, :], X[1, :], c=Y.ravel(), s=40, cmap=plt.cm.Spectral) ax.set_xlabel(&#39;$X_1$&#39;) ax.set_ylabel(&#39;$X_2$&#39;) ax.set_title(&#39;Visualize data&#39;) . Text(0.5, 1.0, &#39;Visualize data&#39;) . shape_X = X.shape shape_Y = Y.shape print (&#39;The shape of X is: &#39; + str(shape_X)) print (&#39;The shape of Y is: &#39; + str(shape_Y)) . The shape of X is: (2, 800) The shape of Y is: (1, 800) . 3 - Simple Logistic Regression . Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn&#39;s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset. . clf = sklearn.linear_model.LogisticRegressionCV(); clf.fit(X.T, Y.ravel()); . Convenience function to plot a decision boundary for the classification model . def plot_decision_boundary(func, x_input, y_input): xx_1, xx_2 = np.mgrid[np.min(x_input[:,0]):np.max(x_input[:,0]):.01, np.min(x_input[:,1]):np.max(x_input[:,1]):.01] grid = np.c_[xx_1.ravel(), xx_2.ravel()] y_pred_grid = func(grid).reshape(xx_1.shape) y_pred = func(x_input) fig, ax = plt.subplots(figsize=(10, 10)) contour = ax.contourf(xx_1, xx_2, y_pred_grid, alpha=0.7, cmap=&quot;Spectral&quot;) ax.scatter(x_input[:,0], x_input[:, 1], c=y_pred, s=50, cmap=&quot;Spectral&quot;, edgecolor=&quot;white&quot;, linewidth=1) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.set(aspect=&#39;equal&#39;, xlim=(np.min(x_input[:,0]), np.max(x_input[:,0])), ylim=(np.min(x_input[:,1]),np.max(x_input[:,1])), xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;) . plot_decision_boundary(lambda x: clf.predict(x), X.T, Y.T) plt.title(&quot;Logistic Regression&quot;) . Text(0.5, 1.0, &#39;Logistic Regression&#39;) . LR_predictions = clf.predict(X.T) print (&#39;Accuracy of logistic regression: %d &#39; % float((np.dot(Y, LR_predictions) + np.dot(1-Y, 1-LR_predictions))/float(Y.size)*100) + &#39;% &#39; + &quot;(percentage of correctly labelled datapoints)&quot;) . Accuracy of logistic regression: 66 % (percentage of correctly labelled datapoints) . Interpretation: The dataset is not linearly separable, so logistic regression doesn&#39;t perform well. Hopefully a neural network will do better. . 4 - Neural Network model . Logistic regression did not work well on the dataset. You are going to train a Neural Network with a single hidden layer. . Here is our model: . Mathematically: . For one example $x^{(i)}$: $$z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1]} tag{1}$$ $$a^{[1] (i)} = tanh(z^{[1] (i)}) tag{2}$$ $$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]} tag{3}$$ $$ hat{y}^{(i)} = a^{[2] (i)} = sigma(z^{ [2] (i)}) tag{4}$$ $$y^{(i)}_{prediction} = begin{cases} 1 &amp; mbox{if } a^{[2](i)} &gt; 0.5 0 &amp; mbox{otherwise } end{cases} tag{5}$$ . Given the predictions on all the examples, you can also compute the cost $J$ as follows: $$J = - frac{1}{m} sum limits_{i = 0}^{m} large left( small y^{(i)} log left(a^{[2] (i)} right) + (1-y^{(i)}) log left(1- a^{[2] (i)} right) large right) small tag{6}$$ . The general methodology to build a Neural Network is to: . 1. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model&#39;s parameters 3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent) . 4.1 - Defining the neural network structure . Define three variables: . - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4) - n_y: the size of the output layer . def layer_sizes(X, Y, n_h=4): &quot;&quot;&quot; Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer &quot;&quot;&quot; n_x = X.shape[0] # size of input layer n_h = n_h n_y = Y.reshape(-1,1).T.shape[0] # size of output layer return (n_x, n_h, n_y) . (n_x, n_h, n_y) = layer_sizes(X, Y) print(&quot;The size of the input layer is: n_x = &quot; + str(n_x)) print(&quot;The size of the hidden layer is: n_h = &quot; + str(n_h)) print(&quot;The size of the output layer is: n_y = &quot; + str(n_y)) . The size of the input layer is: n_x = 2 The size of the hidden layer is: n_h = 4 The size of the output layer is: n_y = 1 . 4.2 - Initialize the model&#39;s parameters . Initialize the weights matrices with random values. Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). | . | Initialize the bias vectors as zeros. Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros. | . | . def initialize_parameters(n_x, n_h, n_y): &quot;&quot;&quot; Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) &quot;&quot;&quot; np.random.seed(42) # we set up a seed so that your output matches ours although the initialization is random. W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y,1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2} return parameters . 4.3 - The Loop . Implement forward_propagation(). . Instructions: . Look above at the mathematical representation of your classifier. | You can use the function sigmoid(). It is built-in (imported) in the notebook. | You can use the function np.tanh(). It is part of the numpy library. | The steps you have to implement are: Retrieve each parameter from the dictionary &quot;parameters&quot; (which is the output of initialize_parameters()) by using parameters[&quot;..&quot;]. | Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set). | | Values needed in the backpropagation are stored in &quot;cache&quot;. The cache will be given as an input to the backpropagation function. | . def sigmoid(x): z = 1/(1 + np.exp(-x)) return z . def forward_propagation(X, parameters): &quot;&quot;&quot; Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot; &quot;&quot;&quot; # Retrieve each parameter from the dictionary &quot;parameters&quot; W1 = parameters[&#39;W1&#39;] b1 = parameters[&#39;b1&#39;] W2 = parameters[&#39;W2&#39;] b2 = parameters[&#39;b2&#39;] ### END CODE HERE ### # Implement Forward Propagation Z1 = np.dot(W1,X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2,A1) + b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = {&quot;Z1&quot;: Z1, &quot;A1&quot;: A1, &quot;Z2&quot;: Z2, &quot;A2&quot;: A2} return A2, cache . Loss function . Now that you have computed $A^{[2]}$ (in the Python variable &quot;A2&quot;), which contains $a^{[2](i)}$ for every example, you can compute the cost function as follows: . $$ J = - frac{1}{m} sum limits_{i = 1}^{m} large{(} small y^{(i)} log left(a^{[2] (i)} right) + (1-y^{(i)}) log left(1- a^{[2] (i)} right) large{)} small tag{13} $$Exercise: Implement compute_cost() to compute the value of the cost $J$. . Instructions: . There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented $- sum limits_{i=0}^{m} y^{(i)} log(a^{[2](i)})$:logprobs = np.multiply(np.log(A2),Y) cost = - np.sum(logprobs) # no need to use a for loop! . | . (you can use either np.multiply() and then np.sum() or directly np.dot()). Note that if you use np.multiply followed by np.sum the end result will be a type float, whereas if you use np.dot, the result will be a 2D numpy array. We can use np.squeeze() to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array). We can cast the array as a type float using float(). . def compute_cost(A2, Y): &quot;&quot;&quot; Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- &quot;true&quot; labels vector of shape (1, number of examples) Returns: cost -- cross-entropy cost given equation (13) &quot;&quot;&quot; m = Y.shape[1] # number of example # Compute the cross-entropy cost logprobs = np.dot(Y,np.log(A2).T) + np.dot((1-Y),np.log((1-A2)).T) cost = -logprobs/m cost = float(np.squeeze(cost)) # makes sure cost is the dimension we expect. E.g., turns [[17]] into 17 assert(isinstance(cost, float)) return cost . Back-propogation . Using the cache computed during forward propagation, you can now implement backward propagation. . $ frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } = frac{1}{m} (a^{[2](i)} - y^{(i)})$ . $ frac{ partial mathcal{J} }{ partial W_2 } = frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } a^{[1] (i) T} $ . $ frac{ partial mathcal{J} }{ partial b_2 } = sum_i{ frac{ partial mathcal{J} }{ partial z_{2}^{(i)}}}$ . $ frac{ partial mathcal{J} }{ partial z_{1}^{(i)} } = W_2^T frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $ . $ frac{ partial mathcal{J} }{ partial W_1 } = frac{ partial mathcal{J} }{ partial z_{1}^{(i)} } X^T $ . $ frac{ partial mathcal{J} _i }{ partial b_1 } = sum_i{ frac{ partial mathcal{J} }{ partial z_{1}^{(i)}}}$ . Note that $*$ denotes elementwise multiplication. | The notation you will use is common in deep learning coding: . dW1 = $ frac{ partial mathcal{J} }{ partial W_1 }$ | db1 = $ frac{ partial mathcal{J} }{ partial b_1 }$ | dW2 = $ frac{ partial mathcal{J} }{ partial W_2 }$ | db2 = $ frac{ partial mathcal{J} }{ partial b_2 }$ | . | Tips: . To compute dZ1 you&#39;ll need to compute $g^{[1]&#39;}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]&#39;}(z) = 1-a^2$. So you can compute $g^{[1]&#39;}(Z^{[1]})$ using (1 - np.power(A1, 2)). | . | . def backward_propagation(parameters, cache, X, Y): &quot;&quot;&quot; Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;. X -- input data of shape (2, number of examples) Y -- &quot;true&quot; labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters &quot;&quot;&quot; m = X.shape[1] # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;. W1 = parameters[&#39;W1&#39;] W2 = parameters[&#39;W2&#39;] # Retrieve also A1 and A2 from dictionary &quot;cache&quot;. A1 = cache[&#39;A1&#39;] A2 = cache[&#39;A2&#39;] # Backward propagation: calculate dW1, db1, dW2, db2. dZ2 = A2 - Y dW2 = (1/m) * np.dot(dZ2,A1.T) db2 = (1/m) * np.sum(dZ2,axis=1, keepdims=True) dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = (1/m) * np.dot(dZ1, X.T) db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True) grads = {&quot;dW1&quot;: dW1, &quot;db1&quot;: db1, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2} return grads . General gradient descent rule: $$ theta = theta - alpha frac{ partial J }{ partial theta }$$ . where: $ alpha$ is the learning rate and $ theta$ represents a parameter. . Illustration: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley. . . def update_parameters(parameters, grads, learning_rate = 1.2): &quot;&quot;&quot; Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters &quot;&quot;&quot; # Retrieve each parameter from the dictionary &quot;parameters&quot; W1 = parameters[&#39;W1&#39;] b1 = parameters[&#39;b1&#39;] W2 = parameters[&#39;W2&#39;] b2 = parameters[&#39;b2&#39;] # Retrieve each gradient from the dictionary &quot;grads&quot; dW1 = grads[&#39;dW1&#39;] db1 = grads[&#39;db1&#39;] dW2 = grads[&#39;dW2&#39;] db2 = grads[&#39;db2&#39;] # Update rule for each parameter W1 = W1 - learning_rate*dW1 b1 = b1 - learning_rate*db1 W2 = W2 - learning_rate*dW2 b2 = b2 - learning_rate*db2 parameters = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2} return parameters . Integrate parts 4.1, 4.2 and 4.3 in nn_model() . def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): &quot;&quot;&quot; Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. &quot;&quot;&quot; np.random.seed(42) n_x, n_h, n_y = layer_sizes(X, Y, n_h=n_h) # Initialize parameters parameters = initialize_parameters(n_x, n_h, n_y) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;. A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;. cost = compute_cost(A2, Y) # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;. grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;. parameters = update_parameters(parameters, grads, learning_rate = 1.2) # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print (&quot;Cost after iteration %i: %f&quot; %(i, cost)) return parameters . 4.5 Predictions . Question: Use your model to predict by building predict(). Use forward propagation to predict results. . Reminder: predictions = $y_{prediction} = mathbb 1 text = begin{cases} 1 &amp; text{if} activation &gt; 0.5 0 &amp; text{otherwise} end{cases}$ . As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X &gt; threshold) . def predict(parameters, X): &quot;&quot;&quot; Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) &quot;&quot;&quot; # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. A2, cache = forward_propagation(X, parameters) threshold = 0.5 predictions = (A2 &gt; threshold) return predictions . It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units. . parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True) . Cost after iteration 0: 0.693141 Cost after iteration 1000: 0.052671 Cost after iteration 2000: 0.040765 Cost after iteration 3000: 0.032499 Cost after iteration 4000: 0.027457 Cost after iteration 5000: 0.023722 Cost after iteration 6000: 0.020082 Cost after iteration 7000: 0.016282 Cost after iteration 8000: 0.013001 Cost after iteration 9000: 0.010872 . def plot_decision_boundary_NN(func, x_input, y_input, ax=None): xx_1, xx_2 = np.mgrid[np.min(x_input[:,0]):np.max(x_input[:,0]):.01, np.min(x_input[:,1]):np.max(x_input[:,1]):.01] grid = np.c_[xx_1.ravel(), xx_2.ravel()].T y_pred_grid = func(grid).reshape(xx_1.shape) y_pred = func(x_input.T) if ax == None: fig, ax = plt.subplots(1,1, figsize=(10,10)) contour = ax.contourf(xx_1, xx_2, y_pred_grid, alpha=0.7, cmap=&quot;Spectral&quot;) ax.scatter(x_input[:,0], x_input[:, 1], c=y_pred, s=50, cmap=&quot;Spectral&quot;, edgecolor=&quot;white&quot;, linewidth=1) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.set(aspect=&#39;equal&#39;, xlim=(np.min(x_input[:,0]), np.max(x_input[:,0])), ylim=(np.min(x_input[:,1]),np.max(x_input[:,1])), xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;) return ax . plot_decision_boundary_NN(lambda x: predict(parameters, x), X.T, Y.T) plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4)) . Text(0.5, 1.0, &#39;Decision Boundary for hidden layer size 4&#39;) . predictions = predict(parameters, X) print (&#39;Accuracy: %d&#39; % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + &#39;%&#39;) . Accuracy: 99% . Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. . Now, let&#39;s try out several hidden layer sizes. . 4.6 - Tuning hidden layer size . Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes. . plt.figure(figsize=(16, 32)) hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50] for i, n_h in enumerate(hidden_layer_sizes): ax = plt.subplot(5, 2,i+1) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary_NN(lambda x: predict(parameters, x), X.T, Y.T, ax) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) ax.title.set_text(&#39;Hidden Layer of size {} | Accuracy: {}%&#39;.format(n_h, accuracy)) . The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. | The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticeable overfitting. | . Reference: . http://scs.ryerson.ca/~aharley/neural-networks/ | http://cs231n.github.io/neural-networks-case-study/ | .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/machine-learning/2021/04/25/_planar_data_classification_with_onehidden_layer_PUSHKAR_v6c.html",
            "relUrl": "/python/machine-learning/2021/04/25/_planar_data_classification_with_onehidden_layer_PUSHKAR_v6c.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Gradient descent flavors",
            "content": "Using scikit-learn for linear regression . Linear regression is probably one of the most important and widely used regression techniques. It‚Äôs among the simplest regression methods. One of its main advantages is the ease of interpreting results. . Two drawbacks of linear regression: . May overfit the data | Cannot model the non-linearity in the parameter space (It can model non-linear feature space but fails to show model correaltion in the dataset between features) | The main way linear regression fits data is by fitting weights for the linear variables ($x$ = given as independent variable) and estimate the dependent variable ($y$ = in this case) . begin{equation*} y = b_{0} + b_{1}x_{1} + b_{2}x_{2} + b_{3}x_{3} + .... + epsilon end{equation*}Mean squared errors . To get the best weights, you usually minimize the sum of squared residuals (SSR) or Mean Squared Errors (MSR) for all observations: begin{equation*} MSE = frac{1}{m} sum_{i=1}^{i=m}(y^{(i)}-ùê±^{(i)} theta)^2 end{equation*} This approach is called the method of ordinary least squares. Link: https://realpython.com/linear-regression-in-python/#implementing-linear-regression-in-python . Coefficient of determination . What % of total variation in y is described by the line or by the variation in $x$? . This is the coefficient of determination or $R-square$. . To answer this question we could look at what % of variation is NOT described by the line or by x -- that would be the $SE_{line}$ since that is THE error we get for the fitting. Therefore the remaining of that would be which is explained by the line . $$R_{square} = 1- frac{SE_{line}}{SE_{y}}$$ . If close to 1 then lot of the variation in y is described by variation in x . import numpy as np from sklearn.linear_model import LinearRegression import matplotlib.pyplot as plt import seaborn as sns sns.set_palette(&#39;deep&#39;) %config InlineBackend.figure_format = &#39;retina&#39; #To ensure we get similar results at each run -- if not initiated every successive will give more random #shuffled indices risking the possibility of the algo seeing the entire dataset! np.random.seed(42) . Solving for the parameters . To find the value that minimizes the cost function (MSE) we have following options: . Normal equation OR Single value decomposition (SVD) -- Analytical solution to the problem. Inverting of the larger features is a bottle neck as the dimension of the features increases. | Numerical solution by minimizing the cost function by moving along the gradient of the same. -- GRADIENT DESCENT. In this case we do not solve the equation EXACTLY but solve to an acceptable value given by the value of the LOSS FUNCTION or the gradient | 1. Normal equation . x = np.random.randn(200,1) #Normally distributed x around 3.0 y = 10.0 + 1.5 * x + np.random.randn(200,1) #Introducing some noise plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x7fb2c437e6d8&gt; . x_b = np.c_[np.ones((200,1)),x] #Here the x is 2dim array -- np.c_ column stacking weights = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y) print(&#39;Value of weights: {}, {}&#39;.format(weights[0][0],weights[1][0])) y_new = x_b.dot(weights) plt.plot(x_b[:,1],y_new,&#39;r-&#39;,label=&#39;normal_equation&#39;) plt.scatter(x,y,label=&#39;data&#39;) plt.legend() print(&#39;MSE:{}&#39;.format(np.sum((y_new-y)**2)/len(y_new))) #Coefficient of determination def sq_err_cal(data_1,data_2): return(np.sum((data_1-data_2)**2)) r2=1-sq_err_cal(y_new,y)/sq_err_cal(y,np.mean(y)) print(&#39;R-square: {0} n&#39;.format(round(r2,3))) . Value of weights: 10.089980744101755, 1.6008697534021734 MSE:0.9605306224045633 R-square: 0.697 . Using scikit-learn&#39;s Linear Regression module . from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score LinReg = LinearRegression() model = LinReg.fit(x,y) print(&#39;SCIKIT LEARN LINEAR REGRESSION&#39;) print(&#39;Value of weights: {}, {}&#39;.format(model.intercept_[0],model.coef_[0][0])) print(&#39;MSE:{}&#39;.format(mean_squared_error(model.predict(x),y))) print(&#39;R-square:{}&#39;.format(r2_score(y,model.predict(x)))) . SCIKIT LEARN LINEAR REGRESSION Value of weights: 10.089980744101753, 1.6008697534021723 MSE:0.9605306224045634 R-square:0.6970664852772347 . 2. Gradient Descent . To solve the minimization problem numerically. Useful when dimensionality of data is large and the number of instances make calculating the inverse/pseudoinverse difficult. . We minimize the gradient of MSE: begin{equation*} MSE = frac{1}{m} sum_{i=1}^{i=m}(y^{(i)}-ùê±^{(i)} theta)^2 end{equation*} . begin{equation*} nabla_{ theta}(MSE( theta,X)) = frac{-2}{m} sum_{i=1}^{i=m}(y^{(i)}-ùê±^{(i)} theta)(x^{(i)}) end{equation*}In vector form: begin{equation*} nabla_{ theta}(MSE( theta,X)) = frac{2}{m}X^{T}.(X theta-Y) end{equation*} . Batch Gradient Descent . def loss_func(X,Y,theta): return(np.mean((Y-X.dot(theta))**2)) #BATCH GRADIENT DESCENT def grad_batch(X,Y,theta_guess,eta=0.9,n_iter=1000,tol=1E-5): m = float(np.shape(X)[0]) for i in range(n_iter): gradient = 2/m * X.T.dot(X.dot(theta_guess)-Y) if np.max(abs(gradient)&lt;=tol): return(theta_guess) break else: theta_guess = theta_guess - eta * gradient return(theta_guess) . X = x_b.reshape(200,2) Y = y.reshape(200,1) theta_guess = np.random.randn(2,1) theta_opt = grad_batch(X,Y,theta_guess) print(&#39;BATCH GRADIENT DESCENT&#39;) print(&#39;Value of weights: {}, {}&#39;.format(theta_opt[0][0],theta_opt[1][0])) print(&#39;MSE: {}&#39;.format(loss_func(X,Y,theta_opt))) print(&#39;R-square:{}&#39;.format(r2_score(y,X.dot(theta_opt)))) . BATCH GRADIENT DESCENT Value of weights: 10.089998501410582, 1.600864834914306 MSE: 0.960530622747911 R-square:0.6970664851689492 . plt.scatter(X[:,1],Y,label=&#39;true data&#39;) plt.plot(X[:,1],X.dot(theta_opt),&#39;r-&#39;,linewidth=&#39;2.5&#39;,label=&#39;bath_gd&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fb2c7860710&gt; . def grad_batch_plots(X,Y,theta_guess,eta=0.9,n_iter=1000,tol=1E-5): m = float(np.shape(X)[0]) for i in range(n_iter): if i%5 == 0.0: plt.plot(X[:,1],X.dot(theta_guess),linewidth=&#39;2.5&#39;,linestyle=&#39;-&#39;,label=&#39;n={}&#39;.format(i)) gradient = 2/m * X.T.dot(X.dot(theta_guess)-Y) if np.max(abs(gradient)&lt;=tol): break else: theta_guess = theta_guess - eta * gradient fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(X[:,1],Y,label=&#39;true data&#39;) theta_guess = np.random.randn(2,1) grad_batch_plots(X,Y,theta_guess) ax.plot(X[:,1],X.dot(theta_opt),&#39;k-&#39;,linewidth=&#39;5.5&#39;,label=&#39;bath_gd&#39;) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) plt.legend(bbox_to_anchor=(1.05, 1), loc=&#39;upper left&#39;); . def path_grad_batch(X,Y,theta_guess,eta=0.1,n_iter=1000,tol=1E-5): m = float(np.shape(X)[0]) path_a = theta_guess[0] path_b = theta_guess[1] for i in range(n_iter): gradient = 2/m * X.T.dot(X.dot(theta_guess)-Y) if np.max(abs(gradient)&lt;=tol): return(path_a,path_b) break else: theta_guess = theta_guess - eta * gradient path_a = np.append(path_a,theta_guess[0][0]) path_b = np.append(path_b,theta_guess[1][0]) return(path_a,path_b) . theta_initial = np.array([[6],[5]]) path_a, path_b = path_grad_batch(X,Y,theta_initial,eta=0.4) #Contour plot of the loss function a = np.linspace(np.min(path_a)-1,np.max(path_a)+1,50) b = np.linspace(np.min(path_b)-1,np.max(path_b)+1,50) Z = np.zeros(shape=(len(a),len(b))) for i in range(len(a)): for j in range(len(b)): guess = np.array([[a[i]],[b[j]]]) Z[j,i] = loss_func(X,Y,guess) plt.figure(figsize=(10,10)) contours = plt.contour(a, b, Z, 10, colors=&#39;black&#39;) plt.clabel(contours, inline=True, fontsize=10) plt.imshow(Z, extent=[np.min(a),np.max(a), np.min(b),np.max(b)], origin=&#39;lower&#39;, cmap=&#39;RdGy&#39;, alpha=0.6) plt.plot(path_a,path_b,&#39;bo-&#39;,alpha=0.5) plt.scatter(theta_opt[0,0],theta_opt[1,0],s=100,c=&#39;red&#39;,edgecolor=&#39;black&#39;) . &lt;matplotlib.collections.PathCollection at 0x7fb2c5058f98&gt; . Stochastic and Mini-batch Gradient Descent . def loss_func(X,Y,theta): return(np.mean((Y-X.dot(theta))**2)) #STOCHASTIC GRADIENT DESCENT def stoch_mini_grad_descent(X,Y,theta_guess,batch_size=10,n_iter=1000,eta=0.1,tol=1E-5): m = float(np.shape(X)[0]) for i in range(n_iter): random_batch_indices = np.random.randint(m,size=batch_size) X_rand = X[random_batch_indices] Y_rand = Y[random_batch_indices] gradient = 2/np.shape(X_rand)[0] * X_rand.T.dot(X_rand.dot(theta_guess)-Y_rand) if np.max(abs(gradient)&lt;=tol): return(theta_guess) break else: theta_guess = theta_guess - eta * gradient return(theta_guess) X = x_b.reshape(200,2) Y = y.reshape(200,1) theta_guess = np.random.randn(2,1) theta_opt = stoch_mini_grad_descent(X,Y,theta_guess) print(&#39;BATCH GRADIENT DESCENT&#39;) print(&#39;Value of weights: {}, {}&#39;.format(theta_opt[0][0],theta_opt[1][0])) print(&#39;MSE: {}&#39;.format(loss_func(X,Y,theta_opt))) print(&#39;R-square:{}&#39;.format(r2_score(y,X.dot(theta_opt)))) . BATCH GRADIENT DESCENT Value of weights: 10.0012054989112, 1.5581287901525054 MSE: 0.9696807919920066 R-square:0.69418069177021 . def path_stoch_mini_grad_descent(X,Y,theta_guess,batch_size=1,n_iter=1000,eta=0.1,tol=1E-5): m = float(np.shape(X)[0]) path_a = theta_guess[0] path_b = theta_guess[1] for i in range(n_iter): random_batch_indices = np.random.randint(m,size=batch_size) X_rand = X[random_batch_indices] Y_rand = Y[random_batch_indices] gradient = 2/np.shape(X_rand)[0] * X_rand.T.dot(X_rand.dot(theta_guess)-Y_rand) if np.max(abs(gradient)&lt;=tol): return(path_a,path_b) break else: theta_guess = theta_guess - eta * gradient path_a = np.append(path_a,theta_guess[0][0]) path_b = np.append(path_b,theta_guess[1][0]) return(path_a,path_b) #List of thetas taken for optimization theta_initial = np.array([[6],[5]]) path_a, path_b = path_stoch_mini_grad_descent(X,Y,theta_initial,batch_size=10,eta=0.4) #Contour plot of the loss function a = np.linspace(np.min(path_a)-1,np.max(path_a)+1,50) b = np.linspace(np.min(path_b)-1,np.max(path_b)+1,50) Z = np.zeros(shape=(len(a),len(b))) for i in range(len(a)): for j in range(len(b)): guess = np.array([[a[i]],[b[j]]]) Z[j,i] = loss_func(X,Y,guess) fig, ax = plt.subplots(1,1, figsize=(10,10)) contours = ax.contour(a, b, Z, 10, colors=&#39;black&#39;) ax.clabel(contours, inline=True, fontsize=10) ax.imshow(Z, extent=[np.min(a),np.max(a), np.min(b),np.max(b)], origin=&#39;lower&#39;, cmap=&#39;RdGy&#39;, alpha=0.6) ax.plot(path_a, path_b,&#39;bo-&#39;,alpha=0.6, zorder=0) ax.scatter(theta_opt[0,0],theta_opt[1,0],s=100,c=&#39;red&#39;, edgecolor=&#39;black&#39;) . &lt;matplotlib.collections.PathCollection at 0x7fb2a9c85a20&gt; . Stochastic Gradient Descent + Nesterov . The intuition behind momentum is that the previous gradient evaluation(s) potentially give us good information about the direction of the minimum. The analogy comes from the physics of an object rolling down the hill. It will start slowly, but if it is consistently downhill in the same direction, then the object ‚Äúaccelerates‚Äù toward the minimum. The change in algorithm is tiny but powerful . An even better variant developed by Nesterov calculates the gradient at $x_{i} + beta m_{i}$ after applying the momentum displacement. That is, on step i+1 we know the update from momentum alone ($ beta m_{i}$) before we calculate the gradient ‚Äì so why not calculate the gradient at that point, $x_{i} + beta m_{i}$, since it is probably closer to $x_{i+1}$ than $x_{i}$ is. . if $ beta = 0$ -- its normal mini-batch stochastic gradient descent . def loss_func(X,Y,theta): return(np.mean((Y-X.dot(theta))**2)) #STOCHASTIC GRADIENT DESCENT def nestrov_batch_grad_descent(X,Y,theta_guess,beta=0.9,batch_size=10,n_iter=1000,eta=0.1,tol=1E-5): feature_size = float(np.shape(X)[0]) m = np.zeros(shape=(np.shape(theta_guess)[0],1)) for i in range(n_iter): random_batch_indices = np.random.randint(feature_size,size=batch_size) X_rand = X[random_batch_indices] theta_guess = theta_guess + beta * m Y_rand = Y[random_batch_indices] gradient = 2/np.shape(X_rand)[0] * X_rand.T.dot(X_rand.dot(theta_guess)-Y_rand) if np.max(abs(gradient)&lt;=tol): return(theta_guess) break else: m = beta*m - eta * gradient theta_guess = theta_guess + m return(theta_guess) X = x_b.reshape(200,2) Y = y.reshape(200,1) theta_guess = np.random.randn(2,1) theta_opt = nestrov_batch_grad_descent(X,Y,theta_guess) print(&#39;BATCH GRADIENT DESCENT&#39;) print(&#39;Value of weights: {}, {}&#39;.format(theta_opt[0][0],theta_opt[1][0])) print(&#39;MSE: {}&#39;.format(loss_func(X,Y,theta_opt))) print(&#39;R-square:{}&#39;.format(r2_score(y,X.dot(theta_opt)))) . BATCH GRADIENT DESCENT Value of weights: 10.703607136268275, 1.915562159550262 MSE: 1.4068945646512006 R-square:0.5562915899055692 . def nestrov_batch_grad_descent(X,Y,theta_guess,beta=0.9,batch_size=10,n_iter=1000,eta=0.1,tol=1E-5): feature_size = float(np.shape(X)[0]) path_a = theta_guess[0] path_b = theta_guess[1] m = np.zeros(shape=(np.shape(theta_guess)[0],1)) for i in range(n_iter): random_batch_indices = np.random.randint(feature_size,size=batch_size) X_rand = X[random_batch_indices] theta_guess = theta_guess + beta * m Y_rand = Y[random_batch_indices] gradient = 2/np.shape(X_rand)[0] * X_rand.T.dot(X_rand.dot(theta_guess)-Y_rand) if np.max(abs(gradient)&lt;=tol): return(path_a,path_b) break else: m = beta*m - eta * gradient theta_guess = theta_guess + m path_a = np.append(path_a,theta_guess[0][0]) path_b = np.append(path_b,theta_guess[1][0]) return(path_a,path_b) #List of thetas taken for optimization theta_initial = np.array([[6],[5]]) path_a, path_b = nestrov_batch_grad_descent(X,Y,theta_initial,beta=0.9,batch_size=10,eta=0.4) #Contour plot of the loss function a = np.linspace(np.min(path_a)-1,np.max(path_a)+1,50) b = np.linspace(np.min(path_b)-1,np.max(path_b)+1,50) Z = np.zeros(shape=(len(a),len(b))) for i in range(len(a)): for j in range(len(b)): guess = np.array([[a[i]],[b[j]]]) Z[j,i] = loss_func(X,Y,guess) fig, ax = plt.subplots(1,1, figsize=(10,10)) contours = ax.contour(a, b, Z, 10, colors=&#39;black&#39;) #ax.clabel(contours, inline=True, fontsize=10) ax.imshow(Z, extent=[np.min(a),np.max(a), np.min(b),np.max(b)], origin=&#39;lower&#39;, cmap=&#39;RdGy&#39;, alpha=0.6) ax.plot(path_a,path_b,&#39;bo-&#39;,alpha=0.5, zorder=0) ax.scatter(theta_opt[0,0],theta_opt[1,0],s=100,c=&#39;red&#39;, edgecolor=&#39;black&#39;); .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/machine-learning/2021/04/25/_gradient_descent_flavors.html",
            "relUrl": "/python/machine-learning/2021/04/25/_gradient_descent_flavors.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Plotting surface in matplotlib",
            "content": "This is adapted from the following Tutorial: Link . import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; . fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) x = np.array([[1, 3], [2, 4]]) #Array format: [[a,b],[c,d]] -- a b are in row; c d are in row y = np.array([[5, 6], [7, 8]]) z = np.array([[9, 12], [10, 11]]) ax.plot_surface(x, y, z) ax.set(xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, zlabel=&#39;z&#39;) fig.tight_layout() . Meshgrid . Mesh is important to create a surface since just looking at the x, y vector by themselves what you would look at is the diagonal of the matrix formed by combination of all the possible x values with y values. For the given x and y vector, every entry in x vector can have the entire y vector as a possible point. So it is important to generate an array which captures all these possible pairing. . So using mesh-grid if x-vector is of dimensions M and y-vector is of dimensions N -- the final resulting matrix is NxM dimensions where every $n^{th}$ entry in y all the entries of x are added. Finally the ouput is given as x coordinate of that matrix and y coordinate of that matrix. . Example: . $X$ : $ begin{bmatrix} x_{1} &amp; x_{2} &amp; x_{3} end{bmatrix}$ | $Y$ : $ begin{bmatrix} y_{1} &amp; y_{2} end{bmatrix}$ | . Then resulting mesh would be: $$ X-Y-Mesh = begin{bmatrix} x_{1}y_{1} &amp; x_{2}y_{1} &amp; x_{3}y_{1} x_{1}y_{2} &amp; x_{2}y_{2} &amp; x_{3}y_{2} end{bmatrix}$$ . $$ X-path = begin{bmatrix} x_{1} &amp; x_{2} &amp; x_{3} x_{1} &amp; x_{2} &amp; x_{3} end{bmatrix}$$ . $$ X-path = begin{bmatrix} y_{1} &amp; y_{1} &amp; y_{1} y_{2} &amp; y_{2} &amp; y_{2} end{bmatrix}$$ . x_axis_range = np.arange(-2,2.1,1) y_axis_range = np.arange(-4,4.1,1) #Make the meshgrid for the x and y (x,y) = np.meshgrid(x_axis_range, y_axis_range, sparse=True) . z = x + y . fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) ax.plot_surface(x, y, z) fig.tight_layout() . Plotting this 2D function: $$ z = e^{- sqrt {x^2 + y^2}}cos(4x)cos(4y) $$ using the surface . import matplotlib.cm as cm x_axis_bound = np.linspace(-1.8,1.8,100) y_axis_bound = np.linspace(-1.8,1.8,100) (x,y) = np.meshgrid(x_axis_bound, y_axis_bound, sparse=True) def f(x,y): return np.exp(-np.sqrt( x**2 + y**2 )) * np.cos(4*x) * np.cos(4*y) Z = f(x,y) fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) ax.plot_surface(x, y, Z, cmap=cm.hot) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) fig.tight_layout() .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/plotting/machine-learning/2021/04/25/_creating_meshes.html",
            "relUrl": "/python/plotting/machine-learning/2021/04/25/_creating_meshes.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Vectorisation in python using numpy",
            "content": "import numpy as np import time a = np.random.randint(10E6,size=(50,1000)) print(np.shape(a)) w = np.random.randint(100,size=(50,1)) print(np.shape(w)) . (50, 1000) (50, 1) . t_start = time.time() z = np.dot(w.T,a).T t_stop = time.time() print(&#39;Time take: {} ms&#39;.format(1000*(t_stop-t_start))) #Non vectorized version z_for = [] t_start = time.time() for j in range(np.shape(a)[1]): _count = 0.0 for i in range(np.shape(a)[0]): _count+=w[i,0]*a[i,j] z_for.append(_count) t_stop = time.time() print(&#39;Time take for for-loop: {} ms&#39;.format(1000*(t_stop-t_start))) #Check the output print(&#39;Check sum: {}&#39;.format(np.sum(np.asarray(z_for).reshape(np.shape(z))-z))) . Time take: 0.3979206085205078 ms Time take for for-loop: 33.74624252319336 ms Check sum: 0.0 . #If I want to have expoenential of different values in the array a = np.random.randint(10,size=(10,2)) #With for loops: import math exp_a = np.zeros(np.shape(a)) for j in range(np.shape(a)[1]): for i in range(np.shape(a)[0]): exp_a[i,j] = math.exp(a[i,j]) . exp_a_numpy = np.exp(a) #Vector already setup -- element-wise exponential #Other vectorized functions: # np.log(x) # np.abs(x) # np.maximum(x,0) -- computes element-wise maximum comparing to 0 # x**2 for numpy array # 1/x for numpy array . exp_a_numpy - exp_a . array([[0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.]]) . food_cal = np.array([[56.0,0.0,4.4,68.0], [1.2, 104, 52, 8.], [1.8, 135.,99., 0.9]]) . carb = np.array([food_cal[0,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) protein = np.array([food_cal[1,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) fat = np.array([food_cal[2,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) cal = np.array([carb,protein,fat]) print(cal) . [[94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] . cal = food_cal.sum(axis=0) #AXIS = 0 is sum vertically -- along column #AXIS = 1 is sum horizontally -- along row print(cal) . [ 59. 239. 155.4 76.9] . #Here the cal is BROADCASTING from 1,4 to 4,4 percentage = 100*food_cal/cal.reshape(1,4) print(percentage) . [[94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] . #Example 1 A = np.linspace(1,5,5) print(A.shape) B = A+10. print(A, B, B.shape) # Here 10. was broadcasted into 5x1 vector . (5,) [1. 2. 3. 4. 5.] [11. 12. 13. 14. 15.] (5,) . A = np.array([[1,2,3], [4,5,6]]) print(A.shape) B = np.array([100,200,300]) print(B.shape) C = A + B print(C.shape) print(A,B) print(C) # Here B was broadcasted from (3,) to 2x3! . (2, 3) (3,) (2, 3) [[1 2 3] [4 5 6]] [100 200 300] [[101 202 303] [104 205 306]] . General principle . (m,n) matrix with (+, -, *, /) with (1,n) or (m,1) lead of copying it to (m,n) before conducting computing. . Good practices and tips . import numpy as np a = np.random.randn(5) print(a) . [ 0.68281763 -1.3579685 0.99577659 0.31269709 0.595569 ] . print(a.shape) . (5,) . Here a is a array of rank 1. It is neither a row or a column vector. So this has some non-intuitive effects . print(a.T) . [ 0.68281763 -1.3579685 0.99577659 0.31269709 0.595569 ] . print(np.dot(a,a.T)) . 3.7543713020122427 . So it is recommended for consistency to NOT use data-structures have rank 1 like the one above but instead instantiate the array as the fixed array of known size . ALWAYS COMMIT TO MAKING DEFINED ROW AND COLUMN VECTORS . a1 = np.random.randn(5,1) print(a1) print(a1.shape) . [[-0.7474656 ] [-0.75790159] [ 0.30984002] [ 0.18874051] [-0.80470167]] (5, 1) . print(a1.T) . [[-0.7474656 -0.75790159 0.30984002 0.18874051 -0.80470167]] . Here there are two Square Brackets compared to the previous transport of a suggesting in the case of a1 it is well-defined 1x5 row vector . print(np.dot(a1,a1.T)) #Outer product . [[ 0.55870482 0.56650536 -0.23159476 -0.14107704 0.60148682] [ 0.56650536 0.57441482 -0.23482825 -0.14304673 0.60988468] [-0.23159476 -0.23482825 0.09600084 0.05847936 -0.24932878] [-0.14107704 -0.14304673 0.05847936 0.03562298 -0.1518798 ] [ 0.60148682 0.60988468 -0.24932878 -0.1518798 0.64754478]] . assert(a1.shape==(5,1)) #Assertion statement to check the known size a = a.reshape((5,1)) print(a.shape) . (5, 1) . A = np.random.randn(4,3) . print(A) . [[ 0.22469294 0.78832742 -1.13148285] [-0.04070683 -0.74061401 -1.59838506] [ 0.12821164 0.72892812 0.4912876 ] [ 0.09323584 1.66090848 1.87905216]] . np.sum(A,axis=1,keepdims=True).shape . (4, 1) .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/machine-learning/2021/04/25/_Vectorisation_and_TF_example.html",
            "relUrl": "/python/machine-learning/2021/04/25/_Vectorisation_and_TF_example.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Implement Support Vector Machines in scikit-learn",
            "content": "This tutorial is borrowed from Jake VanderPlas&#39;s example of SVM in his notebook: Python Data Science Handbook . Motivation for Support Vector Machines . We want to find a line/curve (in 2D) or a manifold (in n-D) that divides the class from each other. This is a type of Discriminative Classification | Consider a simple case of classification task, in which the two classes of points are well separated | . %matplotlib inline import numpy as np import matplotlib.pyplot as plt from scipy import stats random_state = 42 #Using seaborn plotting defaults import seaborn as sns; sns.set() . from sklearn.datasets.samples_generator import make_blobs X, y = make_blobs(n_samples=200, centers=2, random_state=random_state, cluster_std=1.5) #Check what is X and y -# print(&#39;X is a {} array with x-y coordinates of the cluster points n&#39;.format(np.shape(X))) print(X[:3]) print(&#39; n&#39;) print(&#39;y is a {} array with a classification of the points to which cluster they belong to n&#39;.format(np.shape(y))) print(y[:3]) print(&#39; n&#39;) plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) . X is a (200, 2) array with x-y coordinates of the cluster points [[2.24823735 1.07410715] [5.12395668 0.73232327] [4.6766441 2.72016712]] y is a (200,) array with a classification of the points to which cluster they belong to [1 1 1] . &lt;matplotlib.collections.PathCollection at 0x121c4f710&gt; . A linear discriminative classifier would attempt to draw a straight line separating the two data-sets and thereby creating a model for classification. For the 2D data like the shown above, this task could be done by hand. But there is more than one line that can divide this data in two halves! . x_fit = np.linspace(min(X[:,0]),max(X[:,0])) plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) plt.plot([-2],[4],&#39;x&#39;,color=&#39;blue&#39;, markeredgewidth=2, markersize=10) for m,b in [(3.5,5),(2,5),(0.9,5)]: plt.plot(x_fit, x_fit*m+b, &#39;-k&#39;) plt.xlim(min(X[:,0]),max(X[:,0])) plt.ylim(min(X[:,1]),max(X[:,1])) . (-0.9549620153430207, 13.094539878082749) . What&#39;s a better methodology to determine the cutting plane? Something like k-nearest neighbor clustering wherein you find the plane with best separation from the two clusters based on some distance metric. However, k-nearest neighbors is based on non-parametric method of finding the cluster and classifying the new data-point. What if I want something which is Learned is used as a function every other time . Support Vector Machines: . Rather than simply drawing a zero-width line between classes, we can draw round each line a margin of some width, up to the nearest point. . x_fit = np.linspace(min(X[:,0]),max(X[:,0])) plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) plt.plot([-2],[4],&#39;x&#39;,color=&#39;blue&#39;, markeredgewidth=2, markersize=10) for m, b, d in [(3.5,5,0.33),(2,5,0.55),(0.9,5,0.8)]: y_fit = x_fit*m + b plt.plot(x_fit, y_fit, &#39;-k&#39;) plt.fill_between(x_fit, y_fit-d, y_fit+d, edgecolor=&#39;none&#39;, color=&#39;#AAAAAA&#39;, alpha=0.4) plt.xlim(min(X[:,0]),max(X[:,0])) plt.ylim(min(X[:,1]),max(X[:,1])) . (-0.9549620153430207, 13.094539878082749) . In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator. . Fitting a support vector machine . Using Scikit-learn&#39;s SVM module to train a classifier on the above data. We will use a linear-kernel and set C parameters to a very large value. . from sklearn.svm import SVC model = SVC(kernel=&#39;linear&#39;,C=1E10) model.fit(X,y) . SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . To better visualize the SVM classification, use a convenience function to visualize the decision boundary as made by the SVM module. Code is adopted from Jake&#39;s tutorial. . def plot_svc_decision_function(model, ax=None, plot_support=True, list_vectors=False): &quot;&quot;&quot;Plot the decision function for a 2D SVC&quot;&quot;&quot; if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() # create grid to evaluate model x = np.linspace(xlim[0], xlim[1], 30) y = np.linspace(ylim[0], ylim[1], 30) Y, X = np.meshgrid(y, x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) # plot decision boundary and margins ax.contour(X, Y, P, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) # plot support vectors if plot_support: ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;,linestyle=&#39;--&#39;); if list_vectors: print(model.support_vectors_) ax.set_xlim(xlim) ax.set_ylim(ylim) . plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(model) . This is the dividing line that maximizes the margin between the two sets of points. . Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure. . These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. . In Scikit-Learn, the identity of these points are stored in the supportvectors attribute of the classifier. . model.support_vectors_ . array([[-0.40500616, 6.91150953], [ 2.65952903, 4.72035783], [ 2.07017704, 4.00397825]]) . A key to this classifier&#39;s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! . Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin. . We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset: . def plot_svm(N=10, ax=None): X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.60) X = X[:N] y = y[:N] model = SVC(kernel=&#39;linear&#39;, C=1E10) model.fit(X, y) ax = ax or plt.gca() ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plot_svc_decision_function(model, ax) fig, ax = plt.subplots(1, 2, figsize=(16, 6)) fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1) for axi, N in zip(ax, [60, 120]): plot_svm(N, axi) axi.set_title(&#39;N = {0}&#39;.format(N)) . In spite of increasing the training points, once the margins and the corresponding support vectors are identified the model does not change. This is one of the strengths of this algorithm -- the insensitivity to the exact behavior of the distant point is one of the strengths of SVM model . from ipywidgets import interact, fixed interact(plot_svm, N=[10, 50, 100, 150, 200], ax=fixed(None)); . Beyond linear kernels: Kernel SVM . Kernels are helpful in projecting data into higher dimensional feature space. This can be useful in simplest case to fit non-linear data using linear regression models. Similarly in the case of SVM: Projecting the data into higher dimensions through either polynomial or gaussian kernels we can fit non-linear relationships to a linear classifier . Let&#39;s look at a data-set which is not linearly separated: . from sklearn.datasets.samples_generator import make_circles X, y = make_circles(200, factor=0.1, noise=0.1) clf = SVC(kernel=&#39;linear&#39;).fit(X,y) plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(clf, plot_support=False) . There is not straight forward way to separate this data however we can project the data into higher dimensions based on its properties in the current dimensional space and get more information about its spread. One way of doing so is computing a radial basis function centered at the middle lump . r = np.exp(-(np.sum((X)**2,axis=1))) from mpl_toolkits import mplot3d def plot_3D(elev=30, azim=30, X=X, y=y): ax = plt.subplot(projection=&#39;3d&#39;) ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap=&#39;autumn&#39;) ax.view_init(elev=elev, azim=azim) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_zlabel(&#39;r&#39;) interact(plot_3D, elev=[-90, -45, -30, 30, 45, 60, 90], azip=(-180, 180), X=fixed(X), y=fixed(y)); . Projecting the data in an additonal dimensions we can see can having a plane at r=0.7 could give us good separation. . Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. . In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use. . One strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a kernel transformation, as it is based on a similarity relationship (or kernel) between each pair of points. . A potential problem with this strategy‚Äîprojecting N points into N dimensions‚Äîis that it might become very computationally intensive as N grows large. However, because of a neat little procedure known as the kernel trick, a fit on kernel-transformed data can be done implicitly‚Äîthat is, without ever building the full N-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful. . In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the kernel model hyperparameter: . clf = SVC(kernel=&#39;rbf&#39;, C=1E6) clf.fit(X, y) . /Users/pghaneka/miniconda3/envs/py37/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from &#39;auto&#39; to &#39;scale&#39; in version 0.22 to account better for unscaled features. Set gamma explicitly to &#39;auto&#39; or &#39;scale&#39; to avoid this warning. &#34;avoid this warning.&#34;, FutureWarning) . SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) . plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(clf) plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=300, lw=1, facecolors=&#39;none&#39;); . Softer margins . X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.2) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;); . To handle this case, the SVM implementation has a bit of a fudge-factor which &quot;softens&quot; the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as C. . For very large C, the margin is hard, and points cannot lie in it. For smaller C, the margin is softer, and can grow to encompass some points. . The plot shown below gives a visual picture of how a changing C parameter affects the final fit, via the softening of the margin: . X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8) fig, ax = plt.subplots(1, 3, figsize=(16, 6)) for axi, C in zip(ax, [1E10, 10.0, 0.1]): model = SVC(kernel=&#39;linear&#39;, C=C).fit(X, y) axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(model, axi) axi.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;,linestyle=&#39;--&#39;) axi.set_title(&#39;C = {0:.1f}&#39;.format(C), size=14) . Example: Facial Recognition . As an example of support vector machines in action, let&#39;s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn: . from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people(min_faces_per_person=60) print(faces.target_names) print(faces.images.shape) . [&#39;Ariel Sharon&#39; &#39;Colin Powell&#39; &#39;Donald Rumsfeld&#39; &#39;George W Bush&#39; &#39;Gerhard Schroeder&#39; &#39;Hugo Chavez&#39; &#39;Junichiro Koizumi&#39; &#39;Tony Blair&#39;] (1348, 62, 47) . fig, ax = plt.subplots(3,5,figsize=(20,20)) for i,axi in enumerate(ax.flat): axi.imshow(faces.images[i],cmap=&#39;bone&#39;) axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]]) . Each image contains [62√ó47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis to extract 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline: . from sklearn.svm import SVC from sklearn.decomposition import PCA from sklearn.pipeline import make_pipeline pca = PCA(n_components=150, whiten=True, svd_solver=&#39;randomized&#39;, random_state=0) svc = SVC(kernel=&#39;rbf&#39;, class_weight=&#39;balanced&#39;) model = make_pipeline(pca,svc) . from sklearn.model_selection import train_test_split, GridSearchCV Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=0) . Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model: . param_grid = {&#39;svc__C&#39;: [0.001, 0.1, 1, 5, 10, 50], &#39;svc__gamma&#39;: [0.0001, 0.0005, 0.001, 0.005]} grid = GridSearchCV(model, param_grid, cv=5) %time grid.fit(Xtrain, ytrain) print(grid.best_params_) . /Users/pghaneka/miniconda3/envs/py37/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) . CPU times: user 3min 36s, sys: 15.1 s, total: 3min 51s Wall time: 48.4 s {&#39;svc__C&#39;: 10, &#39;svc__gamma&#39;: 0.005} . model = grid.best_estimator_ yfit = model.predict(Xtest) . fig, ax = plt.subplots(4, 6,figsize=(20,20)) for i, axi in enumerate(ax.flat): axi.imshow(Xtest[i].reshape(62, 47), cmap=&#39;bone&#39;) axi.set(xticks=[], yticks=[]) axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color=&#39;black&#39; if yfit[i] == ytest[i] else &#39;red&#39;) fig.suptitle(&#39;Predicted Names; Incorrect Labels in Red&#39;, size=14); . from sklearn.metrics import classification_report print(classification_report(ytest, yfit, target_names=faces.target_names)) . precision recall f1-score support Ariel Sharon 1.00 0.69 0.81 16 Colin Powell 0.85 0.87 0.86 61 Donald Rumsfeld 0.75 0.69 0.72 35 George W Bush 0.78 0.97 0.86 125 Gerhard Schroeder 0.86 0.66 0.75 29 Hugo Chavez 1.00 0.68 0.81 19 Junichiro Koizumi 1.00 0.76 0.87 17 Tony Blair 0.93 0.77 0.84 35 accuracy 0.83 337 macro avg 0.90 0.76 0.82 337 weighted avg 0.85 0.83 0.83 337 . from sklearn.metrics import confusion_matrix mat = confusion_matrix(ytest, yfit) sns.heatmap(mat.T, square=True, annot=True, fmt=&#39;d&#39;, cbar=False, xticklabels=faces.target_names, yticklabels=faces.target_names) plt.xlabel(&#39;true label&#39;) plt.ylabel(&#39;predicted label&#39;); .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/machine-learning/2021/04/25/_SVM_example.html",
            "relUrl": "/python/machine-learning/2021/04/25/_SVM_example.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Lambda, Filter, and Map functions in Python",
            "content": "Lambda . Lambda is an important function/operator to create anonymous in-line functions in pyhton. . Basic syntax: . lamda arguments: expression . def func(x,y): return(x+y) func(2,3) . 5 . add=lambda x,y:x+y add(2,3) . 5 . lambda functions can be used in place of a iterator when sorting -- this allows for selecting the column or the varible according to which sorting has to be done . import numpy as np np.random.seed(42) a1=np.random.choice(10,5,replace=False) file_list=[] for i in a1: file_list.append(&#39;{0}-{1}&#39;.format(&#39;Filename&#39;,i)) . print(file_list) . [&#39;Filename-8&#39;, &#39;Filename-1&#39;, &#39;Filename-5&#39;, &#39;Filename-0&#39;, &#39;Filename-7&#39;] . file_list=sorted(file_list, key=lambda x:x.split(&#39;-&#39;)[-1]) . file_list . [&#39;Filename-0&#39;, &#39;Filename-1&#39;, &#39;Filename-5&#39;, &#39;Filename-7&#39;, &#39;Filename-8&#39;] . Map . When you want to have multiple outputs for the functions but do not want to write a for all explicitly you can use map function for pseeding things up . def square(x): return(x**2) print(a1) ans=[square(i) for i in a1] print(ans) . [8 1 5 0 7] [64, 1, 25, 0, 49] . map(square,a1) . &lt;map at 0x7f85058e5eb8&gt; . list(map(square,a1)) . [64, 1, 25, 0, 49] . Combining the two: . a=np.random.choice(10,5,replace=False) b=np.random.choice(50,5,replace=False) result=map(lambda x,y:x*y,a,b) print(a) print(b) print(np.asarray(list(result))) . [0 1 8 5 3] [36 16 4 9 45] [ 0 16 32 45 135] .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/2021/04/25/_Lambda-Map.html",
            "relUrl": "/python/2021/04/25/_Lambda-Map.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "MNIST digit classification using scikit-learn",
            "content": "This notebook is adopted from Aur√©lien Geron&#39;s hands-on machine learning tutorial . from __future__ import division, print_function, unicode_literals # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) . MNIST dataset . def sort_by_target(mnist): reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1] reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1] mnist.data[:60000] = mnist.data[reorder_train] mnist.target[:60000] = mnist.target[reorder_train] mnist.data[60000:] = mnist.data[reorder_test + 60000] mnist.target[60000:] = mnist.target[reorder_test + 60000] try: from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings sort_by_target(mnist) # fetch_openml() returns an unsorted dataset except ImportError: from sklearn.datasets import fetch_mldata mnist = fetch_mldata(&#39;MNIST original&#39;) . KeyError Traceback (most recent call last) &lt;ipython-input-3-273964c01de6&gt; in &lt;module&gt; 11 mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) 12 mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings &gt; 13 sort_by_target(mnist) # fetch_openml() returns an unsorted dataset 14 except ImportError: 15 from sklearn.datasets import fetch_mldata &lt;ipython-input-3-273964c01de6&gt; in sort_by_target(mnist) 2 reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1] 3 reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1] -&gt; 4 mnist.data[:60000] = mnist.data[reorder_train] 5 mnist.target[:60000] = mnist.target[reorder_train] 6 mnist.data[60000:] = mnist.data[reorder_test + 60000] ~/miniconda3/envs/ace_gcn/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key) 2910 if is_iterator(key): 2911 key = list(key) -&gt; 2912 indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1] 2913 2914 # take() does not accept boolean indexers ~/miniconda3/envs/ace_gcn/lib/python3.6/site-packages/pandas/core/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing) 1252 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr) 1253 -&gt; 1254 self._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing) 1255 return keyarr, indexer 1256 ~/miniconda3/envs/ace_gcn/lib/python3.6/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing) 1296 if missing == len(indexer): 1297 axis_name = self.obj._get_axis_name(axis) -&gt; 1298 raise KeyError(f&#34;None of [{key}] are in the [{axis_name}]&#34;) 1299 1300 # We (temporarily) allow for some missing keys with .loc, except in KeyError: &#34;None of [Int64Index([ 1, 21, 34, 37, 51, 56, 63, 68, 69, n 75, n ... n 59910, 59917, 59927, 59939, 59942, 59948, 59969, 59973, 59990, n 59992], n dtype=&#39;int64&#39;, length=60000)] are in the [columns]&#34; . mnist.data.shape . 70,000 small images of hand-written numbers. Each image has 784 features. Those features are split in 28x28 pixels and each feature is simply that pixel gray-scale intensity. Value for each pixel ranges from 0 to 255. . X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;] . random_digit=X[62123] print(&#39;The {0} entry is a photo of {1}&#39;.format(62123,y[62123])) random_digit_image=random_digit.reshape(28,28) plt.imshow(random_digit_image, cmap=mpl.cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . The 62123 entry is a photo of 2 . (-0.5, 27.5, 27.5, -0.5) . def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] . Before training we shuffle the data to ensure all cross-validation folds to be similar. Moreover some classficiation algorithms are sensitive to the order of training instances, and they perform poorly if they get many similar instances in a row. . import numpy as np index_shuffle = np.random.permutation(60000) X_train, y_train = X_train[index_shuffle], y_train[index_shuffle] . Binary classification . Here we will build a single digit classifier -- for example looking at just 2. Hence in total there will be only 2 classes -- Those which are 2 and those which are not. . y_train_2 = (y_train == 2) #True for all 2s, False for all other digits y_test_2 = (y_test == 2) . Using Stochastic Gradient Descent classifier. Known to handle large datasets very well. . from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42) sgd_clf.fit(X_train, y_train_2) . SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=5, n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;, power_t=0.5, random_state=42, shuffle=True, tol=-inf, validation_fraction=0.1, verbose=0, warm_start=False) . sgd_clf.predict([random_digit]) . array([ True]) . Performance metrics . Evaluating classifiers is often significantly challenging than the case for a regressor wherein we can use RMSE or MAE. Let&#39;s look at some usual metrics used to gauge the classifier performance. . 1. Accuracy using Cross-validation . It involves splitting your training data in K-folds. Training the model on K-1 folds and testing it on the left out fold. Scikit learn has in-built method to do so: cross_val_score(). We can implement our own version as well. . from sklearn.model_selection import StratifiedKFold from sklearn.base import clone skfolds = StratifiedKFold(n_splits=3, random_state=42) for train_index, test_index in skfolds.split(X_train,y_train_2): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_2[train_index] X_test_folds = X_train[test_index] y_test_folds = y_train_2[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred=clone_clf.predict(X_test_folds) n_correct = sum(y_pred == y_test_folds) print(n_correct/len(y_pred)) . 0.96675 0.97025 0.97345 . from sklearn.model_selection import cross_val_score cross_val_score(sgd_clf, X_train, y_train_2, cv=3, scoring=&#39;accuracy&#39;) . array([0.96675, 0.97025, 0.97345]) . Does this high accuracy let us anything? Is the sample space we are looking at uniform enough for this accuracy? Maybe we have way less one-digit samples for training in the first place. . _count=0. for i in range(len(y_train)): if y_train[i] == 2: _count=_count+1. print(_count/len(y_train)*100) . 9.93 . So ~9% of the sample are actually 2. So even if we guess ALWAYS that image is not 2 we will be right 90% of the time! . The dumb classifier . To check whether classifier accuracy of ~95% is good enough so just a over-exagerration . from sklearn.base import BaseEstimator class Never2(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return(np.zeros((len(X),1),dtype=bool)) . never2 = Never2() cross_val_score(never2,X_train,y_train_2,cv=3,scoring=&#39;accuracy&#39;) . array([0.9017, 0.9001, 0.9003]) . This shows our data is skewed! . 2. Confusion Matrix . General idea is to count the number of times instances of Class A are classified as Class B. . Table that describes the performance of a classification model by grouping predictions into 4 categories. . True Positives: we¬†correctly¬†predicted they do have diabetes | True Negatives: we¬†correctly¬†predicted they don‚Äôt have diabetes | False Positives: we¬†incorrectly¬†predicted they do have diabetes (Type I error) | False Negatives: we¬†incorrectly¬†predicted they don‚Äôt have diabetes (Type II error) | . The ROWS in the matrix are the real class-labels i.e. the TRUTH values while COLUMNS are the predicted values. . from sklearn.model_selection import cross_val_predict y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3) . from sklearn.metrics import confusion_matrix confusion_matrix(y_train_2, y_train_pred) . array([[53277, 765], [ 1026, 4932]]) . Each row in the confusion matrix represent actual class, while each column represents a predicted class. Following are the terms of the confusion matrix: . First row of this matrix is the non-2 images -- 53277 instances were correctly classified as non 2 (True Negative) | 765 instances were wrongly classified as 2s (False Positive) | . | Second row considers the images of 5 -- 1026 instances were wrongly classified as non 2s (False negatives) | 4932 instances were correctly classified as 2s (True positives) | . | An ideal classifier would be a diagonal matrix with no false positives or false negatives . Precision . It is the ratio of the total classification whether as True or Wrongly classified as True to True. That is, TP/(TP+FP) . This is looking at +ve classification and how many are really +ve and how many are wrongly shown as +ve. So Precision looks at the prediction of +ve results. . Recall . It is the ratio of total classification on the +ve samples from where they are classified correctly (TP) to wrongly classified as negative (FN). TP/(FN+TP) . So Recall looks at the prediction of the +ve samples. . This is by just comparing the +ve samples in the binary classification. To check how many of them are correctly recalled as +ve. . F1 score . Harmonic mean of recall and precision. Higher the Precision and Recall, lower are the instances of FP and FN. So we want to have higher Recall and Precision both. . F1 favors classifiers with similar recall and precision. . from sklearn.metrics import precision_score, recall_score, f1_score print(&#39;Precision score: {}&#39;.format(precision_score(y_train_2, y_train_pred))) print(&#39;Recall score: {}&#39;.format(recall_score(y_train_2, y_train_pred))) print(&#39;F1 score: {}&#39;.format(f1_score(y_train_2, y_train_pred))) . Precision score: 0.8657187993680885 Recall score: 0.8277945619335347 F1 score: 0.8463320463320464 . Recall/Precision tradeoff . Unfortunately increasing precision reduces recall and vise-versa. However sometimes one of the qualities could be desirable in a model. . Recall looks at lowering the False Negatives so culling +ve cases. That could be detrimental in catching robberies. So we need classifiers with high recall and we can fine low Precision wherein we would get False alarms. . Meanwhile, if we are censoring videos we need high Precision to ensure unsafe videos categorised as Safe ones. While we could be removing good videos by wrongly classifying them to be Unsafe (low recall). . Decision functions evaluate a decision_score we can manually set the threshold for the score to whether that will accpted or rejected for the binary case. . Increasing threshold reduces recall, but increases precision. . Why? The more Precise you want to be i.e. more True Positive than False Positives -- the higher the threshold for passing the case of accepting the data as a given class. However doing so we are strict in what we define as a ideal class and can neglect samples which are positive but are not closest to ideal. Hence we do incorrectly mark them as Negative thus increasing the case of False Negaitives and hence lowering Recall. . y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) . from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_2, y_scores) . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;, linewidth=2) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;, linewidth=2) plt.xlabel(&quot;Threshold&quot;, fontsize=16) plt.legend(loc=&quot;best&quot;, fontsize=16) plt.ylim([0, 1]) plt.figure(figsize=(8, 4)) plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.xlim([-700000, 700000]) plt.show() . plt.figure(figsize=(8, 4)) plt.plot(recalls[:-1],precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;, linewidth=2) plt.ylabel(&quot;Precision&quot;, fontsize=16) plt.xlabel(&quot;Recall&quot;, fontsize=16) . Text(0.5, 0, &#39;Recall&#39;) . If someone says let&#39;s reach 99% PRECISION, we must ALWAYS ask at what RECALL? . Manually set the Recall/Precision using threshold . y_scores = sgd_clf.decision_function([random_digit]) print(y_scores) y_pred_thresh = sgd_clf.predict([random_digit]) print(y_pred_thresh) #Setting threshold higher than the y_score threshold = y_scores + 1.0 y_pred_thresh = (y_scores &gt; threshold) print(y_pred_thresh) . [38942.84607344] [ True] [False] . y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) y_train_pred_90 = (y_scores &gt; 200000) print(&#39;Precision score: {}&#39;.format(precision_score(y_train_2, y_train_pred_90))) print(&#39;Recall score: {}&#39;.format(recall_score(y_train_2, y_train_pred_90))) print(&#39;F1 score: {}&#39;.format(f1_score(y_train_2, y_train_pred_90))) . Precision score: 0.979538146740719 Recall score: 0.5624370594159114 F1 score: 0.7145751146177631 . We have made classifier with an arbitrary Precision score: 97% However doing so we reduced the Recall. . The ROC curve . Another common tool used for binary classifiers apart from Precision/Recall. Instead of plotting precision vs recall we plot True Positive Rate (TPR) i.e. Recall against False Positive Rate (FPR). FPR is the ratio of negative instances that are incorrectly classified as positive. . ROC plots sensitivity vs 1-specificty . from sklearn.metrics import roc_curve #Decision scores for all instnces in the training set -- y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) fpr, tpr, thresholds = roc_curve(y_train_2, y_scores) def plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([0, 1, 0, 1]) plt.xlabel(&#39;False Positive Rate&#39;, fontsize=16) plt.ylabel(&#39;True Positive Rate&#39;, fontsize=16) plt.figure(figsize=(8, 6)) plot_roc_curve(fpr, tpr) plt.show() . from sklearn.metrics import roc_auc_score roc_auc_score(y_train_2, y_scores) . 0.9679227148419455 . PR curve when we care of precision -- getting False +ve and not so much of getting False -ve. We are okay with losing some +ve cases but for sure do not want to neglect any -ve ones. . Random forest classifier . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=10, random_state=42) y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_2, cv=3, method=&quot;predict_proba&quot;) . y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_2, y_scores_forest) . plt.figure(figsize=(8, 6)) plt.plot(fpr, tpr, &quot;b:&quot;, linewidth=2, label=&quot;SGD&quot;) plot_roc_curve(fpr_forest, tpr_forest, &quot;Random Forest&quot;) plt.legend(loc=&quot;lower right&quot;, fontsize=16) plt.show() . Multiclass classification . Multiclass classifiers are able to label and distinguish between more than two classes. Some algorithms such as Random Forest and N√§ive Bayes are capable of handling this directly. Having said that, Naive Baye&#39;s has shortcomming of considering class conditional independence and having discrete entries in the input. . OvA (One-versus-all classifiers): Herein, we would train n binary classifiers for n type of labels and see which n-th classifier has highest decision score. . | OvO (One-versus-one strategy): Binary classifier for every pair. So for n labels we will have n(n-1)/2 classifiers. . | . Error analysis . X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] index_shuffle = np.random.permutation(60000) X_train, y_train = X_train[index_shuffle], y_train[index_shuffle] sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42) sgd_clf.fit(X_train, y_train) y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3) y_scores = cross_val_predict(sgd_clf, X_train, y_train, cv=3, method=&quot;decision_function&quot;) conf_mx = confusion_matrix(y_train, y_train_pred) . plt.matshow(conf_mx, cmap=plt.cm.gray) plt.show() . Plotting heat-map for the errors in the classification . row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums #Diagonals are filled to be zero to concentrate only at the errors np.fill_diagonal(norm_conf_mx, 0) plt.matshow(norm_conf_mx, cmap=plt.cm.gray) plt.show() . ROWS in the confusion matrix are the REAL labels. COLUMNS in the confusion matrix are the PREDICTED values. It can seen that in the case of row 3 and column 5: . 5 is most of the times confused with 3 and 8 | 9 is confused with 4 and 7 | . def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = mpl.cm.binary, **options) plt.axis(&quot;off&quot;) cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)] plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5) plt.show() . Given above are two sets of &#39;3&#39; and &#39;5&#39; -- the boxes to the left are 3 and 5 classified as 3. Top left are the images of 3 classified as 3 while Bottom left are the images of 5 classified as 3. It can seen that some imags of 5 quite poor and the algorithm (which is linear in this case) will have difficulty predicting it. .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/machine-learning/2021/04/25/MNIST_Scikit_learn-Classification.html",
            "relUrl": "/python/machine-learning/2021/04/25/MNIST_Scikit_learn-Classification.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Making equal spaces parity plots using Matplotlib",
            "content": "import os import matplotlib.pyplot as plt import numpy as np # High DPI rendering for mac # %config InlineBackend.figure_format = &#39;retina&#39; . X = np.linspace(0,5,200) Y = X + np.random.normal(0.01, size=X.shape) fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(X, Y) . &lt;matplotlib.collections.PathCollection at 0x7fac528192e8&gt; . fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(X, Y, label=&#39;data&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;parity&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlim(lims) ax.set_ylim(lims) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) handles, labels = ax.get_legend_handles_labels() print(labels) ax.legend(handles=handles, labels=labels, title=&quot;Facet&quot;) . [&#39;parity&#39;, &#39;data&#39;] . &lt;matplotlib.legend.Legend at 0x7fac540e6f28&gt; .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/plotting/2021/04/24/matplotlib-equal-aspect.html",
            "relUrl": "/python/plotting/2021/04/24/matplotlib-equal-aspect.html",
            "date": " ‚Ä¢ Apr 24, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Bayesian Neural Networks",
            "content": "import torch from torch import nn, optim import numpy as np . import matplotlib.pyplot as plt from matplotlib.pyplot import cm %config InlineBackend.figure_format = &#39;retina&#39; plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . print(torch.cuda.device_count()) # Device configuration device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) . 1 cuda . m = 100 x = (torch.rand(m) - 0.5) * 20 #Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1) y = x * torch.sin(x) #y = 2 * torch.exp( - torch.sin( (x/2)**2 )) . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.plot(x.numpy(), y.numpy(), &#39;o&#39;) ax.axis(&#39;equal&#39;); . class MLP(nn.Module): def __init__(self, hidden_layers=[20, 20], droprate=0.2, activation=&#39;relu&#39;): super(MLP, self).__init__() self.model = nn.Sequential() self.model.add_module(&#39;input&#39;, nn.Linear(1, hidden_layers[0])) if activation == &#39;relu&#39;: self.model.add_module(&#39;relu0&#39;, nn.ReLU()) elif activation == &#39;tanh&#39;: self.model.add_module(&#39;tanh0&#39;, nn.Tanh()) for i in range(len(hidden_layers)-1): self.model.add_module(&#39;dropout&#39;+str(i+1), nn.Dropout(p=droprate)) self.model.add_module(&#39;hidden&#39;+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1])) if activation == &#39;relu&#39;: self.model.add_module(&#39;relu&#39;+str(i+1), nn.ReLU()) elif activation == &#39;tanh&#39;: self.model.add_module(&#39;tanh&#39;+str(i+1), nn.Tanh()) self.model.add_module(&#39;dropout&#39;+str(i+2), nn.Dropout(p=droprate)) self.model.add_module(&#39;final&#39;, nn.Linear(hidden_layers[i+1], 1)) def forward(self, x): return self.model(x) . net = MLP(hidden_layers=[200, 100, 80], droprate=0.1).to(device) #Move model to the GPU print(net) . MLP( (model): Sequential( (input): Linear(in_features=1, out_features=200, bias=True) (relu0): ReLU() (dropout1): Dropout(p=0.1, inplace=False) (hidden1): Linear(in_features=200, out_features=100, bias=True) (relu1): ReLU() (dropout2): Dropout(p=0.1, inplace=False) (hidden2): Linear(in_features=100, out_features=80, bias=True) (relu2): ReLU() (dropout3): Dropout(p=0.1, inplace=False) (final): Linear(in_features=80, out_features=1, bias=True) ) ) . criterion = nn.MSELoss() optimizer = optim.Adam(net.parameters(), lr=0.005, weight_decay=0.00001) . x_dev = x.view(-1, 1).to(device) . for epoch in range(6000): x_dev = x.view(-1, 1).to(device) y_dev = y.view(-1, 1).to(device) y_hat = net(x_dev) loss = criterion(y_hat, y_dev) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print(&#39;Epoch[{}] - Loss:{}&#39;.format(epoch, loss.item())) . Epoch[0] - Loss:13.978106498718262 Epoch[500] - Loss:1.8814525604248047 Epoch[1000] - Loss:0.8525617122650146 Epoch[1500] - Loss:0.5456532835960388 Epoch[2000] - Loss:0.5604020357131958 Epoch[2500] - Loss:0.49089789390563965 Epoch[3000] - Loss:0.35926154255867004 Epoch[3500] - Loss:0.33446982502937317 Epoch[4000] - Loss:0.49845507740974426 Epoch[4500] - Loss:0.2624220848083496 Epoch[5000] - Loss:0.17623820900917053 Epoch[5500] - Loss:0.2600337862968445 . XX = torch.linspace(-11, 11, 1000) . def predict_reg(model, X, T=10): model = model.train() Y_hat = list() with torch.no_grad(): for t in range(T): X_out = model(X.view(-1,1).to(device)) Y_hat.append(X_out.cpu().squeeze()) Y_hat = torch.stack(Y_hat) model = model.eval() with torch.no_grad(): X_out = model(X.view(-1,1).to(device)) Y_eval = X_out.cpu().squeeze() return Y_hat, Y_eval . %time y_hat, y_eval = predict_reg(net, XX, T=1000) mean_y_hat = y_hat.mean(axis=0) std_y_hat = y_hat.std(axis=0) . CPU times: user 14 ¬µs, sys: 1 ¬µs, total: 15 ¬µs Wall time: 29.3 ¬µs . fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(XX.numpy(), mean_y_hat.numpy(), &#39;C1&#39;, label=&#39;prediction&#39;) ax.fill_between(XX.numpy(), (mean_y_hat + std_y_hat).numpy(), (mean_y_hat - std_y_hat).numpy(), color=&#39;C2&#39;, label=&#39;confidence&#39;) ax.plot(x.numpy(), y.numpy(), &#39;oC0&#39;, label=&#39;ground truth&#39;) ax.plot(XX.numpy(), (XX * torch.sin(XX)).numpy(), &#39;k&#39;, label=&#39;base function&#39;) ax.axis(&#39;equal&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fb054b64978&gt; .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/pytorch/machine-learning/plotting/2021/01/11/Simple_Dropout.html",
            "relUrl": "/python/pytorch/machine-learning/plotting/2021/01/11/Simple_Dropout.html",
            "date": " ‚Ä¢ Jan 11, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "S&P 500 analysis using beautifulsoup and pandas",
            "content": "Skills used: . Panda DateTime data wrangling | Beautifulsoup web scraping | . To extract stock information . To extract stock information we will use yfinance module which is a convenient way to download data from Yahoo Finance. The official API for Yahoo Finance was decommissioned some time back. More details about this module can be found here. . from requests import get import numpy as np import pandas as pd from bs4 import BeautifulSoup import time as time from tqdm import tqdm import yfinance as yf from IPython.core.display import clear_output . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns sns.set(style=&quot;whitegrid&quot;) sns.color_palette(&quot;husl&quot;) %config InlineBackend.figure_format = &#39;retina&#39; plot_params = { &#39;font.size&#39; : 30, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . 1. Generate list of S&amp;P 500 companies . Parse wikipedia to generate a list . wiki_url = &#39;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies&#39; response = get(wiki_url) html_soup = BeautifulSoup(response.text, &#39;html.parser&#39;) tab = html_soup.find(&quot;table&quot;,{&quot;class&quot;:&quot;wikitable sortable&quot;}) . column_headings = [entry.text.strip() for entry in tab.findAll(&#39;th&#39;)] print(column_headings) . [&#39;Symbol&#39;, &#39;Security&#39;, &#39;SEC filings&#39;, &#39;GICS Sector&#39;, &#39;GICS Sub-Industry&#39;, &#39;Headquarters Location&#39;, &#39;Date first added&#39;, &#39;CIK&#39;, &#39;Founded&#39;] . SP_500_dict = {keys:[] for keys in column_headings} . for i, name in enumerate(SP_500_dict.keys()): print(i, name) . 0 Symbol 1 Security 2 SEC filings 3 GICS Sector 4 GICS Sub-Industry 5 Headquarters Location 6 Date first added 7 CIK 8 Founded . Populate each row entry as per company data . for row_entry in tab.findAll(&#39;tr&#39;)[1:]: row_elements = row_entry.findAll(&#39;td&#39;) for key, _elements in zip(SP_500_dict.keys(), row_elements): SP_500_dict[key].append(_elements.text.strip()) . SP_500_df = pd.DataFrame(SP_500_dict, columns=SP_500_dict.keys()) . SP_500_df . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 0 MMM | 3M Company | reports | Industrials | Industrial Conglomerates | St. Paul, Minnesota | 1976-08-09 | 0000066740 | 1902 | . 1 ABT | Abbott Laboratories | reports | Health Care | Health Care Equipment | North Chicago, Illinois | 1964-03-31 | 0000001800 | 1888 | . 2 ABBV | AbbVie Inc. | reports | Health Care | Pharmaceuticals | North Chicago, Illinois | 2012-12-31 | 0001551152 | 2013 (1888) | . 3 ABMD | ABIOMED Inc | reports | Health Care | Health Care Equipment | Danvers, Massachusetts | 2018-05-31 | 0000815094 | 1981 | . 4 ACN | Accenture plc | reports | Information Technology | IT Consulting &amp; Other Services | Dublin, Ireland | 2011-07-06 | 0001467373 | 1989 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 500 YUM | Yum! Brands Inc | reports | Consumer Discretionary | Restaurants | Louisville, Kentucky | 1997-10-06 | 0001041061 | 1997 | . 501 ZBRA | Zebra Technologies | reports | Information Technology | Electronic Equipment &amp; Instruments | Lincolnshire, Illinois | 2019-12-23 | 0000877212 | 1969 | . 502 ZBH | Zimmer Biomet | reports | Health Care | Health Care Equipment | Warsaw, Indiana | 2001-08-07 | 0001136869 | 1927 | . 503 ZION | Zions Bancorp | reports | Financials | Regional Banks | Salt Lake City, Utah | 2001-06-22 | 0000109380 | 1873 | . 504 ZTS | Zoetis | reports | Health Care | Pharmaceuticals | Florham Park, New Jersey | 2013-06-21 | 0001555280 | 1952 | . 505 rows √ó 9 columns . SP_500_df[&#39;GICS Sector&#39;].value_counts() . Information Technology 74 Industrials 73 Financials 65 Health Care 63 Consumer Discretionary 61 Consumer Staples 32 Real Estate 30 Materials 28 Utilities 28 Communication Services 26 Energy 25 Name: GICS Sector, dtype: int64 . Visualize distribution of the companies as per sectors . fig, ax = plt.subplots(1,1, figsize=(10,10)) SP_500_df[&#39;GICS Sector&#39;].value_counts().plot.pie(y=&#39;GICS Sector&#39;, autopct=&#39;%1.1f%%&#39;, fontsize=20, ax = ax, colormap=&#39;tab20&#39;) plt.axis(&#39;off&#39;) . (-1.25, 1.25, -1.25, 1.25) . SP_500_df.loc[ SP_500_df[&#39;GICS Sector&#39;] == &#39;Energy&#39;] . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 44 APA | Apache Corporation | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1997-07-28 | 0000006769 | 1954 | . 59 BKR | Baker Hughes Co | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 2017-07-07 | 0001701605 | 2017 | . 80 COG | Cabot Oil &amp; Gas | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 2008-06-23 | 0000858470 | 1989 | . 100 CVX | Chevron Corp. | reports | Energy | Integrated Oil &amp; Gas | San Ramon, California | 1957-03-04 | 0000093410 | 1879 | . 120 CXO | Concho Resources | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Midland, Texas | 2016-02-22 | 0001358071 | 2004 | . 121 COP | ConocoPhillips | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1957-03-04 | 0001163165 | 2002 | . 140 DVN | Devon Energy | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Oklahoma City, Oklahoma | 2000-08-30 | 0001090012 | 1971 | . 142 FANG | Diamondback Energy | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Midland, Texas | 2018-12-03 | 0001539838 | 2007 | . 169 EOG | EOG Resources | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 2000-11-02 | 0000821189 | 1999 | . 183 XOM | Exxon Mobil Corp. | reports | Energy | Integrated Oil &amp; Gas | Irving, Texas | 1957-03-04 | 0000034088 | 1999 | . 219 HAL | Halliburton Co. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 1957-03-04 | 0000045012 | 1919 | . 227 HES | Hess Corporation | reports | Energy | Integrated Oil &amp; Gas | New York, New York | 1984-05-31 | 0000004447 | 1919 | . 230 HFC | HollyFrontier Corp | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Dallas, Texas | 2018-06-18 | 0000048039 | 1947 | . 274 KMI | Kinder Morgan | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Houston, Texas | 2012-05-25 | 0001506307 | 1997 | . 298 MRO | Marathon Oil Corp. | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1991-05-01 | 0000101778 | 1887 | . 299 MPC | Marathon Petroleum | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Findlay, Ohio | 2011-07-01 | 0001510295 | 2009 (1887) | . 344 NOV | NOV Inc. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 2005-03-14 | 0001021860 | 1841 | . 350 OXY | Occidental Petroleum | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1982-12-31 | 0000797468 | 1920 | . 353 OKE | ONEOK | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Tulsa, Oklahoma | 2010-03-15 | 0001039684 | 1906 | . 369 PSX | Phillips 66 | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Houston, Texas | 2012-05-01 | 0001534701 | 2012 (1917) | . 371 PXD | Pioneer Natural Resources | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Irving, Texas | 2008-09-24 | 0001038357 | 1997 | . 407 SLB | Schlumberger Ltd. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Cura√ßao, Kingdom of the Netherlands | 1965-03-31 | 0000087347 | 1926 | . 434 FTI | TechnipFMC | reports | Energy | Oil &amp; Gas Equipment &amp; Services | London, United Kingdom | 2009-06-05 | 0001681459 | 2017 (1958) | . 463 VLO | Valero Energy | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | San Antonio, Texas | | 0001035002 | 1980 | . 493 WMB | Williams Companies | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Tulsa, Oklahoma | 1975-03-31 | 0000107263 | 1908 | . We can parse these tables and search companies based on the sector . SP_500_df.loc[ SP_500_df[&#39;GICS Sector&#39;] == &#39;Information Technology&#39;] . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 4 ACN | Accenture plc | reports | Information Technology | IT Consulting &amp; Other Services | Dublin, Ireland | 2011-07-06 | 0001467373 | 1989 | . 6 ADBE | Adobe Inc. | reports | Information Technology | Application Software | San Jose, California | 1997-05-05 | 0000796343 | 1982 | . 7 AMD | Advanced Micro Devices Inc | reports | Information Technology | Semiconductors | Santa Clara, California | 2017-03-20 | 0000002488 | 1969 | . 13 AKAM | Akamai Technologies Inc | reports | Information Technology | Internet Services &amp; Infrastructure | Cambridge, Massachusetts | 2007-07-12 | 0001086222 | 1998 | . 38 APH | Amphenol Corp | reports | Information Technology | Electronic Components | Wallingford, Connecticut | 2008-09-30 | 0000820313 | 1932 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 488 WDC | Western Digital | reports | Information Technology | Technology Hardware, Storage &amp; Peripherals | San Jose, California | 2009-07-01 | 0000106040 | 1970 | . 489 WU | Western Union Co | reports | Information Technology | Data Processing &amp; Outsourced Services | Englewood, Colorado | 2006-09-29 | 0001365135 | 1851 | . 497 XRX | Xerox | reports | Information Technology | Technology Hardware, Storage &amp; Peripherals | Norwalk, Connecticut | | 0001770450 | 1906 | . 498 XLNX | Xilinx | reports | Information Technology | Semiconductors | San Jose, California | 1999-11-08 | 0000743988 | 1984 | . 501 ZBRA | Zebra Technologies | reports | Information Technology | Electronic Equipment &amp; Instruments | Lincolnshire, Illinois | 2019-12-23 | 0000877212 | 1969 | . 74 rows √ó 9 columns . Get total number of Shares . We will use yfinance to extact Tickr information for each SP500 company and use pandas datareader . yf_tickr = yf.Ticker(&#39;ADBE&#39;) yf_tickr.info[&#39;sharesOutstanding&#39;] #info has good summary info for the stock . import yfinance as yf . START_DATE = &quot;2020-01-01&quot; END_DATE = &quot;2020-07-26&quot; . yf_tickr = yf.Ticker(&#39;TSLA&#39;) . _shares_outstanding = yf_tickr.info[&#39;sharesOutstanding&#39;] _previous_close = yf_tickr.info[&#39;previousClose&#39;] print(&#39;Outstanding shares: {}&#39;.format(_shares_outstanding)) print(&#39;Market Cap: {} Million USD&#39;.format((_shares_outstanding * _previous_close)/10**6)) . Outstanding shares: 947900992 Market Cap: 800966.8592300799 Million USD . df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, interval=&quot;1wk&quot;, actions=False) df_tckr[&#39;Market_Cap&#39;] = df_tckr[&#39;Open&#39;] * _shares_outstanding df_tckr[&#39;YTD&#39;] = (df_tckr[&#39;Open&#39;] - df_tckr[&#39;Open&#39;][0]) * 100 / df_tckr[&#39;Open&#39;][0] . fig, ax = plt.subplots(1,1, figsize=(20,10)) df_tckr.plot(use_index=True, y=&quot;YTD&quot;,ax=ax, linewidth=4, grid=False, label=&#39;TSLA&#39;) ax.set_xlabel(&#39;Date&#39;) ax.set_ylabel(&#39;% YTD change (Weekly basis)&#39;) . Text(0, 0.5, &#39;% YTD change (Weekly basis)&#39;) . Extend this to plotting for multiple companies . import time as time def plot_market_cap(tickr_list, START_DATE, END_DATE): total_data = {} for tickr in tickr_list: total_data[tickr] = {} print(&#39;Looking at: {}&#39;.format(tickr)) yf_tickr = yf.Ticker(tickr) #try: # _shares_outstanding = yf_tickr.info[&#39;sharesOutstanding&#39;] #except(IndexError): # print(&#39;Shares outstanding not found&#39;) # _shares_outstanding = None df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, actions=False) df_tckr[&#39;YTD&#39;] = (df_tckr[&#39;Open&#39;] - df_tckr[&#39;Open&#39;][0]) * 100 / df_tckr[&#39;Open&#39;][0] total_data[tickr][&#39;hist&#39;] = df_tckr #total_data[tickr][&#39;shares&#39;] = _shares_outstanding time.sleep(np.random.randint(10)) return total_data . tickr_list = [&#39;AAPL&#39;, &#39;TSLA&#39;,&#39;FB&#39;,&#39;DAL&#39;,&#39;XOM&#39;] #tickr_list = [&#39;S5INFT&#39;, &#39;SPX&#39;,&#39;XLG&#39;,&#39;SPN&#39;,&#39;S5INDU&#39;] #tickr_list = SP_500_df[&#39;Symbol&#39;].to_list() data = plot_market_cap(tickr_list, START_DATE, END_DATE) . Looking at: AAPL Looking at: TSLA Looking at: FB Looking at: DAL Looking at: XOM . company_name = [SP_500_df[SP_500_df[&#39;Symbol&#39;].str.contains(i)][&#39;Security&#39;].values[0] for i in tickr_list] . company_name . [&#39;Apple Inc.&#39;, &#39;Tesla, Inc.&#39;, &#39;Facebook, Inc.&#39;, &#39;Delta Air Lines Inc.&#39;, &#39;Exxon Mobil Corp.&#39;] . print(len(data[&#39;AAPL&#39;][&#39;hist&#39;][&#39;YTD&#39;])) . 142 . ytd_stat = pd.DataFrame() for tickr in tickr_list: print(len(data[tickr][&#39;hist&#39;][&#39;YTD&#39;])) ytd_stat[tickr] = data[tickr][&#39;hist&#39;][&#39;YTD&#39;].values . 142 142 142 142 142 . ytd_stat[&#39;Date&#39;] = ytd_stat.index . Final plot for returns . fig, ax = plt.subplots(1,1,figsize=(15,10)) for i, tickr in enumerate(tickr_list): ax.plot(ytd_stat[&#39;Date&#39;], ytd_stat[tickr], linewidth=5.0, label=company_name[i]) ax.set_ylabel(&#39;YTD %Return 2020&#39;) ax.set_xlabel(&#39;Date&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7fcaffa7da90&gt; .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/plotting%20web-scrapping/2020/08/01/SP_500.html",
            "relUrl": "/python/plotting%20web-scrapping/2020/08/01/SP_500.html",
            "date": " ‚Ä¢ Aug 1, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Activation functions",
            "content": "Function that activates the particular neuron or node if the value across a particular threshold. These functions add the necessary non-linearity in the ANNs. Each perceptron is, in reality (and traditionally), a logistic regression unit. When N units are stacked on top of each other we get a basic single layer perceptron which serves as the basis of Artificial neural network. . Click here for Google&#39;s ML glossary definition . There are different types of activation function and each has its benefits and faults. One of the consideration is the ease in evaluation of the gradient. It should be easy but also help in the final learning process by translating the necessary abstraction and non-linearity across the network. Some of the activation functions are primarily used to model the output of the ANN. Traditionally for a classification task, we would use a sigmoid activation function for a binary classification to predict a binary output (yes/no). In the case of multi-class classification that activation is replaced by softmax activation to estimate the &#39;probability&#39; across different classes. . Some of the traditionally used Activation functions: . Sigmoid activaton function | tanh (hyperbolic tangent) activaton function | ReLU activaton function | Leaky ReLU activaton function | Softplus function | Softmax function | . import numpy as np import matplotlib.pyplot as plt %config InlineBackend.figure_format = &#39;retina&#39; import seaborn as sns sns.set_palette(&quot;deep&quot;) . ## Baseline reference . z = np.linspace(-10,10,100) . Sigmoid activation function . def sigmoid(z): return 1/(1+np.exp(-z)) # derivative of Sigmoid Function def dsigmoid(a): return a*(1-a) # returns a derivative od sigmoid function if a=sigmoid then a&#39;=a(1-a) . plt.plot(z, sigmoid(z), label = r&#39;$sigmoid$&#39;) plt.plot(z, dsigmoid(sigmoid(z)), label = r&#39;$ frac{ partial (sigmoid)}{ partial z}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.show() . import torch x = torch.tensor(z, requires_grad=True) print(x.requires_grad) b = torch.sigmoid(x) . True . x . tensor([-10.0000, -9.7980, -9.5960, -9.3939, -9.1919, -8.9899, -8.7879, -8.5859, -8.3838, -8.1818, -7.9798, -7.7778, -7.5758, -7.3737, -7.1717, -6.9697, -6.7677, -6.5657, -6.3636, -6.1616, -5.9596, -5.7576, -5.5556, -5.3535, -5.1515, -4.9495, -4.7475, -4.5455, -4.3434, -4.1414, -3.9394, -3.7374, -3.5354, -3.3333, -3.1313, -2.9293, -2.7273, -2.5253, -2.3232, -2.1212, -1.9192, -1.7172, -1.5152, -1.3131, -1.1111, -0.9091, -0.7071, -0.5051, -0.3030, -0.1010, 0.1010, 0.3030, 0.5051, 0.7071, 0.9091, 1.1111, 1.3131, 1.5152, 1.7172, 1.9192, 2.1212, 2.3232, 2.5253, 2.7273, 2.9293, 3.1313, 3.3333, 3.5354, 3.7374, 3.9394, 4.1414, 4.3434, 4.5455, 4.7475, 4.9495, 5.1515, 5.3535, 5.5556, 5.7576, 5.9596, 6.1616, 6.3636, 6.5657, 6.7677, 6.9697, 7.1717, 7.3737, 7.5758, 7.7778, 7.9798, 8.1818, 8.3838, 8.5859, 8.7879, 8.9899, 9.1919, 9.3939, 9.5960, 9.7980, 10.0000], dtype=torch.float64, requires_grad=True) . b.backward(torch.ones(x.shape)) . x.grad . tensor([4.5396e-05, 5.5558e-05, 6.7994e-05, 8.3213e-05, 1.0184e-04, 1.2463e-04, 1.5252e-04, 1.8666e-04, 2.2843e-04, 2.7954e-04, 3.4207e-04, 4.1859e-04, 5.1221e-04, 6.2673e-04, 7.6682e-04, 9.3817e-04, 1.1477e-03, 1.4039e-03, 1.7172e-03, 2.1000e-03, 2.5677e-03, 3.1389e-03, 3.8362e-03, 4.6869e-03, 5.7241e-03, 6.9876e-03, 8.5250e-03, 1.0394e-02, 1.2661e-02, 1.5407e-02, 1.8724e-02, 2.2721e-02, 2.7521e-02, 3.3259e-02, 4.0084e-02, 4.8151e-02, 5.7615e-02, 6.8615e-02, 8.1257e-02, 9.5592e-02, 1.1158e-01, 1.2906e-01, 1.4771e-01, 1.6703e-01, 1.8633e-01, 2.0471e-01, 2.2118e-01, 2.3471e-01, 2.4435e-01, 2.4936e-01, 2.4936e-01, 2.4435e-01, 2.3471e-01, 2.2118e-01, 2.0471e-01, 1.8633e-01, 1.6703e-01, 1.4771e-01, 1.2906e-01, 1.1158e-01, 9.5592e-02, 8.1257e-02, 6.8615e-02, 5.7615e-02, 4.8151e-02, 4.0084e-02, 3.3259e-02, 2.7521e-02, 2.2721e-02, 1.8724e-02, 1.5407e-02, 1.2661e-02, 1.0394e-02, 8.5250e-03, 6.9876e-03, 5.7241e-03, 4.6869e-03, 3.8362e-03, 3.1389e-03, 2.5677e-03, 2.1000e-03, 1.7172e-03, 1.4039e-03, 1.1477e-03, 9.3817e-04, 7.6682e-04, 6.2673e-04, 5.1221e-04, 4.1859e-04, 3.4207e-04, 2.7954e-04, 2.2843e-04, 1.8666e-04, 1.5252e-04, 1.2463e-04, 1.0184e-04, 8.3213e-05, 6.7994e-05, 5.5558e-05, 4.5396e-05], dtype=torch.float64) . plt.plot(x.data.numpy(), b.data.numpy(), label = r&#39;$sigmoid$&#39;) plt.plot(x.data.numpy(), x.grad.data.numpy(), label = r&#39;$ frac{ partial (sigmoid)}{ partial z}$&#39;) plt.legend(fontsize = 12) . &lt;matplotlib.legend.Legend at 0x7f8b5f3ece48&gt; . np.unique(np.round((x.grad.data.numpy() - dsigmoid(sigmoid(z))),4)) . array([0.]) . Hyperbolic tangent activation function . def tanh(z): return np.tanh(z) # derivative of tanh def dtanh(a): return 1-np.power(a,2) . plt.plot(z, tanh(z),&#39;b&#39;, label = &#39;tanh&#39;) plt.plot(z, dtanh(tanh(z)),&#39;r&#39;, label=r&#39;$ frac{dtanh}{dz}$&#39;) plt.legend(fontsize = 12) plt.show() . ReLU (Rectified Linear Unit) Activation function . def ReLU(z): return np.maximum(0,z) # derivative of ReLu def dReLU(a): return 1*(a&gt;0) . plt.plot(z, ReLU(z),&#39;b&#39;, label =&#39;ReLU&#39;) plt.plot(z, dReLU(ReLU(z)),&#39;r&#39;, label=r&#39;$ frac{dReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.ylim(0,4) plt.xlim(-4,4) plt.show() . Leaky ReLU Activation function . def LeakyReLU(z): return np.maximum(0.01*z,z) # derivative of ReLu def dLeakyReLU(a): return 0.01*(a&gt;0) . plt.plot(z, LeakyReLU(z),&#39;b&#39;, label = &#39;LeakyReLU&#39;) plt.plot(z, dLeakyReLU(LeakyReLU(z)),&#39;r&#39;, label=r&#39;$ frac{dLeakyReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.ylim(0,4) plt.xlim(-4,4) plt.show() . Comparison of derivative for activation functions . plt.plot(z, dsigmoid(sigmoid(z)),label = r&#39;$ frac{dsigmoid}{dz}$&#39; ) plt.plot(z, dtanh(tanh(z)), label = r&#39;$ frac{dtanh}{dz}$&#39;) plt.plot(z, dReLU(ReLU(z)), label=r&#39;$ frac{dReLU}{dz}$&#39;) plt.plot(z, dLeakyReLU(LeakyReLU(z)), label=r&#39;$ frac{dLeakyReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.title(&#39;Derivatives of activation functions&#39;) plt.show() .",
            "url": "https://pgg1610.github.io/blog_fastpages/python/machine-learning/pytorch/2020/04/22/activation_functions.html",
            "relUrl": "/python/machine-learning/pytorch/2020/04/22/activation_functions.html",
            "date": " ‚Ä¢ Apr 22, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://pgg1610.github.io/blog_fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Data aesthete at heart, I am a Ph.D. candidate in the Davidson School of Chemical Engineering at Purdue University, advised by Jeffrey P. Greeley. Understanding molecular-level behavior of the catalyst through data and science is at the heart of my Ph.D. research. . Besides research, I enjoy teaching and making scientific ideas accessible to the public. I have taught introductory chemistry and mathematics to middle-schoolers, and have assisted in designing modules that introduce critical thinking, logical reasoning, and coding in high-school curricula. . Outside of work, I enjoy cooking (link to some latest experiments), gaming, diving deep in arcane internet rabbit holes, reading philosophy, and going on longer-than-necessary bike rides. .",
          "url": "https://pgg1610.github.io/blog_fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://pgg1610.github.io/blog_fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}