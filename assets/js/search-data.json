{
  
    
        "post0": {
            "title": "Cheminformatics Literature and Resources",
            "content": "Noteworthy blogs to follow: . Patrick Walters Blog on Cheminformatics Pat Walter’s Cheminformatics Resources list | Cheminformatics Hands-on workshop | . | Is Life Worth Living Very helpful cookbook on Python for Cheminformatics | . | Cheminformia . | Depth-First | Reviews: . Navigating through the Maze of Homogeneous Catalyst Design with Machine Learning . | Coley, C. W. Defining and Exploring Chemical Spaces. Trends in Chemistry 2020 . | Applications of Deep learning in molecular generation and molecular property prediction . | Bayer’s ADMET platform review . | Utilising Graph Machine Learning within Drug Discovery and Development . | Special Journal Issues: . Nice collection of recent papers in Nature Communications on ML application and modeling . | Journal of Medicinal Chemistry compendium of AI in Drug discovery issue . | Specific Articles . Few key papers which I have found useful when learning more about the state-of-the-art in Cheminformatics. I’ve tried to categorize them roughly based on their area of application: . Representation: . Representation of Molecular in NN: Molecular representation in AI-driven drug discovery: review and guide . | Screening of energetic molecules – comparing different representations . | M. Krenn, F. Hase, A. Nigam, P. Friederich, and A. Aspuru-Guzik, “Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation,” Mach. Learn. Sci. Technol., pp. 1–9, 2020 . | . Uncertainty quantification: . Alan Aspuru-Guzik perspective on uncertainty and confidence . | Uncertainty Quantification Using Neural Networks for Molecular Property Prediction. J. Chem. Inf. Model. (2020) Hirschfeld, L., Swanson, K., Yang, K., Barzilay, R. &amp; Coley, C. W. . | . Benchmark different models and uncertainty metrics for molecular property prediction. . Evidential Deep learning for guided molecular property prediction and disocovery Ava Soleimany, Conor Coley, et. al.. Slides | . Train network to output the parameters of an evidential distribution. One forward-pass to find the uncertainty as opposed to dropout or ensemble - principled incorporation of uncertainties . Differentiable sampling of molecular geometries with uncertainty-based adversarial attacks | . Active Learning . Active learning provides strategies for efficient screening of subsets of the library. In many cases, we can identify a large portion of the most promising molecules with a fraction of the compute cost. . Reker, D. Practical Considerations for Active Machine Learning in Drug Discovery. Drug Discov. Today Technol. 2020 . | B. J. Shields et al., “Bayesian reaction optimization as a tool for chemical synthesis,” Nature, vol. 590, no. June 2020, p. 89, 2021. Github . | . Experimental design using Bayesian Optimization. . Transfer Learning . Approaching coupled cluster accuracy with a general-purpose neural network potential through transfer learning Transfer learning by training a network to DFT data and then retrain on a dataset of gold standard QM calculations (CCSD(T)/CBS) that optimally spans chemical space. The resulting potential is broadly applicable to materials science, biology, and chemistry, and billions of times faster than CCSD(T)/CBS calculations. . | Improving the generative performance of chemical autoencoders through transfer learning . | . Generative models: . B. Sanchez-Lengeling and A. Aspuru-Guzik, “Inverse molecular design using machine learning: Generative models for matter engineering,” Science (80-. )., vol. 361, no. 6400, pp. 360–365, Jul. 2018 . Research Articles: . | R. Gómez-Bombarelli et al., “Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules,” ACS Cent. Sci., vol. 4, no. 2, pp. 268–276, 2018 . | . One of the first implementation of a variation auto-encoder for molecule generation . Penalized Variational Autoencoder . | SELFIES and generative models using STONED . | . Representation using SELFIES proposed to make it much more powerful . W. Jin, R. Barzilay, and T. Jaakkola, “Junction tree variational autoencoder for molecular graph generation,” 35th Int. Conf. Mach. Learn. ICML 2018, vol. 5, pp. 3632–3648, 2018 | . Junction tree based decoding. Define a grammar for the small molecule and find sub-units based on that grammar to construct a molecule . N. De Cao and T. Kipf, “MolGAN: An implicit generative model for small molecular graphs,” 2018 | . Generative adversarial network for finding small molecules using graph networks, quite interesting . Message passing graph networks for molecular generation | . Language models: . LSTM based (RNN) approaches to small molecule generation. Github . | Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. arXiv [cs.LG], 2020. . | SMILES-based deep generative scaffold decorator for de-novo drug design. Github . | . Synthesizability Criteria into Generative Models: . Gao, W.; Coley, C. W. The Synthesizability of Molecules Proposed by Generative Models. J. Chem. Inf. Model. 2020 Paper looks at different ways of integrating synthesizability criteria into generative models. | . Reaction Network Predictions: . Prediction of Organic Reaction Outcomes Using Machine Learning, ACS Cent. Sci. 2017 . | Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction Follow-up: Quantitative interpretation explains machine learning models for chemical reaction prediction and uncovers bias | . | Automatic discovery of chemical reactions using imposed activation . | Machine learning in chemical reaction space | . Code / Packages: . Schnet by Jacobsen et. al. (Neural message passing). Github. Tutorial . | OpenChem. Github . | DeepChem . | DimeNet++ – extension of Directional message pasing working (DimeNet). Github . | PhysNet . | RNN based encoder software . | AutodE . | DScribe . | . Helpful utilities: . RD-Kit Get Atom Indices in the SMILE: | Datamol for manipulating RDKit molecules | . | Papers with code benchmark for QM9 energy predictions . | Molecular generation models benchmark | . Molecules datasets: . GDB Dataset . | Quantum Machine: Website listing useful datasets including QM9s and MD trajectory . | Github repository listing databases for Drug Discovery . | .",
            "url": "http://pgg1610.github.io/blog_fastpages/chemistry/machine-learning/resources/2021/05/08/Cheminformatics_Resources.html",
            "relUrl": "/chemistry/machine-learning/resources/2021/05/08/Cheminformatics_Resources.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Making equal spaces parity plots using Matplotlib",
            "content": "import os import matplotlib.pyplot as plt import numpy as np # High DPI rendering for mac %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . X = np.linspace(0,5,200) Y = X + np.random.normal(0.01, size=X.shape) fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(X, Y) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) . Text(0, 0.5, &#39;Y&#39;) . fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(X, Y, label=&#39;data&#39;) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.plot(lims, lims, &#39;k--&#39;, alpha=0.75, zorder=0, label=&#39;parity&#39;) ax.set_aspect(&#39;equal&#39;) ax.set_xlim(lims) ax.set_ylim(lims) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) handles, labels = ax.get_legend_handles_labels() print(labels) ax.legend(handles=handles, labels=labels, title=&quot;Facet&quot;) . [&#39;parity&#39;, &#39;data&#39;] . &lt;matplotlib.legend.Legend at 0x7fb85a3a7198&gt; .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-visualization/2021/04/08/matplotlib-equal-aspect.html",
            "relUrl": "/python/data-visualization/2021/04/08/matplotlib-equal-aspect.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Implement neural network from scratch for binary classification",
            "content": "In this notebook I build a simple neural network, having a single hidden layer. Next, I compare this model for its classification accuracy to a boilerplate logistic regression. . Implement a 2-class classification neural network with a single hidden layer | Use units with a non-linear activation function, such as tanh | Compute the cross entropy loss | Implement forward and backward propagation | . This code was adapted from Andrew Ng&#39;s Deep Learning Specialization course on Coursera . import numpy as np import matplotlib.pyplot as plt import sklearn import sklearn.datasets as datasets import sklearn.linear_model import copy as copy %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} np.random.seed(42) # set a seed so that the results are consistent . Dataset . Code to make spirals is adapted from: . http://cs231n.github.io/neural-networks-case-study/ . N = 400 # number of points per class D = 2 # dimensionality K = 2 # number of spokes X = np.zeros((N*K,D)) # data matrix (each row = single example) Y = np.zeros(N*K, dtype=&#39;int&#39;) # class labels for j in range(K): ix = range(N*j,N*(j+1)) r = np.linspace(0, 1, N) # radius t = np.linspace(j*4.2, (j+1)*4.2, N) + np.random.randn(N)*0.2 # theta X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] Y[ix] = (0 if j % 2 == 0 else 1) X = copy.deepcopy(X.T) Y = copy.deepcopy(Y.reshape(-1,1).T) . fig, ax = plt.subplots(1,1, figsize=(8,8)) # lets visualize the data: ax.scatter(X[0, :], X[1, :], c=Y.ravel(), s=40, cmap=plt.cm.Spectral) ax.set_xlabel(&#39;$X_1$&#39;) ax.set_ylabel(&#39;$X_2$&#39;) ax.set_title(&#39;Visualize data&#39;) . Text(0.5, 1.0, &#39;Visualize data&#39;) . shape_X = X.shape shape_Y = Y.shape print (&#39;The shape of X is: &#39; + str(shape_X)) print (&#39;The shape of Y is: &#39; + str(shape_Y)) . The shape of X is: (2, 800) The shape of Y is: (1, 800) . Simple Logistic Regression . Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn&#39;s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset. . clf = sklearn.linear_model.LogisticRegression(); clf.fit(X.T, Y.ravel()); . Convenience function to plot a decision boundary for the classification model . def plot_decision_boundary(func, x_input, y_input): xx_1, xx_2 = np.mgrid[np.min(x_input[:,0]):np.max(x_input[:,0]):.01, np.min(x_input[:,1]):np.max(x_input[:,1]):.01] grid = np.c_[xx_1.ravel(), xx_2.ravel()] y_pred_grid = func(grid).reshape(xx_1.shape) y_pred = func(x_input) fig, ax = plt.subplots(figsize=(10, 10)) contour = ax.contourf(xx_1, xx_2, y_pred_grid, alpha=0.7, cmap=&quot;Spectral&quot;) ax.scatter(x_input[:,0], x_input[:, 1], c=y_pred, s=50, cmap=&quot;Spectral&quot;, edgecolor=&quot;white&quot;, linewidth=1) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.set(aspect=&#39;equal&#39;, xlim=(np.min(x_input[:,0]), np.max(x_input[:,0])), ylim=(np.min(x_input[:,1]),np.max(x_input[:,1])), xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;) . plot_decision_boundary(lambda x: clf.predict(x), X.T, Y.T) plt.title(&quot;Logistic Regression&quot;) . Text(0.5, 1.0, &#39;Logistic Regression&#39;) . LR_predictions = clf.predict(X.T) print (&#39;Accuracy of logistic regression: %d &#39; % float((np.dot(Y, LR_predictions) + np.dot(1-Y, 1-LR_predictions))/float(Y.size)*100) + &#39;% &#39; + &quot;(percentage of correctly labelled datapoints)&quot;) . Accuracy of logistic regression: 66 % (percentage of correctly labelled datapoints) . Interpretation: The dataset is not linearly separable, so logistic regression doesn&#39;t perform well. Hopefully a neural network will do better. . Neural Network model . Logistic regression did not work well on the dataset. Let&#39;s train a Neural Network with a single hidden layer and see if it does any better. . Here is basic framework for the model: . Mathematically: . For one example $x^{(i)}$: . $$ z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1]} tag{1} $$ . $$ a^{[1] (i)} = tanh(z^{[1] (i)}) tag{2} $$$$ z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]} tag{3} $$$$ hat{y}^{(i)} = a^{[2] (i)} = sigma(z^{ [2] (i)}) tag{4} $$$$ y^{(i)}_{prediction} = begin{cases} 1 &amp; mbox{if } a^{[2](i)} &gt; 0.5 0 &amp; mbox{otherwise } end{cases} tag{5} $$Given the predictions on all the examples, you can also compute the cost $J$ as follows: . $$ J = - frac{1}{m} sum limits_{i = 0}^{m} large left( small y^{(i)} log left(a^{[2] (i)} right) + (1-y^{(i)}) log left(1- a^{[2] (i)} right) large right) small tag{6} $$The general methodology to build a Neural Network is to: . Define the neural network structure ( # of input units, # of hidden units, etc). | Initialize the model&#39;s parameters | Loop: Implement forward propagation | Compute loss | Implement backward propagation to get the gradients | Update parameters (gradient descent) | . | Defining the neural network structure . Define three variables: . - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4) - n_y: the size of the output layer . def layer_sizes(X, Y, n_h=4): &quot;&quot;&quot; Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer &quot;&quot;&quot; n_x = X.shape[0] # size of input layer n_h = n_h n_y = Y.reshape(-1,1).T.shape[0] # size of output layer return (n_x, n_h, n_y) . (n_x, n_h, n_y) = layer_sizes(X, Y) print(&quot;The size of the input layer is: n_x = &quot; + str(n_x)) print(&quot;The size of the hidden layer is: n_h = &quot; + str(n_h)) print(&quot;The size of the output layer is: n_y = &quot; + str(n_y)) . The size of the input layer is: n_x = 2 The size of the hidden layer is: n_h = 4 The size of the output layer is: n_y = 1 . Initialize the model&#39;s parameters . Initialize the weights matrices with random values. Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). | . | Initialize the bias vectors as zeros. Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros. | . | . def initialize_parameters(n_x, n_h, n_y): &quot;&quot;&quot; Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) &quot;&quot;&quot; np.random.seed(42) # we set up a seed so that your output matches ours although the initialization is random. W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y,1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2} return parameters . Forward-pass . Implement forward_propagation(): . Retrieve each parameter from the dictionary &quot;parameters&quot; (which is the output of initialize_parameters()) by using parameters[&quot;..&quot;]. | Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set). | Values needed in the backpropagation are stored in &quot;cache&quot;. The cache will be given as an input to the backpropagation function. | . def sigmoid(x): z = 1/(1 + np.exp(-x)) return z . def forward_propagation(X, parameters): &quot;&quot;&quot; Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot; &quot;&quot;&quot; # Retrieve each parameter from the dictionary &quot;parameters&quot; W1 = parameters[&#39;W1&#39;] b1 = parameters[&#39;b1&#39;] W2 = parameters[&#39;W2&#39;] b2 = parameters[&#39;b2&#39;] ### END CODE HERE ### # Implement Forward Propagation Z1 = np.dot(W1,X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2,A1) + b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = {&quot;Z1&quot;: Z1, &quot;A1&quot;: A1, &quot;Z2&quot;: Z2, &quot;A2&quot;: A2} return A2, cache . Loss function . Compute the cost function as follows: . $$ J = - frac{1}{m} sum limits_{i = 1}^{m} large{(} small y^{(i)} log left(a^{[2] (i)} right) + (1-y^{(i)}) log left(1- a^{[2] (i)} right) large{)} small tag{13} $$ def compute_cost(A2, Y): &quot;&quot;&quot; Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- &quot;true&quot; labels vector of shape (1, number of examples) Returns: cost -- cross-entropy cost given equation (13) &quot;&quot;&quot; m = Y.shape[1] # number of example # Compute the cross-entropy cost logprobs = np.dot(Y,np.log(A2).T) + np.dot((1-Y),np.log((1-A2)).T) cost = -logprobs/m cost = float(np.squeeze(cost)) # makes sure cost is the dimension we expect. E.g., turns [[17]] into 17 assert(isinstance(cost, float)) return cost . Back-propogation . Using the cache computed during forward propagation, now implement backward propagation. . $$ frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } = frac{1}{m} (a^{[2](i)} - y^{(i)}) $$$$ frac{ partial mathcal{J} }{ partial W_2 } = frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } a^{[1] (i) T} $$$$ frac{ partial mathcal{J} }{ partial b_2 } = sum_i{ frac{ partial mathcal{J} }{ partial z_{2}^{(i)}}} $$$$ frac{ partial mathcal{J} }{ partial z_{1}^{(i)} } = W_2^T frac{ partial mathcal{J} }{ partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $$$$ frac{ partial mathcal{J} }{ partial W_1 } = frac{ partial mathcal{J} }{ partial z_{1}^{(i)} } X^T $$$$ frac{ partial mathcal{J} _i }{ partial b_1 } = sum_i{ frac{ partial mathcal{J} }{ partial z_{1}^{(i)}}} $$ $*$ denotes elementwise multiplication. | Gradients for each later: dW1 = $ frac{ partial mathcal{J} }{ partial W_1 }$ | db1 = $ frac{ partial mathcal{J} }{ partial b_1 }$ | dW2 = $ frac{ partial mathcal{J} }{ partial W_2 }$ | db2 = $ frac{ partial mathcal{J} }{ partial b_2 }$ | . | . def backward_propagation(parameters, cache, X, Y): &quot;&quot;&quot; Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;. X -- input data of shape (2, number of examples) Y -- &quot;true&quot; labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters &quot;&quot;&quot; m = X.shape[1] # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;. W1 = parameters[&#39;W1&#39;] W2 = parameters[&#39;W2&#39;] # Retrieve also A1 and A2 from dictionary &quot;cache&quot;. A1 = cache[&#39;A1&#39;] A2 = cache[&#39;A2&#39;] # Backward propagation: calculate dW1, db1, dW2, db2. dZ2 = A2 - Y dW2 = (1/m) * np.dot(dZ2,A1.T) db2 = (1/m) * np.sum(dZ2,axis=1, keepdims=True) dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = (1/m) * np.dot(dZ1, X.T) db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True) grads = {&quot;dW1&quot;: dW1, &quot;db1&quot;: db1, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2} return grads . General gradient descent formalism: $$ theta = theta - alpha frac{ partial J }{ partial theta }$$ . where: $ alpha$ is the learning rate and $ theta$ represents a parameter. . def update_parameters(parameters, grads, learning_rate = 1.2): &quot;&quot;&quot; Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters &quot;&quot;&quot; # Retrieve each parameter from the dictionary &quot;parameters&quot; W1 = parameters[&#39;W1&#39;] b1 = parameters[&#39;b1&#39;] W2 = parameters[&#39;W2&#39;] b2 = parameters[&#39;b2&#39;] # Retrieve each gradient from the dictionary &quot;grads&quot; dW1 = grads[&#39;dW1&#39;] db1 = grads[&#39;db1&#39;] dW2 = grads[&#39;dW2&#39;] db2 = grads[&#39;db2&#39;] # Update rule for each parameter W1 = W1 - learning_rate*dW1 b1 = b1 - learning_rate*db1 W2 = W2 - learning_rate*dW2 b2 = b2 - learning_rate*db2 parameters = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2} return parameters . Integrate previous parts nn_model() . def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): &quot;&quot;&quot; Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. &quot;&quot;&quot; np.random.seed(42) n_x, n_h, n_y = layer_sizes(X, Y, n_h=n_h) # Initialize parameters parameters = initialize_parameters(n_x, n_h, n_y) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;. A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;. cost = compute_cost(A2, Y) # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;. grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;. parameters = update_parameters(parameters, grads, learning_rate = 1.2) # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print (&quot;Cost after iteration %i: %f&quot; %(i, cost)) return parameters . Predictions . Use the model to predict: predict(). . Use forward propagation to predict results. . predictions = $y_{prediction} = mathbb 1 text = begin{cases} 1 &amp; text{if} activation &gt; 0.5 0 &amp; text{otherwise} end{cases}$ . def predict(parameters, X): &quot;&quot;&quot; Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) &quot;&quot;&quot; # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. A2, cache = forward_propagation(X, parameters) threshold = 0.5 predictions = (A2 &gt; threshold) return predictions . It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units. . parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True) . Cost after iteration 0: 0.693141 Cost after iteration 1000: 0.052671 Cost after iteration 2000: 0.040765 Cost after iteration 3000: 0.032499 Cost after iteration 4000: 0.027457 Cost after iteration 5000: 0.023722 Cost after iteration 6000: 0.020082 Cost after iteration 7000: 0.016282 Cost after iteration 8000: 0.013001 Cost after iteration 9000: 0.010872 . def plot_decision_boundary_NN(func, x_input, y_input, ax=None): xx_1, xx_2 = np.mgrid[np.min(x_input[:,0]):np.max(x_input[:,0]):.01, np.min(x_input[:,1]):np.max(x_input[:,1]):.01] grid = np.c_[xx_1.ravel(), xx_2.ravel()].T y_pred_grid = func(grid).reshape(xx_1.shape) y_pred = func(x_input.T) if ax == None: fig, ax = plt.subplots(1,1, figsize=(10,10)) contour = ax.contourf(xx_1, xx_2, y_pred_grid, alpha=0.7, cmap=&quot;Spectral&quot;) ax.scatter(x_input[:,0], x_input[:, 1], c=y_pred, s=50, cmap=&quot;Spectral&quot;, edgecolor=&quot;white&quot;, linewidth=1) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.set(aspect=&#39;equal&#39;, xlim=(np.min(x_input[:,0]), np.max(x_input[:,0])), ylim=(np.min(x_input[:,1]),np.max(x_input[:,1])), xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;) return ax . plot_decision_boundary_NN(lambda x: predict(parameters, x), X.T, Y.T) plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4)) . Text(0.5, 1.0, &#39;Decision Boundary for hidden layer size 4&#39;) . predictions = predict(parameters, X) print (&#39;Accuracy: %d&#39; % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + &#39;%&#39;) . Accuracy: 99% . Accuracy is really high compared to Logistic Regression. The model has spirals! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. . Tuning hidden layer size . Run the following code to observe different behaviors of the model for various hidden layer sizes. . plt.figure(figsize=(16, 32)) hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50] for i, n_h in enumerate(hidden_layer_sizes): ax = plt.subplot(5, 2,i+1) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary_NN(lambda x: predict(parameters, x), X.T, Y.T, ax) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) ax.title.set_text(&#39;Hidden Layer of size {} | Accuracy: {}%&#39;.format(n_h, accuracy)) . Reference: . http://scs.ryerson.ca/~aharley/neural-networks/ | http://cs231n.github.io/neural-networks-case-study/ | .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/pytorch/machine-learning/2021/02/05/NN_classification_from_scratch.html",
            "relUrl": "/python/pytorch/machine-learning/2021/02/05/NN_classification_from_scratch.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "t-SNE and UMAP - Effect of initialization on the dimensionality reduction",
            "content": "Recreating the dataset explored in the recent publication looking at the effect of random initializations and sub-methods in well-known dimensionality reduction techniques: Initialization is critical for preserving global data structure in both t-SNE and UMAP . Module used t-SNE: Link | Module used for UMAP: Link | . Key takeaways: . Using either t-SNE or UMAP over another is difficult to justify. There is no evidence per se that UMAP algorithm have any advantage over t-SNE in terms of preserving global structure. . | These algorithms should be used cautiously and with informative initialization by default . | In all embeddings, distances between clusters of points can be completely meaningless. It is often impossible to represent complex topologies in 2 dimensions, and embeddings should be approached with the utmost care when attempting to interpret their layout. . | The only cerrtainty is the closeness of the points and their similarity . | These methods don’t work that great if the intrinsic dimensionality of the data is higher than 2D . | High dimensional data sets typically have lower intrinsic dimensionality $ d &lt;&lt; D $ however $d$ may still be larger than 2 and preserving these distances faithfully might not always be possible. . | When using both UMAP or t-SNE, one must take care not to overinterpret the embedding structure or distances. . | . import numpy as np import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} import openTSNE, umap print(&#39;openTSNE&#39;, openTSNE.__version__) print(&#39;umap&#39;, umap.__version__) . openTSNE 0.6.0 umap 0.5.1 . from openTSNE import TSNE from umap import UMAP . 1. Looking at 2D circle . n = 7000 np.random.seed(42) X = np.random.randn(n, 3) / 1000 X[:,0] += np.cos(np.arange(n)*2*np.pi/n) X[:,1] += np.sin(np.arange(n)*2*np.pi/n) plt.plot(X[:,0], X[:,1]); plt.axis(&#39;equal&#39;); . %%time # BH is faster for this sample size Z1 = TSNE(n_jobs=-1, initialization=&#39;random&#39;, random_state=42, negative_gradient_method=&#39;bh&#39;).fit(X) Z2 = TSNE(n_jobs=-1, negative_gradient_method=&#39;bh&#39;).fit(X) . CPU times: user 48.3 s, sys: 760 ms, total: 49.1 s Wall time: 40.8 s . %%time Z3 = UMAP(init=&#39;random&#39;, random_state=42).fit_transform(X) Z4 = UMAP().fit_transform(X) . CPU times: user 58.4 s, sys: 2.59 s, total: 1min Wall time: 33.4 s . %%time from sklearn import decomposition pca_2D = decomposition.PCA(n_components=2) pca_2D.fit(X) Z5 = pca_2D.transform(X) . CPU times: user 6.17 ms, sys: 5.36 ms, total: 11.5 ms Wall time: 5.72 ms . from matplotlib.colors import ListedColormap cmap = ListedColormap(sns.husl_palette(n)) titles = [&#39;Data&#39;, &#39;t-SNE, random init&#39;, &#39;t-SNE, PCA init&#39;, &#39;UMAP, random init&#39;, &#39;UMAP, LE init&#39;, &#39;PCA&#39;] plt.figure(figsize=(10,2)) for i,Z in enumerate([X,Z1,Z2,Z3,Z4,Z5],1): plt.subplot(1,6,i) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;datalim&#39;) plt.scatter(Z[:,0], Z[:,1], s=1, c=np.arange(n), cmap=cmap, edgecolor=&#39;none&#39;, rasterized=True) plt.xticks([]) plt.yticks([]) plt.title(titles[i-1], fontsize=8) #sns.despine(left=True, bottom=True) . 2. Looking at hand-written digit data . from sklearn.datasets import load_digits digits = load_digits() . digits.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) . X = digits.data Y = digits.target . %%time # BH is faster for this sample size Z1 = TSNE(n_jobs=-1, initialization=&#39;random&#39;, random_state=42, negative_gradient_method=&#39;bh&#39;).fit(X) Z2 = TSNE(n_jobs=-1, negative_gradient_method=&#39;bh&#39;).fit(X) . CPU times: user 14.8 s, sys: 331 ms, total: 15.2 s Wall time: 13.3 s . %%time Z3 = UMAP(init=&#39;random&#39;, random_state=42).fit_transform(X) Z4 = UMAP().fit_transform(X) . CPU times: user 17.7 s, sys: 311 ms, total: 18 s Wall time: 12.5 s . %%time pca_2D = decomposition.PCA(n_components=2) pca_2D.fit(X) Z5 = pca_2D.transform(X) . CPU times: user 17.8 ms, sys: 16.7 ms, total: 34.5 ms Wall time: 10.2 ms . from matplotlib.colors import ListedColormap cmap = ListedColormap(sns.husl_palette(len(np.unique(Y)))) titles = [&#39;Data&#39;, &#39;t-SNE, random init&#39;, &#39;t-SNE, PCA init&#39;, &#39;UMAP, random init&#39;, &#39;UMAP, LE init&#39;, &#39;PCA&#39;] fig, ax = plt.subplots(3,2, figsize=(15,15)) ax = ax.flatten() for i,Z in enumerate([X,Z1,Z2,Z3,Z4,Z5],0): im = ax[i].scatter(Z[:,0], Z[:,1], s=10, c=Y, cmap=cmap, edgecolor=&#39;none&#39;) ax[i].set_xticks([]) ax[i].set_yticks([]) ax[i].set_title(titles[i], fontsize=15) fig.subplots_adjust(right=0.8) cbar_ax = fig.add_axes([0.85, 0.25, 0.01, 0.5], label=&#39;digit&#39;) cbar = fig.colorbar(im, cax=cbar_ax,label=&#39;Digit&#39;) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-visualization/2021/02/03/tSNEvsUMAP.html",
            "relUrl": "/python/data-visualization/2021/02/03/tSNEvsUMAP.html",
            "date": " • Feb 3, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Bayesian optimisation implementation",
            "content": "If $f$ (objective function) is cheap to evaluate we can sample various points and built a potential surface however, if the $f$ is expensive -- like in case of first-principles electronic structure calculations, it is important to minimize the number of $f$ calls and number of samples drawn from this evaluation. In that case, if an exact functional form for f is not available (that is, f behaves as a “black box”), what can we do? . Bayesian optimization proceeds by maintaining a probabilistic belief about $f$ and designing a so called acquisition function to determine where to evaluate the next function call. Bayesian optimization is particularly well-suited to global optimization problems where: . $f$ is an expensive black-box function | Analytical solution for the gradient of the function is difficult to evaluate | The idea is the find &quot;global&quot; minimum with least number of steps. Incorporating prior beliefs about the underlying process and update the prior with samples draw from the model to better estimate the posterior. . Model used for approximating the objective function is called the surrogate model. . Following are few links I have found useful in understanding the inner workings of the Bayesian opitmization and certain typical surrogate functions used in it: . Good introductory write-up on Bayesian optimization (Distill Blog) . | Nice lecture explaining the working of Gaussian Processes here . | . Surrogate model . A popular surrogate model applied for Bayesian optimization, although strictly not required, are Gaussian Processes (GPs). These are used to define a prior beliefs about the objective function. The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement. Herein, we could substitute this for a ANNs or other surrogate models. . Acquisition functions . Used to propose sampling points in the search space. Have to consider the trade-off between exploitation vs exploration. . Exploitation == sampling where objective function value is high . | Exploration == where uncertainty is high . | . Both correspond to high acquisition function value. The goal is the maximize the acquisition value to determine next sampling point. . Popular acquisition functions: . Maximum probability of improvement . PI involves sampling for points which improve on the current best objective function value. The point in the sample space with the highest probability of improvement, based on the value predicted by the surrogate function, is chosen as the next point for evaluating through the expensive method. However in this searching scheme we look only at the probability improvement and not the extent of improvement. This might lead it to get stuck in a local minima. Instead we can turn to the Expected value of improvement wherein we consider the extent of improvement as well. . | Expected improvement (EI) . | Upper confidence bound (UCB) . | Optimization strategy . Following strategy is followed when optimizing using Bayesian optimization: Find the next sampling point $ mathbf{x}_t$ by optimizing the acquisition function over a surrogate model (in this case a GP) fit over a distribution $ mathcal{D}_{1:t-1}$ | Evaluate $f$ at $f(x_{t})$ i.e. sample $f(x_{t})$ from the $f$ | Add the new point to the prior of the GP now $ mathcal{D}_{1:t} = ( mathcal{D}_{1:t-1}, (x_{t},f(x_{t})) )$ | . Expected improvement . Expected improvement is defined as: $$ mathrm{EI}( mathbf{x}) = max((f( mathbf{x}) - f( mathbf{x}^+), 0)) tag{1}$$ . where $f( mathbf{x}^+)$ is the value of the best sample so far and $ mathbf{x}^+$ is the location of that sample i.e. $ mathbf{x}^+ = mathrm{argmax}_{ mathbf{x}_i in mathbf{x}_{1:t}} f( mathbf{x}_i)$. The expected improvement can be evaluated analytically under the GP model . Expected improvement can be evaluated analytically for a GP model. . Implementation with Numpy and Scipy . import matplotlib.pyplot as plt import numpy as np %config InlineBackend.figure_format = &#39;retina&#39; # Plot matplotlib plots with white background: %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} from sklearn.gaussian_process import GaussianProcessRegressor as GPR from sklearn.gaussian_process import kernels . from Bayesian_optimization import plotting_utils, acquisition, objectives, opti . plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . ## Import the acquisition functions implemented . EI = acquisition.ExpectedImprovement(delta = 0.01) LCB = acquisition.LowerConfidenceBound(sigma = 1.96) . A One-Dimensional Example . Egg-carton objective function . objective = objectives.egg_carton (low, high) = (0.0, 10.0) domain = np.array([[low], [high]]) x_pts = np.linspace(low, high, 1000).reshape(-1, 1) y_pts = objective(x_pts) . num_sample_points = 10 noise_ = 0.1 generator = np.random.default_rng(42) x_sample = generator.uniform(low, high, size = (num_sample_points, 1)) y_sample = objective(x_sample, noise_) . fig, ax = plt.subplots(1,1, figsize=(8,3)) ax.plot(x_pts, y_pts, &#39;k-&#39;, linewidth=2.0, label=&#39;Ground truth&#39;) ax.plot(x_sample, y_sample, &#39;ro&#39;, label=&#39;Noisy sampled points&#39;) ax.set_xlabel(r&quot;$x$&quot;, fontsize = 20) ax.set_ylabel(r&quot;$f(x)$&quot;, fontsize = 20) ax.set_title(&quot;Initial setup&quot;, fontsize = 20) ax.legend(fontsize = 15) ax.grid(True) . Fit a GPR model (surrogate function) to the sampled points . constant = kernels.ConstantKernel() matern = kernels.Matern(nu = 2.5) rbf = kernels.RBF() gpr_model = GPR(kernel = constant*rbf, alpha = 1e-3, n_restarts_optimizer = 20, normalize_y = False, random_state = 42) gpr_model.fit(x_sample, y_sample) . GaussianProcessRegressor(alpha=0.001, kernel=1**2 * RBF(length_scale=1), n_restarts_optimizer=20, random_state=42) . (mean_pred, stddev_pred) = gpr_model.predict(x_pts, return_std = True) gpr_model.kernel_ . 2.78**2 * RBF(length_scale=0.782) . Plot the Initial Sample . (fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB) . Run a Few Iterations and Assess . pkwargs = {&quot;num_sample&quot;: 10, &quot;num_improve&quot;: 5, &quot;generator&quot;: generator} res_ec, _ = opti.bayesian_optimization(objective, gpr_model, LCB, domain, max_iter=10, noise=noise_, prop_kwargs = pkwargs) gpr_model.fit(res_ec[&quot;X&quot;], res_ec[&quot;y&quot;]) # Incorporate final point into plots. . 10 . GaussianProcessRegressor(alpha=0.001, kernel=1**2 * RBF(length_scale=1), n_restarts_optimizer=20, random_state=42) . (fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB, num_sample_points) . fig_ec . Run a Few More Iterations . res_ec, _ = opti.bayesian_optimization(objective, gpr_model, LCB, domain, noise=noise_, prop_kwargs = pkwargs) gpr_model.fit(res_ec[&quot;X&quot;], res_ec[&quot;y&quot;]) # Incorporate final point into plots. . 20 . GaussianProcessRegressor(alpha=0.001, kernel=1**2 * RBF(length_scale=1), n_restarts_optimizer=20, random_state=42) . (fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB, num_sample_points) . In total the noisy estimation of the ground-truth is conducted on 30 additional points. It is evident from the plot that most of those points are near the x = (4,6) since that is the minimum value region for the function. .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2021/01/27/Bayesian_Optimization.html",
            "relUrl": "/python/machine-learning/2021/01/27/Bayesian_Optimization.html",
            "date": " • Jan 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Estimating prediction confidence through dropout",
            "content": "Adapted from Deep Learning online course notes from NYU. Note link | Paper about using Dropout as a Bayesian Approximation | . Another notebook which uses PyTorch dropout: Link . New paper on evidential deep learning for guided molecular property prediction | . In addition to predicting a value from a model it is also important to know the confidence in that prediction. Dropout is one way of estimating this. After multiple rounds of predictions, the mean and standard deviation in the prediction can be viewed as the prediction value and the corresponding confidence in the prediction. It is important to note that this is different from the error in the prediction. The model may have error in the prediction but could be precise in that value. It is similar to the idea of accuracy vs precision. . When done with dropout -- the weights in the NN are scale by $ frac{1}{1-r}$ to account for dropping of the weights . Type of uncertainties: Aleaotric and Epistemic uncertainty . Aleatoric uncertainty captures noise inherent in the observations | Epistemic uncertainty accounts for uncertainty in the model | . The ideal way to measure epistemic uncertainty is to train many different models, each time using a different random seed and possibly varying hyperparameters. Then use all of them for each input and see how much the predictions vary. This is very expensive to do, since it involves repeating the whole training process many times. Fortunately, we can approximate the same effect in a less expensive way: by using dropout -- effectively training a huge ensemble of different models all at once. Each training sample is evaluated with a different dropout mask, corresponding to a different random subset of the connections in the full model. Usually we only perform dropout during training and use a single averaged mask for prediction. But instead, let&#39;s use dropout for prediction too. We can compute the output for lots of different dropout masks, then see how much the predictions vary. This turns out to give a reasonable estimate of the epistemic uncertainty in the outputs . import torch from torch import nn, optim import numpy as np . import matplotlib.pyplot as plt from matplotlib.pyplot import cm %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . print(torch.cuda.device_count()) # Device configuration device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) . 0 cpu . m = 40 x = (torch.rand(m) - 0.5) * 20 #Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1) y = x * torch.sin(x) #y = 2 * torch.exp( - torch.sin( (x/2)**2 )) . fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.plot(x.numpy(), y.numpy(), &#39;o&#39;) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) ax.axis(&#39;equal&#39;); . class MLP(nn.Module): def __init__(self, hidden_layers=[20, 20], droprate=0.2, activation=&#39;relu&#39;): super(MLP, self).__init__() self.model = nn.Sequential() self.model.add_module(&#39;input&#39;, nn.Linear(1, hidden_layers[0])) if activation == &#39;relu&#39;: self.model.add_module(&#39;relu0&#39;, nn.ReLU()) elif activation == &#39;tanh&#39;: self.model.add_module(&#39;tanh0&#39;, nn.Tanh()) for i in range(len(hidden_layers)-1): self.model.add_module(&#39;dropout&#39;+str(i+1), nn.Dropout(p=droprate)) self.model.add_module(&#39;hidden&#39;+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1])) if activation == &#39;relu&#39;: self.model.add_module(&#39;relu&#39;+str(i+1), nn.ReLU()) elif activation == &#39;tanh&#39;: self.model.add_module(&#39;tanh&#39;+str(i+1), nn.Tanh()) self.model.add_module(&#39;dropout&#39;+str(i+2), nn.Dropout(p=droprate)) self.model.add_module(&#39;final&#39;, nn.Linear(hidden_layers[i+1], 1)) def forward(self, x): return self.model(x) . net = MLP(hidden_layers=[200, 100, 80], droprate=0.1).to(device) #Move model to the GPU print(net) . MLP( (model): Sequential( (input): Linear(in_features=1, out_features=200, bias=True) (relu0): ReLU() (dropout1): Dropout(p=0.1, inplace=False) (hidden1): Linear(in_features=200, out_features=100, bias=True) (relu1): ReLU() (dropout2): Dropout(p=0.1, inplace=False) (hidden2): Linear(in_features=100, out_features=80, bias=True) (relu2): ReLU() (dropout3): Dropout(p=0.1, inplace=False) (final): Linear(in_features=80, out_features=1, bias=True) ) ) . criterion = nn.MSELoss() optimizer = optim.Adam(net.parameters(), lr=0.005, weight_decay=0.00001) . x_dev = x.view(-1, 1).to(device) . for epoch in range(6000): x_dev = x.view(-1, 1).to(device) y_dev = y.view(-1, 1).to(device) y_hat = net(x_dev) loss = criterion(y_hat, y_dev) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print(&#39;Epoch[{}] - Loss:{}&#39;.format(epoch, loss.item())) . Epoch[0] - Loss:12.69531536102295 Epoch[500] - Loss:1.740363359451294 Epoch[1000] - Loss:1.177356243133545 Epoch[1500] - Loss:1.9534406661987305 Epoch[2000] - Loss:1.0815060138702393 Epoch[2500] - Loss:0.5457165837287903 Epoch[3000] - Loss:0.24786725640296936 Epoch[3500] - Loss:0.38788101077079773 Epoch[4000] - Loss:0.7004671096801758 Epoch[4500] - Loss:0.4352916181087494 Epoch[5000] - Loss:0.5015718936920166 Epoch[5500] - Loss:0.44577136635780334 . Define a separate continuous vector XX . XX = torch.linspace(-11, 11, 1000) . def predict_reg(model, X, T=10): &#39;&#39;&#39; Running the model in training mode. model = torch.model: NN implemented in pytorch X = torch.tensor: Input vector T = int: number of samples run OUT: Y_hat = sample of predictions from NN model Y_eval = average prediction value from NN model &#39;&#39;&#39; model = model.train() Y_hat = list() with torch.no_grad(): for t in range(T): X_out = model(X.view(-1,1).to(device)) Y_hat.append(X_out.cpu().squeeze()) Y_hat = torch.stack(Y_hat) model = model.eval() with torch.no_grad(): X_out = model(X.view(-1,1).to(device)) Y_eval = X_out.cpu().squeeze() return Y_hat, Y_eval . %%time y_hat, y_eval = predict_reg(net, XX, T=1000) mean_y_hat = y_hat.mean(axis=0) std_y_hat = y_hat.std(axis=0) . CPU times: user 11.9 s, sys: 94 ms, total: 12 s Wall time: 3.04 s . fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(XX.numpy(), mean_y_hat.numpy(), &#39;C1&#39;, label=&#39;prediction&#39;) ax.fill_between(XX.numpy(), (mean_y_hat + std_y_hat).numpy(), (mean_y_hat - std_y_hat).numpy(), alpha=0.5, color=&#39;C2&#39;, label=&#39;confidence&#39;) ax.plot(x.numpy(), y.numpy(), &#39;oC0&#39;, zorder=1, label=&#39;ground truth&#39;) ax.plot(XX.numpy(), (XX * torch.sin(XX)).numpy(), &#39;k--&#39;, alpha=0.4, zorder=0, label=&#39;original function&#39;) ax.set_title(&#39;Plotting the NN predictions and ground truth&#39;) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) ax.axis(&#39;equal&#39;) plt.legend(loc=&#39;best&#39;, fontsize=10) . &lt;matplotlib.legend.Legend at 0x7fb0b4eb73a0&gt; . The red line represents the predictions. The purple shaded region around the predictions represent the uncertainty i.e. variance of predictions. . Looking at the plot we can see the region in space where the neural network model does a good job and where it fails to capture the function. The orange line tracks the NN predictions that follows the dashed black line which depicts the original functions. The NN does a poor job in the region where the data points are not provided in the NN training. Not surprising, but good to visualize the model pitfalls. Coming the model confidence, the green bars show the confidence interval for model predictions. The variance in the prediction (seen by larger green window) is present for points away from the training points. This shows that model is uncertain outside the domain of training data. .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/pytorch/machine-learning/2021/01/11/Simple_Dropout.html",
            "relUrl": "/python/pytorch/machine-learning/2021/01/11/Simple_Dropout.html",
            "date": " • Jan 11, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Relational analysis of spices used in Indian cuisine",
            "content": "Find out which relations between different Indian spices . Spices are central to Indian cuisine. What is referred to colloquially as ‘Indian’ food is made of many different sub-cuisines. As a result, there are a plethora of spices usually brought up when considering ‘Indian’ food. Knowing which spices are most frequently used can help cooks novice or seasoned to make an informed decision about spices that promise the most bang for the buck. . I use a Kaggle dataset containing 6000+ recipes from https://www.archanaskitchen.com/. Using this data as base collection of recipes representing most of the indian food, I analyze which spices occur most freqeuntly and which spices are most connected to each other. . Dataset for Indian recipe: This dataset 6000+ recipe scrapped from | Link to the dataset | . import pandas as pd import numpy as np . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . Read the dataset . food_df = pd.read_csv(&#39;./data/IndianFoodDatasetCSV.csv&#39;) . food_df.columns . Index([&#39;Srno&#39;, &#39;RecipeName&#39;, &#39;TranslatedRecipeName&#39;, &#39;Ingredients&#39;, &#39;TranslatedIngredients&#39;, &#39;PrepTimeInMins&#39;, &#39;CookTimeInMins&#39;, &#39;TotalTimeInMins&#39;, &#39;Servings&#39;, &#39;Cuisine&#39;, &#39;Course&#39;, &#39;Diet&#39;, &#39;Instructions&#39;, &#39;TranslatedInstructions&#39;, &#39;URL&#39;], dtype=&#39;object&#39;) . food_df.shape . (6871, 15) . columns_to_drop = [&#39;CookTimeInMins&#39;, &#39;Servings&#39;, &#39;Course&#39;, &#39;Diet&#39;, &#39;Instructions&#39;, &#39;TranslatedInstructions&#39;, &#39;URL&#39;] food_df = food_df.drop(columns = columns_to_drop).dropna() . cuisines_to_drop = [&#39;Mexican&#39;, &#39;Italian Recipes&#39;, &#39;Thai&#39;, &#39;Chinese&#39;, &#39;Asian&#39;, &#39;Middle Eastern&#39;, &#39;European&#39;, &#39;Arab&#39;, &#39;Japanese&#39;, &#39;Vietnamese&#39;, &#39;British&#39;, &#39;Greek&#39;, &#39;French&#39;, &#39;Mediterranean&#39;, &#39;Sri Lankan&#39;, &#39;Indonesian&#39;, &#39;African&#39;, &#39;Korean&#39;, &#39;American&#39;, &#39;Carribbean&#39;, &#39;World Breakfast&#39;, &#39;Malaysian&#39;, &#39;Dessert&#39;, &#39;Afghan&#39;, &#39;Snack&#39;, &#39;Jewish&#39;, &#39;Brunch&#39;, &#39;Lunch&#39;, &#39;Continental&#39;, &#39;Fusion&#39;] food_df = food_df.loc[ ~ food_df[&#39;Cuisine&#39;].isin(cuisines_to_drop) ] #Dropping entries in `food_df` which have non-indian cuisines . food_df.shape . (4881, 8) . food_df.head(5) . Srno RecipeName TranslatedRecipeName Ingredients TranslatedIngredients PrepTimeInMins TotalTimeInMins Cuisine . 0 1 | Masala Karela Recipe | Masala Karela Recipe | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | 15 | 45 | Indian | . 1 2 | टमाटर पुलियोगरे रेसिपी - Spicy Tomato Rice (Re... | Spicy Tomato Rice (Recipe) | 2-1/2 कप चावल - पका ले,3 टमाटर,3 छोटा चमच्च बी... | 2-1 / 2 cups rice - cooked, 3 tomatoes, 3 teas... | 5 | 15 | South Indian Recipes | . 2 3 | Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | 20 | 50 | South Indian Recipes | . 3 4 | Gongura Chicken Curry Recipe - Andhra Style Go... | Gongura Chicken Curry Recipe - Andhra Style Go... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | 15 | 45 | Andhra | . 4 5 | आंध्रा स्टाइल आलम पचड़ी रेसिपी - Adrak Chutney ... | Andhra Style Alam Pachadi Recipe - Adrak Chutn... | 1 बड़ा चमच्च चना दाल,1 बड़ा चमच्च सफ़ेद उरद दाल,2... | 1 tablespoon chana dal, 1 tablespoon white ura... | 10 | 30 | Andhra | . Drop non-english entries for consistency . def filter_english(string): try: string.encode(&#39;utf-8&#39;).decode(&#39;ascii&#39;) out = True except UnicodeDecodeError: out = False return out . df = food_df.loc[ food_df[&#39;TranslatedIngredients&#39;].apply(filter_english) ] . df.shape . (4273, 8) . df = df.reset_index() . Generate a consistent list of Indian spices for better tabulation . Next for consistent tabulation I needed a list of spices to look for. Wikipedia has a page on Indian spices which lists various spices used in Indian cuisine. I use this list to search names of spices in the recipe entries. . wiki_file_pd = pd.read_html(&#39;https://en.wikipedia.org/wiki/List_of_Indian_spices&#39;) spices_list = wiki_file_pd[0][&#39;Standard English&#39;].copy().str.lower() #some important spices to add spices_to_add = pd.Series([&#39;black salt&#39;, &#39;green chillies&#39;, &#39;chilli powder&#39;]) #some spices are too common (such as pepper) or not a spice, but a vegetable, or are otherwise corrupted (for example, #cardamom is often listed as &quot;cardamom&quot; nto specifying whether it is black or green) spices_to_drop = [&#39;black pepper&#39;, &#39;capers&#39;, &#39;chili pepper powder&#39;, &#39;cinnamon buds&#39;, &#39;citric acid&#39;, &#39;garlic&#39;, &#39;capsicum&#39;, &#39;charoli&#39;, &#39;garcinia gummi-gutta&#39;, &#39;inknut&#39;, &#39;garcinia indica&#39;, &#39;black mustard seeds/raee&#39;, &#39;cumin seed ground into balls&#39;, &#39;dried ginger&#39;, &#39;green chili pepper&#39;, &#39;long pepper&#39;, &#39;four seeds&#39;, &#39;cubeb&#39;, &#39;gum tragacanth&#39;, &#39;jakhya&#39;, &#39;licorice powder&#39;, &#39;indian bedellium tree&#39;, &#39;mango extract&#39;, &#39;coriander powder&#39;, &#39;saffron pulp&#39;, &#39;black cardamom&#39;, &#39;brown mustard seed&#39;, &#39;black cumin&#39;, &#39;panch phoron&#39;] spices_list = spices_list.loc[ ~spices_list.isin(spices_to_drop) ].append(spices_to_add).reset_index(drop=True) . spices_list . 0 alkanet root 1 amchoor 2 asafoetida 3 celery / radhuni seed 4 bay leaf, indian bay leaf 5 cinnamon 6 cloves 7 coriander seed 8 cumin seed 9 curry tree or sweet neem leaf 10 fennel seed 11 fenugreek leaf 12 fenugreek seed 13 garam masala 14 ginger 15 green cardamom 16 indian gooseberry 17 kalpasi 18 mustard seed 19 nigella seed 20 nutmeg 21 mace 22 pomegranate seed 23 poppy seed 24 saffron 25 sesame seed 26 star aniseh 27 tamarind 28 thymol/carom seed 29 turmeric 30 white pepper 31 black salt 32 green chillies 33 chilli powder dtype: object . One more step is editing the spices so that my string counter can find different versions of the same spice. . spices_list = spices_list.str.replace(&#39;amchoor&#39;, &#39;amchur/amchoor/mango extract&#39;) .replace(&#39;asafoetida&#39;, &#39;asafetida/asafoetida/hing&#39;) .replace(&#39;thymol/carom seed&#39;, &#39;ajwain/thymol/carom seed&#39;) .replace(&#39;alkanet root&#39;, &#39;alkanet/alkanet root&#39;) .replace(&#39;chilli powder&#39;, &#39;red chilli powder/chilli powder/kashmiri red chilli powder&#39;) .replace(&#39;celery / radhuni seed&#39;, &#39;celery/radhuni seed&#39;) .replace(&#39;bay leaf, indian bay leaf&#39;, &#39;bay leaf/bay leaves/tej patta&#39;) .replace(&#39;curry tree or sweet neem leaf&#39;, &#39;curry leaf/curry leaves&#39;) .replace(&#39;fenugreek leaf&#39;, &#39;fenugreek/kasoori methi&#39;) .replace(&#39;nigella seed&#39;, &#39;nigella/black cumin&#39;) .replace(&#39;ginger&#39;, &#39;dried ginger/ginger powder&#39;) .replace(&#39;cloves&#39;, &#39;cloves/laung&#39;) .replace(&#39;green cardamom&#39;, &#39;cardamom/green cardamom/black cardamom&#39;) .replace(&#39;indian gooseberry&#39;, &#39;indian gooseberry/amla&#39;) .replace(&#39;coriander seed&#39;, &#39;coriander seed/coriander powder&#39;) .replace(&#39;star aniseh&#39;, &#39;star anise&#39;) .replace(&#39;cumin seed&#39;, &#39;cumin powder/cumin seeds/cumin/jeera&#39;) . spices_list . 0 alkanet/alkanet root 1 amchur/amchoor/mango extract 2 asafetida/asafoetida/hing 3 celery/radhuni seed 4 bay leaf/bay leaves/tej patta 5 cinnamon 6 cloves/laung 7 coriander seed/coriander powder 8 cumin powder/cumin seeds/cumin/jeera 9 curry leaf/curry leaves 10 fennel seed 11 fenugreek/kasoori methi 12 fenugreek seed 13 garam masala 14 dried ginger/ginger powder 15 cardamom/green cardamom/black cardamom 16 indian gooseberry/amla 17 kalpasi 18 mustard seed 19 nigella/black cumin 20 nutmeg 21 mace 22 pomegranate seed 23 poppy seed 24 saffron 25 sesame seed 26 star anise 27 tamarind 28 ajwain/thymol/carom seed 29 turmeric 30 white pepper 31 black salt 32 green chillies 33 red chilli powder/chilli powder/kashmiri red c... dtype: object . Ingredients in the recipes . ingredients_series = df[[&#39;TranslatedRecipeName&#39;,&#39;TranslatedIngredients&#39;]] . ingredients_series . TranslatedRecipeName TranslatedIngredients . 0 Masala Karela Recipe | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | . 1 Spicy Tomato Rice (Recipe) | 2-1 / 2 cups rice - cooked, 3 tomatoes, 3 teas... | . 2 Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | . 3 Gongura Chicken Curry Recipe - Andhra Style Go... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | . 4 Andhra Style Alam Pachadi Recipe - Adrak Chutn... | 1 tablespoon chana dal, 1 tablespoon white ura... | . ... ... | ... | . 4268 One Pot Punjabi Rajma Masala Recipe In Preethi... | 1 cup Rajma (Large Kidney Beans),1 inch Ginger... | . 4269 Saffron Paneer Peda Recipe | 2 cups Paneer (Homemade Cottage Cheese) - crum... | . 4270 Quinoa Phirnee Recipe (Quinoa Milk Pudding) | 1 cup Quinoa,3/4 cup Sugar,1 teaspoon Cardamom... | . 4271 Ullikadala Pulusu Recipe | Spring Onion Curry | 150 grams Spring Onion (Bulb &amp; Greens) - chopp... | . 4272 Kashmiri Style Kokur Yakhni Recipe-Chicken Coo... | 1 kg Chicken - medium pieces,1/2 cup Mustard o... | . 4273 rows × 2 columns . spices_list_column_to_add = {i: np.zeros(len(ingredients_series)) for i in spices_list.to_list()} . ingredients_series = ingredients_series.join(pd.DataFrame(spices_list_column_to_add)) . ingredients_series . TranslatedRecipeName TranslatedIngredients alkanet/alkanet root amchur/amchoor/mango extract asafetida/asafoetida/hing celery/radhuni seed bay leaf/bay leaves/tej patta cinnamon cloves/laung coriander seed/coriander powder ... saffron sesame seed star anise tamarind ajwain/thymol/carom seed turmeric white pepper black salt green chillies red chilli powder/chilli powder/kashmiri red chilli powder . 0 Masala Karela Recipe | 6 Karela (Bitter Gourd/ Pavakkai) - deseeded,S... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 Spicy Tomato Rice (Recipe) | 2-1 / 2 cups rice - cooked, 3 tomatoes, 3 teas... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 Ragi Semiya Upma Recipe - Ragi Millet Vermicel... | 1-1/2 cups Rice Vermicelli Noodles (Thin),1 On... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 Gongura Chicken Curry Recipe - Andhra Style Go... | 500 grams Chicken,2 Onion - chopped,1 Tomato -... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 Andhra Style Alam Pachadi Recipe - Adrak Chutn... | 1 tablespoon chana dal, 1 tablespoon white ura... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4268 One Pot Punjabi Rajma Masala Recipe In Preethi... | 1 cup Rajma (Large Kidney Beans),1 inch Ginger... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4269 Saffron Paneer Peda Recipe | 2 cups Paneer (Homemade Cottage Cheese) - crum... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4270 Quinoa Phirnee Recipe (Quinoa Milk Pudding) | 1 cup Quinoa,3/4 cup Sugar,1 teaspoon Cardamom... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4271 Ullikadala Pulusu Recipe | Spring Onion Curry | 150 grams Spring Onion (Bulb &amp; Greens) - chopp... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4272 Kashmiri Style Kokur Yakhni Recipe-Chicken Coo... | 1 kg Chicken - medium pieces,1/2 cup Mustard o... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4273 rows × 36 columns . Using the spice_list to find spice name in the recipe . I used regular expression to search for spice names in the entries . import re def search_spice(ingredient_string, spice_string): &#39;&#39;&#39; Check if a spice exists in the list of ingredients for a recipe &#39;&#39;&#39; spice_list = spice_string.split(&#39;/&#39;) for _spice in spice_list: if re.search(_spice.lower(), ingredient_string.lower()): return True break . for row, values in ingredients_series.iterrows(): for spice_entry in spices_list: if search_spice(values[&#39;TranslatedIngredients&#39;], spice_entry): ingredients_series.loc[row, spice_entry] = 1 else: ingredients_series.loc[row, spice_entry] = 0 . food_spice_mix = ingredients_series.drop([&#39;TranslatedIngredients&#39;], axis=1).reset_index(drop=True) . food_spice_mix.rename(columns={&#39;amchur/amchoor/mango extract&#39;:&#39;amchoor&#39;, &#39;asafetida/asafoetida/hing&#39;: &#39;asafoetida&#39;, &#39;ajwain/thymol/carom seed&#39;: &#39;ajwain&#39;, &#39;alkanet/alkanet root&#39;: &#39;alkanet root&#39;, &#39;red chilli powder/chilli powder/kashmiri red chilli powder&#39;: &#39;chilli powder&#39;, &#39;celery/radhuni seed&#39;: &#39;celery seeds&#39;, &#39;bay leaf/bay leaves/tej patta&#39;: &#39;bay leaf&#39;, &#39;curry leaf/curry leaves&#39;: &#39;curry leaves&#39;, &#39;fenugreek/kasoori methi&#39;: &#39;fenugreek leaf&#39;, &#39;nigella/black cumin&#39;: &#39;nigella seed&#39;, &#39;ginger&#39;: &#39;dried ginger&#39;, &#39;cloves/laung&#39;: &#39;cloves&#39;, &#39;cardamom/green cardamom/black cardamom&#39;: &#39;cardamom&#39;, &#39;indian gooseberry/amla&#39;: &#39;indian gooseberry&#39;, &#39;coriander seed/coriander powder&#39;: &#39;coriander seeds/powder&#39;, &#39;cumin powder/cumin seeds/cumin/jeera&#39;: &#39;cumin seeds/powder&#39;, &#39;dried ginger/ginger powder&#39;: &#39;ginger powder&#39;}, inplace=True) . food_spice_mix.columns . Index([&#39;TranslatedRecipeName&#39;, &#39;alkanet root&#39;, &#39;amchoor&#39;, &#39;asafoetida&#39;, &#39;celery seeds&#39;, &#39;bay leaf&#39;, &#39;cinnamon&#39;, &#39;cloves&#39;, &#39;coriander seeds/powder&#39;, &#39;cumin seeds/powder&#39;, &#39;curry leaves&#39;, &#39;fennel seed&#39;, &#39;fenugreek leaf&#39;, &#39;fenugreek seed&#39;, &#39;garam masala&#39;, &#39;ginger powder&#39;, &#39;cardamom&#39;, &#39;indian gooseberry&#39;, &#39;kalpasi&#39;, &#39;mustard seed&#39;, &#39;nigella seed&#39;, &#39;nutmeg&#39;, &#39;mace&#39;, &#39;pomegranate seed&#39;, &#39;poppy seed&#39;, &#39;saffron&#39;, &#39;sesame seed&#39;, &#39;star anise&#39;, &#39;tamarind&#39;, &#39;ajwain&#39;, &#39;turmeric&#39;, &#39;white pepper&#39;, &#39;black salt&#39;, &#39;green chillies&#39;, &#39;chilli powder&#39;], dtype=&#39;object&#39;) . food_spice_mix = food_spice_mix.sort_index(axis=1) . Generating a spice adjacency matrix . num_spice = len(spices_list) spice_col_name = [i for i in food_spice_mix.columns[1:].to_list()] spice_adj = pd.DataFrame(np.zeros(shape=(len(spices_list),len(spices_list))), columns= spice_col_name, index=spice_col_name) spice_adj_freq = pd.DataFrame(np.zeros(shape=(len(spices_list),len(spices_list))), columns= spice_col_name, index=spice_col_name) . for row, value in food_spice_mix.iterrows(): for i in spice_col_name: for j in spice_col_name: if (value[i] == 1) &amp; (value[j] == 1): spice_adj_freq.loc[i,j] += 1 spice_adj.loc[i,j] = 1 . Normalize the spice occurance frequency with the total entries in the main dataset . spice_adj_freq = spice_adj_freq / len(food_spice_mix) * 100 . spice_adj_freq.round(2) . ajwain alkanet root amchoor asafoetida bay leaf black salt cardamom celery seeds chilli powder cinnamon ... nigella seed nutmeg pomegranate seed poppy seed saffron sesame seed star anise tamarind turmeric white pepper . ajwain 5.22 | 0.00 | 0.70 | 1.45 | 0.98 | 0.07 | 1.05 | 0.00 | 3.49 | 1.33 | ... | 0.16 | 0.16 | 0.09 | 0.23 | 0.21 | 0.40 | 0.23 | 0.37 | 2.97 | 0.00 | . alkanet root 0.00 | 0.07 | 0.00 | 0.07 | 0.05 | 0.00 | 0.07 | 0.00 | 0.07 | 0.07 | ... | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . amchoor 0.70 | 0.00 | 4.98 | 1.54 | 0.37 | 0.21 | 0.35 | 0.02 | 3.86 | 0.47 | ... | 0.35 | 0.00 | 0.14 | 0.02 | 0.05 | 0.26 | 0.05 | 0.33 | 3.51 | 0.02 | . asafoetida 1.45 | 0.07 | 1.54 | 24.60 | 1.33 | 0.30 | 1.61 | 0.21 | 9.69 | 1.87 | ... | 0.30 | 0.14 | 0.09 | 0.54 | 0.28 | 1.57 | 0.26 | 5.17 | 15.05 | 0.02 | . bay leaf 0.98 | 0.05 | 0.37 | 1.33 | 10.70 | 0.09 | 6.04 | 0.07 | 6.72 | 6.65 | ... | 0.26 | 0.61 | 0.12 | 0.75 | 0.66 | 0.16 | 1.10 | 0.26 | 7.54 | 0.07 | . black salt 0.07 | 0.00 | 0.21 | 0.30 | 0.09 | 1.64 | 0.09 | 0.02 | 0.61 | 0.09 | ... | 0.02 | 0.00 | 0.09 | 0.00 | 0.09 | 0.02 | 0.02 | 0.28 | 0.33 | 0.00 | . cardamom 1.05 | 0.07 | 0.35 | 1.61 | 6.04 | 0.09 | 17.79 | 0.14 | 6.13 | 7.98 | ... | 0.07 | 1.08 | 0.23 | 1.47 | 3.14 | 0.35 | 1.29 | 0.42 | 6.60 | 0.07 | . celery seeds 0.00 | 0.00 | 0.02 | 0.21 | 0.07 | 0.02 | 0.14 | 0.80 | 0.37 | 0.19 | ... | 0.00 | 0.09 | 0.00 | 0.00 | 0.02 | 0.00 | 0.02 | 0.00 | 0.51 | 0.00 | . chilli powder 3.49 | 0.07 | 3.86 | 9.69 | 6.72 | 0.61 | 6.13 | 0.37 | 37.96 | 7.04 | ... | 0.84 | 0.44 | 0.47 | 1.38 | 0.68 | 1.59 | 0.82 | 3.58 | 28.39 | 0.05 | . cinnamon 1.33 | 0.07 | 0.47 | 1.87 | 6.65 | 0.09 | 7.98 | 0.19 | 7.04 | 13.13 | ... | 0.14 | 0.96 | 0.19 | 1.40 | 0.84 | 0.23 | 1.38 | 0.89 | 8.10 | 0.07 | . cloves 2.01 | 0.07 | 1.43 | 5.05 | 6.58 | 0.16 | 7.65 | 0.19 | 14.14 | 9.29 | ... | 0.35 | 0.70 | 0.23 | 1.59 | 0.80 | 0.96 | 1.43 | 3.93 | 17.29 | 0.02 | . coriander seeds/powder 1.76 | 0.00 | 2.41 | 5.13 | 4.91 | 0.26 | 4.31 | 0.47 | 16.26 | 4.35 | ... | 0.35 | 0.30 | 0.28 | 0.70 | 0.33 | 0.56 | 0.59 | 1.85 | 16.78 | 0.00 | . cumin seeds/powder 2.15 | 0.00 | 3.28 | 13.27 | 5.87 | 1.19 | 5.20 | 0.35 | 21.55 | 6.67 | ... | 0.89 | 0.54 | 0.51 | 1.24 | 0.54 | 1.92 | 0.91 | 5.87 | 27.45 | 0.07 | . curry leaves 0.61 | 0.00 | 0.44 | 12.54 | 1.01 | 0.12 | 1.17 | 0.05 | 8.64 | 2.04 | ... | 0.16 | 0.07 | 0.07 | 0.66 | 0.02 | 1.73 | 0.28 | 7.33 | 16.52 | 0.05 | . fennel seed 0.75 | 0.05 | 0.82 | 1.68 | 1.45 | 0.16 | 1.99 | 0.07 | 3.77 | 2.46 | ... | 0.70 | 0.26 | 0.09 | 0.91 | 0.42 | 0.37 | 0.75 | 0.82 | 4.21 | 0.00 | . fenugreek leaf 0.87 | 0.00 | 0.80 | 4.54 | 1.64 | 0.12 | 1.52 | 0.12 | 6.39 | 1.73 | ... | 0.70 | 0.02 | 0.16 | 0.37 | 0.07 | 0.89 | 0.26 | 3.23 | 8.71 | 0.00 | . fenugreek seed 0.14 | 0.00 | 0.47 | 3.16 | 0.35 | 0.07 | 0.37 | 0.07 | 2.36 | 0.61 | ... | 0.61 | 0.00 | 0.05 | 0.30 | 0.02 | 0.59 | 0.07 | 2.93 | 4.52 | 0.00 | . garam masala 1.43 | 0.00 | 2.29 | 3.18 | 4.40 | 0.26 | 3.98 | 0.28 | 14.07 | 4.00 | ... | 0.33 | 0.14 | 0.30 | 0.63 | 0.49 | 0.42 | 0.44 | 0.49 | 13.25 | 0.00 | . ginger powder 0.21 | 0.02 | 0.05 | 0.44 | 0.35 | 0.05 | 0.84 | 0.02 | 0.59 | 0.49 | ... | 0.00 | 0.19 | 0.02 | 0.02 | 0.12 | 0.07 | 0.00 | 0.07 | 0.47 | 0.00 | . green chillies 1.80 | 0.00 | 1.59 | 7.61 | 4.61 | 0.28 | 3.96 | 0.21 | 12.22 | 4.84 | ... | 0.77 | 0.19 | 0.33 | 1.19 | 0.33 | 1.08 | 0.66 | 2.36 | 17.11 | 0.07 | . indian gooseberry 0.05 | 0.00 | 0.00 | 0.30 | 0.00 | 0.02 | 0.00 | 0.02 | 0.16 | 0.00 | ... | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.00 | 0.05 | 0.21 | 0.00 | . kalpasi 0.02 | 0.00 | 0.00 | 0.02 | 0.02 | 0.00 | 0.07 | 0.00 | 0.02 | 0.07 | ... | 0.00 | 0.02 | 0.00 | 0.07 | 0.00 | 0.00 | 0.05 | 0.00 | 0.00 | 0.00 | . mace 0.37 | 0.00 | 0.02 | 0.09 | 0.82 | 0.00 | 1.08 | 0.05 | 0.80 | 1.05 | ... | 0.00 | 0.49 | 0.02 | 0.26 | 0.14 | 0.00 | 0.30 | 0.00 | 0.89 | 0.00 | . mustard seed 0.75 | 0.00 | 0.87 | 12.71 | 1.15 | 0.07 | 0.73 | 0.02 | 8.03 | 1.59 | ... | 0.70 | 0.09 | 0.07 | 0.80 | 0.05 | 1.64 | 0.28 | 6.74 | 15.35 | 0.00 | . nigella seed 0.16 | 0.00 | 0.35 | 0.30 | 0.26 | 0.02 | 0.07 | 0.00 | 0.84 | 0.14 | ... | 1.66 | 0.00 | 0.02 | 0.16 | 0.02 | 0.07 | 0.02 | 0.07 | 1.26 | 0.02 | . nutmeg 0.16 | 0.00 | 0.00 | 0.14 | 0.61 | 0.00 | 1.08 | 0.09 | 0.44 | 0.96 | ... | 0.00 | 1.52 | 0.00 | 0.30 | 0.23 | 0.07 | 0.28 | 0.05 | 0.56 | 0.00 | . pomegranate seed 0.09 | 0.00 | 0.14 | 0.09 | 0.12 | 0.09 | 0.23 | 0.00 | 0.47 | 0.19 | ... | 0.02 | 0.00 | 0.77 | 0.05 | 0.07 | 0.02 | 0.02 | 0.07 | 0.33 | 0.00 | . poppy seed 0.23 | 0.00 | 0.02 | 0.54 | 0.75 | 0.00 | 1.47 | 0.00 | 1.38 | 1.40 | ... | 0.16 | 0.30 | 0.05 | 3.25 | 0.19 | 0.26 | 0.37 | 0.42 | 1.80 | 0.02 | . saffron 0.21 | 0.00 | 0.05 | 0.28 | 0.66 | 0.09 | 3.14 | 0.02 | 0.68 | 0.84 | ... | 0.02 | 0.23 | 0.07 | 0.19 | 4.03 | 0.07 | 0.14 | 0.00 | 0.59 | 0.00 | . sesame seed 0.40 | 0.00 | 0.26 | 1.57 | 0.16 | 0.02 | 0.35 | 0.00 | 1.59 | 0.23 | ... | 0.07 | 0.07 | 0.02 | 0.26 | 0.07 | 4.19 | 0.07 | 0.82 | 1.61 | 0.00 | . star anise 0.23 | 0.00 | 0.05 | 0.26 | 1.10 | 0.02 | 1.29 | 0.02 | 0.82 | 1.38 | ... | 0.02 | 0.28 | 0.02 | 0.37 | 0.14 | 0.07 | 1.73 | 0.14 | 0.89 | 0.00 | . tamarind 0.37 | 0.00 | 0.33 | 5.17 | 0.26 | 0.28 | 0.42 | 0.00 | 3.58 | 0.89 | ... | 0.07 | 0.05 | 0.07 | 0.42 | 0.00 | 0.82 | 0.14 | 11.96 | 7.21 | 0.00 | . turmeric 2.97 | 0.00 | 3.51 | 15.05 | 7.54 | 0.33 | 6.60 | 0.51 | 28.39 | 8.10 | ... | 1.26 | 0.56 | 0.33 | 1.80 | 0.59 | 1.61 | 0.89 | 7.21 | 48.47 | 0.05 | . white pepper 0.00 | 0.00 | 0.02 | 0.02 | 0.07 | 0.00 | 0.07 | 0.00 | 0.05 | 0.07 | ... | 0.02 | 0.00 | 0.00 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.16 | . 34 rows × 34 columns . temp_name = [i.title() for i in spice_adj_freq.index.to_list()] spice_adj_freq[&#39;Plot_name&#39;] = temp_name . spice_adj_freq = spice_adj_freq.set_index(&#39;Plot_name&#39;) . spice_adj_freq.columns = temp_name . spice_adj_freq . Ajwain Alkanet Root Amchoor Asafoetida Bay Leaf Black Salt Cardamom Celery Seeds Chilli Powder Cinnamon ... Nigella Seed Nutmeg Pomegranate Seed Poppy Seed Saffron Sesame Seed Star Anise Tamarind Turmeric White Pepper . Plot_name . Ajwain 5.218816 | 0.000000 | 0.702083 | 1.450971 | 0.982916 | 0.070208 | 1.053124 | 0.000000 | 3.487011 | 1.333957 | ... | 0.163819 | 0.163819 | 0.093611 | 0.234028 | 0.210625 | 0.397847 | 0.234028 | 0.374444 | 2.972151 | 0.000000 | . Alkanet Root 0.000000 | 0.070208 | 0.000000 | 0.070208 | 0.046806 | 0.000000 | 0.070208 | 0.000000 | 0.070208 | 0.070208 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . Amchoor 0.702083 | 0.000000 | 4.984788 | 1.544582 | 0.374444 | 0.210625 | 0.351041 | 0.023403 | 3.861456 | 0.468055 | ... | 0.351041 | 0.000000 | 0.140417 | 0.023403 | 0.046806 | 0.257430 | 0.046806 | 0.327639 | 3.510414 | 0.023403 | . Asafoetida 1.450971 | 0.070208 | 1.544582 | 24.596302 | 1.333957 | 0.304236 | 1.614791 | 0.210625 | 9.688743 | 1.872221 | ... | 0.304236 | 0.140417 | 0.093611 | 0.538264 | 0.280833 | 1.567985 | 0.257430 | 5.172010 | 15.047976 | 0.023403 | . Bay Leaf 0.982916 | 0.046806 | 0.374444 | 1.333957 | 10.695062 | 0.093611 | 6.037912 | 0.070208 | 6.716593 | 6.646384 | ... | 0.257430 | 0.608472 | 0.117014 | 0.748888 | 0.655277 | 0.163819 | 1.099930 | 0.257430 | 7.535689 | 0.070208 | . Black Salt 0.070208 | 0.000000 | 0.210625 | 0.304236 | 0.093611 | 1.638193 | 0.093611 | 0.023403 | 0.608472 | 0.093611 | ... | 0.023403 | 0.000000 | 0.093611 | 0.000000 | 0.093611 | 0.023403 | 0.023403 | 0.280833 | 0.327639 | 0.000000 | . Cardamom 1.053124 | 0.070208 | 0.351041 | 1.614791 | 6.037912 | 0.093611 | 17.786099 | 0.140417 | 6.131524 | 7.980342 | ... | 0.070208 | 1.076527 | 0.234028 | 1.474374 | 3.135970 | 0.351041 | 1.287152 | 0.421250 | 6.599579 | 0.070208 | . Celery Seeds 0.000000 | 0.000000 | 0.023403 | 0.210625 | 0.070208 | 0.023403 | 0.140417 | 0.795694 | 0.374444 | 0.187222 | ... | 0.000000 | 0.093611 | 0.000000 | 0.000000 | 0.023403 | 0.000000 | 0.023403 | 0.000000 | 0.514861 | 0.000000 | . Chilli Powder 3.487011 | 0.070208 | 3.861456 | 9.688743 | 6.716593 | 0.608472 | 6.131524 | 0.374444 | 37.959279 | 7.044231 | ... | 0.842499 | 0.444652 | 0.468055 | 1.380763 | 0.678680 | 1.591388 | 0.819097 | 3.580623 | 28.387550 | 0.046806 | . Cinnamon 1.333957 | 0.070208 | 0.468055 | 1.872221 | 6.646384 | 0.093611 | 7.980342 | 0.187222 | 7.044231 | 13.128949 | ... | 0.140417 | 0.959513 | 0.187222 | 1.404166 | 0.842499 | 0.234028 | 1.380763 | 0.889305 | 8.097355 | 0.070208 | . Cloves 2.012637 | 0.070208 | 1.427568 | 5.054996 | 6.576176 | 0.163819 | 7.652703 | 0.187222 | 14.135268 | 9.290896 | ... | 0.351041 | 0.702083 | 0.234028 | 1.591388 | 0.795694 | 0.959513 | 1.427568 | 3.931664 | 17.294641 | 0.023403 | . Coriander Seeds/Powder 1.755207 | 0.000000 | 2.410484 | 5.125205 | 4.914580 | 0.257430 | 4.306108 | 0.468055 | 16.264919 | 4.352914 | ... | 0.351041 | 0.304236 | 0.280833 | 0.702083 | 0.327639 | 0.561666 | 0.585069 | 1.848818 | 16.779780 | 0.000000 | . Cumin Seeds/Powder 2.153054 | 0.000000 | 3.276387 | 13.269366 | 5.874093 | 1.193541 | 5.195413 | 0.351041 | 21.553943 | 6.669787 | ... | 0.889305 | 0.538264 | 0.514861 | 1.240346 | 0.538264 | 1.919026 | 0.912708 | 5.874093 | 27.451439 | 0.070208 | . Curry Leaves 0.608472 | 0.000000 | 0.444652 | 12.543880 | 1.006319 | 0.117014 | 1.170138 | 0.046806 | 8.635619 | 2.036040 | ... | 0.163819 | 0.070208 | 0.070208 | 0.655277 | 0.023403 | 1.731804 | 0.280833 | 7.325064 | 16.522350 | 0.046806 | . Fennel Seed 0.748888 | 0.046806 | 0.819097 | 1.684999 | 1.450971 | 0.163819 | 1.989235 | 0.070208 | 3.767845 | 2.457290 | ... | 0.702083 | 0.257430 | 0.093611 | 0.912708 | 0.421250 | 0.374444 | 0.748888 | 0.819097 | 4.212497 | 0.000000 | . Fenugreek Leaf 0.865902 | 0.000000 | 0.795694 | 4.540136 | 1.638193 | 0.117014 | 1.521179 | 0.117014 | 6.388954 | 1.731804 | ... | 0.702083 | 0.023403 | 0.163819 | 0.374444 | 0.070208 | 0.889305 | 0.257430 | 3.229581 | 8.705827 | 0.000000 | . Fenugreek Seed 0.140417 | 0.000000 | 0.468055 | 3.159373 | 0.351041 | 0.070208 | 0.374444 | 0.070208 | 2.363679 | 0.608472 | ... | 0.608472 | 0.000000 | 0.046806 | 0.304236 | 0.023403 | 0.585069 | 0.070208 | 2.925345 | 4.516733 | 0.000000 | . Garam Masala 1.427568 | 0.000000 | 2.293471 | 3.182776 | 4.399719 | 0.257430 | 3.978469 | 0.280833 | 14.065060 | 4.001872 | ... | 0.327639 | 0.140417 | 0.304236 | 0.631875 | 0.491458 | 0.421250 | 0.444652 | 0.491458 | 13.245963 | 0.000000 | . Ginger Powder 0.210625 | 0.023403 | 0.046806 | 0.444652 | 0.351041 | 0.046806 | 0.842499 | 0.023403 | 0.585069 | 0.491458 | ... | 0.000000 | 0.187222 | 0.023403 | 0.023403 | 0.117014 | 0.070208 | 0.000000 | 0.070208 | 0.468055 | 0.000000 | . Green Chillies 1.802013 | 0.000000 | 1.591388 | 7.605897 | 4.610344 | 0.280833 | 3.955067 | 0.210625 | 12.216242 | 4.844372 | ... | 0.772291 | 0.187222 | 0.327639 | 1.193541 | 0.327639 | 1.076527 | 0.655277 | 2.363679 | 17.107419 | 0.070208 | . Indian Gooseberry 0.046806 | 0.000000 | 0.000000 | 0.304236 | 0.000000 | 0.023403 | 0.000000 | 0.023403 | 0.163819 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.023403 | 0.000000 | 0.046806 | 0.210625 | 0.000000 | . Kalpasi 0.023403 | 0.000000 | 0.000000 | 0.023403 | 0.023403 | 0.000000 | 0.070208 | 0.000000 | 0.023403 | 0.070208 | ... | 0.000000 | 0.023403 | 0.000000 | 0.070208 | 0.000000 | 0.000000 | 0.046806 | 0.000000 | 0.000000 | 0.000000 | . Mace 0.374444 | 0.000000 | 0.023403 | 0.093611 | 0.819097 | 0.000000 | 1.076527 | 0.046806 | 0.795694 | 1.053124 | ... | 0.000000 | 0.491458 | 0.023403 | 0.257430 | 0.140417 | 0.000000 | 0.304236 | 0.000000 | 0.889305 | 0.000000 | . Mustard Seed 0.748888 | 0.000000 | 0.865902 | 12.707700 | 1.146735 | 0.070208 | 0.725486 | 0.023403 | 8.027147 | 1.591388 | ... | 0.702083 | 0.093611 | 0.070208 | 0.795694 | 0.046806 | 1.638193 | 0.280833 | 6.739995 | 15.352212 | 0.000000 | . Nigella Seed 0.163819 | 0.000000 | 0.351041 | 0.304236 | 0.257430 | 0.023403 | 0.070208 | 0.000000 | 0.842499 | 0.140417 | ... | 1.661596 | 0.000000 | 0.023403 | 0.163819 | 0.023403 | 0.070208 | 0.023403 | 0.070208 | 1.263749 | 0.023403 | . Nutmeg 0.163819 | 0.000000 | 0.000000 | 0.140417 | 0.608472 | 0.000000 | 1.076527 | 0.093611 | 0.444652 | 0.959513 | ... | 0.000000 | 1.521179 | 0.000000 | 0.304236 | 0.234028 | 0.070208 | 0.280833 | 0.046806 | 0.561666 | 0.000000 | . Pomegranate Seed 0.093611 | 0.000000 | 0.140417 | 0.093611 | 0.117014 | 0.093611 | 0.234028 | 0.000000 | 0.468055 | 0.187222 | ... | 0.023403 | 0.000000 | 0.772291 | 0.046806 | 0.070208 | 0.023403 | 0.023403 | 0.070208 | 0.327639 | 0.000000 | . Poppy Seed 0.234028 | 0.000000 | 0.023403 | 0.538264 | 0.748888 | 0.000000 | 1.474374 | 0.000000 | 1.380763 | 1.404166 | ... | 0.163819 | 0.304236 | 0.046806 | 3.252984 | 0.187222 | 0.257430 | 0.374444 | 0.421250 | 1.802013 | 0.023403 | . Saffron 0.210625 | 0.000000 | 0.046806 | 0.280833 | 0.655277 | 0.093611 | 3.135970 | 0.023403 | 0.678680 | 0.842499 | ... | 0.023403 | 0.234028 | 0.070208 | 0.187222 | 4.025275 | 0.070208 | 0.140417 | 0.000000 | 0.585069 | 0.000000 | . Sesame Seed 0.397847 | 0.000000 | 0.257430 | 1.567985 | 0.163819 | 0.023403 | 0.351041 | 0.000000 | 1.591388 | 0.234028 | ... | 0.070208 | 0.070208 | 0.023403 | 0.257430 | 0.070208 | 4.189094 | 0.070208 | 0.819097 | 1.614791 | 0.000000 | . Star Anise 0.234028 | 0.000000 | 0.046806 | 0.257430 | 1.099930 | 0.023403 | 1.287152 | 0.023403 | 0.819097 | 1.380763 | ... | 0.023403 | 0.280833 | 0.023403 | 0.374444 | 0.140417 | 0.070208 | 1.731804 | 0.140417 | 0.889305 | 0.000000 | . Tamarind 0.374444 | 0.000000 | 0.327639 | 5.172010 | 0.257430 | 0.280833 | 0.421250 | 0.000000 | 3.580623 | 0.889305 | ... | 0.070208 | 0.046806 | 0.070208 | 0.421250 | 0.000000 | 0.819097 | 0.140417 | 11.958811 | 7.208051 | 0.000000 | . Turmeric 2.972151 | 0.000000 | 3.510414 | 15.047976 | 7.535689 | 0.327639 | 6.599579 | 0.514861 | 28.387550 | 8.097355 | ... | 1.263749 | 0.561666 | 0.327639 | 1.802013 | 0.585069 | 1.614791 | 0.889305 | 7.208051 | 48.467119 | 0.046806 | . White Pepper 0.000000 | 0.000000 | 0.023403 | 0.023403 | 0.070208 | 0.000000 | 0.070208 | 0.000000 | 0.046806 | 0.070208 | ... | 0.023403 | 0.000000 | 0.000000 | 0.023403 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.046806 | 0.163819 | . 34 rows × 34 columns . fig, ax = plt.subplots(1,1, figsize=(10,10)) sns.heatmap(spice_adj_freq.round(2).corr(), ax=ax) #plt.savefig(&quot;heatmap.png&quot;, format=&quot;PNG&quot;, dpi=300) . &lt;AxesSubplot:&gt; . Using frequency adjacency matrix we can plot a heatmap showing the pair-wise occurence for a given pair of spices. The idea with such an analysis is that if we can check the variation of Spice 1 with all the other spices in the list and compare that to Spice 2’s variation with all the other spices in the list, if spice 1 and spice 2 should have similar variation. . This map itself is quite interesting. The color intensity of each title shows the frequency that pair of spice occurred together in a recipe. Brighter the color higher their occurence together. . Some prominent spice pairs which show similarity are: . Curry leaves and Mustard seeds . | Tumeric and Chilli Powder . | . Some pair of spices never occur together: . Saffron and Fenugreek seeds . | Nutmeg and Mustard Seeds . | . Those who cook or know indian recipes would see that these pairs make sense and thereby validate the correlation seen from corpus of Indian recipes. . With that analysis, we can go a step further and analyze this information in form of a circular network graph. Using this method of plotting, we can see the interactions between different spices. . Creating network . import networkx as nx . nodes_data = [(i, {&#39;count&#39;:spice_adj_freq.loc[i, i]}) for i in temp_name] . binary_int = [] for i in temp_name: binary_int.append((i, spice_adj_freq.loc[i].sort_values(ascending=False).index[1])) . spice_dict = {i : spice_adj_freq.loc[i, i] for i in temp_name } . spice_dict . {&#39;Ajwain&#39;: 5.218815820266792, &#39;Alkanet Root&#39;: 0.07020828457758016, &#39;Amchoor&#39;: 4.984788205008191, &#39;Asafoetida&#39;: 24.596302363678916, &#39;Bay Leaf&#39;: 10.695062017318044, &#39;Black Salt&#39;: 1.6381933068102035, &#39;Cardamom&#39;: 17.78609875965364, &#39;Celery Seeds&#39;: 0.7956938918792418, &#39;Chilli Powder&#39;: 37.959279194945005, &#39;Cinnamon&#39;: 13.128949216007488, &#39;Cloves&#39;: 28.34074420781652, &#39;Coriander Seeds/Powder&#39;: 20.336999765972386, &#39;Cumin Seeds/Powder&#39;: 43.59934472267727, &#39;Curry Leaves&#39;: 27.615258600514856, &#39;Fennel Seed&#39;: 7.208050549964897, &#39;Fenugreek Leaf&#39;: 12.965129885326467, &#39;Fenugreek Seed&#39;: 7.208050549964897, &#39;Garam Masala&#39;: 18.34776503627428, &#39;Ginger Powder&#39;: 1.357360168499883, &#39;Green Chillies&#39;: 29.815118183945703, &#39;Indian Gooseberry&#39;: 0.3978469459396209, &#39;Kalpasi&#39;: 0.07020828457758016, &#39;Mace&#39;: 1.2403463608705827, &#39;Mustard Seed&#39;: 24.315469225368595, &#39;Nigella Seed&#39;: 1.6615960683360638, &#39;Nutmeg&#39;: 1.5211794991809033, &#39;Pomegranate Seed&#39;: 0.7722911303533817, &#39;Poppy Seed&#39;: 3.2529838520945473, &#39;Saffron&#39;: 4.025274982447929, &#39;Sesame Seed&#39;: 4.189094313128949, &#39;Star Anise&#39;: 1.7318043529136438, &#39;Tamarind&#39;: 11.958811139714488, &#39;Turmeric&#39;: 48.46711912005617, &#39;White Pepper&#39;: 0.16381933068102036} . edges_data = [] for i in temp_name: for j in temp_name: if i != j: if spice_adj_freq.loc[i,j] != 0.0: edges_data.append((i, j, {&#39;weight&#39;:spice_adj_freq.loc[i,j], &#39;distance&#39;:1})) . #BUILD THE INITIAL FULL GRAPH G=nx.Graph() G.add_nodes_from(nodes_data) G.add_edges_from(edges_data) . print(nx.info(G)) . Name: Type: Graph Number of nodes: 34 Number of edges: 471 Average degree: 27.7059 . deg_l = {i:G.degree(i) for i in temp_name} . highest_centrality_node = max(deg_l.items(), key=lambda x: x[1])[0] . highest_centrality_node . &#39;Asafoetida&#39; . n = len(nodes_data) edges = G.edges() weights = [G[u][v][&#39;weight&#39;] for u,v in edges] w_arr = np.array(weights) norm_weight = (w_arr - w_arr.min())/(w_arr.max() - w_arr.min()) angle = [] angle_dict = {} node_list = sorted(G.nodes()) for i, node in zip(np.arange(n),node_list): theta = 2.0*np.pi*i/n angle.append((np.cos(theta),np.sin(theta))) angle_dict[node] = theta pos = {} for node_i, node in enumerate(node_list): pos[node] = angle[node_i] fig, ax = plt.subplots(figsize=(20,20)) margin=0.33 fig.subplots_adjust(margin, margin, 1.-margin, 1.-margin) ax.axis(&#39;equal&#39;) nx.draw(G,pos=pos,with_labels=False, node_size=[spice_dict[k]*20 for k in spice_dict], width=norm_weight*2.0, node_color=np.arange(n), cmap=plt.cm.viridis, ax=ax) description = nx.draw_networkx_labels(G,pos) r = fig.canvas.get_renderer() trans = plt.gca().transData.inverted() for node, t in description.items(): bb = t.get_window_extent(renderer=r) bbdata = bb.transformed(trans) radius = 1.1+bbdata.width/2 position = (radius*np.cos(angle_dict[node]),radius* np.sin(angle_dict[node])) t.set_position(position) t.set_rotation(angle_dict[node]*360.0/(2.0*np.pi)) t.set_clip_on(False) #plt.savefig(&quot;Graph.png&quot;, format=&quot;PNG&quot;, dpi=300) . Finally a networkx circular graph is made where each node is a spice entry. Each edge between a pair of spice is a connection provided those two spices are found together in a recipe. The size of the node is the frequency of that spice to occur in all of 6000 food recipes. The thickness of the edge connecting a give spice-pair is the normalized frequency that pair occured among 6000 recipes. . Representing the analysis this way we find few key takeaways: . Tumeric, Mustard Seeds, Chilli Powder, Corriander Seeds, Cumin Seeds, Curry Leaves, Green Chillies, Asafoetida are the key spices in the Indian cuisine. . | Most recipes use Tumeric + Chilli Powder + Cumin Powder (Seeds) in them. . | .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/data-visualization/web-scrapping/2020/12/09/food_relations.html",
            "relUrl": "/python/data-analysis/data-visualization/web-scrapping/2020/12/09/food_relations.html",
            "date": " • Dec 9, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Network analysis hands-on",
            "content": "Good resource to learn basics of Network science: . http://networksciencebook.com/chapter/0 | . Recent summary of Graph Network and their use in ML: . Relational inductive biases, deep learning, and graph networks | . Examples of Network graphs: . NetworkX Example dataset | Stanford Large Network Dataset Collection | Network building and manipulation will be done using NetworkX - a python package made for this exact function . import os import numpy as np import networkx as nx #-- PLOTTING PARAMS -# import matplotlib.pyplot as plt from matplotlib.pyplot import cm %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} . 1. Basics . Nodes: Points which are connected to each other. Can represent people, words, or atoms -- objects which have attributes of their own . Edges: Connection between the nodes - show how nodes (entities) are connected, bond distance, social network (friendships) -- property which connect the entities . G = nx.Graph() # Add a node G.add_node(42) # Add node from list of entities temp_list = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;] G.add_nodes_from(temp_list) . G.nodes . NodeView((42, &#39;A&#39;, &#39;B&#39;, &#39;C&#39;)) . G.remove_node(42) #This is definite node name and should exist in the network # Multiple nodes G.remove_nodes_from([&#39;A&#39;,&#39;Z&#39;,&#39;Blah&#39;]) #Here it is compared to the element to that in the list . G.nodes . NodeView((&#39;B&#39;, &#39;C&#39;)) . # this also adds nodes if they don&#39;t already exist G.add_edge(&#39;C&#39;,&#39;Z&#39;) . print(G.edges, G.nodes) . [(&#39;C&#39;, &#39;Z&#39;)] [&#39;B&#39;, &#39;C&#39;, &#39;Z&#39;] . G.add_edges_from([(&#39;B&#39;, &#39;C&#39;) , (&#39;B&#39;, &#39;Z&#39;)]) . G.edges . EdgeView([(&#39;B&#39;, &#39;C&#39;), (&#39;B&#39;, &#39;Z&#39;), (&#39;C&#39;, &#39;Z&#39;)]) . # remove multiple edges (list of tuples) G.remove_edges_from([(&#39;A&#39;, &#39;B&#39;) , (&#39;C&#39;, &#39;B&#39;)]) #Here list are commutative . G.edges . EdgeView([(&#39;B&#39;, &#39;Z&#39;), (&#39;C&#39;, &#39;Z&#39;)]) . G.number_of_nodes() . 3 . G.number_of_edges() . 2 . G.degree(&#39;B&#39;) . 1 . G.clear() . 2. Reading from a file . For example we will look at Facebook dataset installed from SNAP dataset . G = nx.read_edgelist(&#39;./data/facebook_combined.txt&#39;) . G.number_of_nodes() . 4039 . G.number_of_edges() . 88234 . dict_neighbors = G.neighbors(&#39;2&#39;) . G.degree(&#39;2&#39;) . 10 . list(dict_neighbors) . [&#39;0&#39;, &#39;20&#39;, &#39;115&#39;, &#39;116&#39;, &#39;149&#39;, &#39;226&#39;, &#39;312&#39;, &#39;326&#39;, &#39;333&#39;, &#39;343&#39;] . G.clear() . 3. Type of different networks . a. Weighted Graphs . Edge weight Consider that the edge that you are adding should contain additional information, such as the strength of the connection. This would be important, for example, when analyzing communication networks to check friendship/connectivity strength. You want to capture how many times they exchanged e-mails, calls, text messages, to indicate the strength of the connection. For this you will assign weights to the edge, values that can be the number of communications, or the fraction of communications, normalized. . I had used this type of graph in my analysis for Indian spices. In that case, the edge was assigned a weight corresponding to the number of times a pair of spice occured together in a recipe. . G.add_edge(&#39;Water&#39;,&#39;Soda&#39;, weight=10) . Ways to access edge property: . G.edges.data() . EdgeDataView([(&#39;Water&#39;, &#39;Soda&#39;, {&#39;weight&#39;: 10})]) . G[&#39;Soda&#39;][&#39;Water&#39;] . {&#39;weight&#39;: 10} . G[&#39;Water&#39;][&#39;Soda&#39;] . {&#39;weight&#39;: 10} . G[&#39;Water&#39;][&#39;Soda&#39;][&#39;weight&#39;] = -1 . G.edges.data() . EdgeDataView([(&#39;Water&#39;, &#39;Soda&#39;, {&#39;weight&#39;: -1})]) . b. Directed Graphs . Incorporate directionality in the edge. Instead of having just the edge showing the connection: A B encode a type of connection. If A is giving (food, resources, atoms, electrons) to B. In that case: A -&gt; B . G.nodes . NodeView((&#39;Water&#39;, &#39;Soda&#39;)) . dg = nx.to_directed(G) . dg.edges . OutEdgeView([(&#39;Water&#39;, &#39;Soda&#39;), (&#39;Soda&#39;, &#39;Water&#39;)]) . dg.get_edge_data(&#39;Water&#39;,&#39;Soda&#39;) . {&#39;weight&#39;: -1} . c. Multigraphs . NetworkX provides classes for graphs which allow multiple edges between any pair of nodes. The MultiGraph and MultiDiGraph classes allow you to add the same edge twice, possibly with different edge data. This can be powerful for some applications, but many algorithms are not well defined on such graphs. . MG = nx.MultiGraph() MG.add_weighted_edges_from([(1, 2, 3.0), (1, 2, 75), (2, 3, 5), (1, 2, 4.2)]) . MG.edges . MultiEdgeView([(1, 2, 0), (1, 2, 1), (1, 2, 2), (2, 3, 0)]) . MG.edges.data(&#39;weight&#39;) . MultiEdgeDataView([(1, 2, 3.0), (1, 2, 75), (1, 2, 4.2), (2, 3, 5)]) . MG.edges.data() . MultiEdgeDataView([(1, 2, {&#39;weight&#39;: 3.0}), (1, 2, {&#39;weight&#39;: 75}), (1, 2, {&#39;weight&#39;: 4.2}), (2, 3, {&#39;weight&#39;: 5})]) . MG[1][2] . AtlasView({0: {&#39;weight&#39;: 3.0}, 1: {&#39;weight&#39;: 75}, 2: {&#39;weight&#39;: 4.2}}) . d. Bipartite . Bipartite graphs B = (U, V, E) have two node sets U,V and edges in E that only connect nodes from opposite sets. It is common in the literature to use an spatial analogy referring to the two node sets as top and bottom nodes. . from networkx.algorithms import bipartite . bip = nx.Graph() . bip.add_nodes_from([&#39;apple&#39;, &#39;peach&#39;, &#39;watermelon&#39;, &#39;pear&#39;], bipartite=0) bip.add_nodes_from([&#39;Alice&#39;, &#39;Steve&#39;, &#39;Mary&#39;], bipartite=1) . bip.add_edges_from([(&#39;Alice&#39;, &#39;apple&#39;), (&#39;Alice&#39;, &#39;peach&#39;), (&#39;Steve&#39;, &#39;watermelon&#39;), (&#39;Mary&#39;, &#39;pear&#39;), (&#39;Mary&#39;, &#39;apple&#39;), (&#39;Mary&#39;, &#39;watermelon&#39;)]) . nx.draw(bip, with_labels=True) . Currently, NetworkX does not provide a bipartite graph visualization method to visually delimit the two sets of nodes. However, we can draw the left and right set of nodes and see how they connect to each other. Further, you can play around with coloring the nodes based on the &#39;bipartite&#39; attribute to further refine visually to which node set each node belongs to. . import scipy.sparse as sparse X, Y = bipartite.sets(bip) pos = dict() pos.update((n, (1, i*10)) for i, n in enumerate(X)) pos.update((n, (1.5, i*10)) for i, n in enumerate(Y)) nx.draw(bip, with_labels=True, pos=pos) . Bipartite graphs can be projected as two separate graphs G1 = (U, E1) and G2 = (V, E2). The edges will be different though. . We can create a network of fruits, where nodes will be fruits and the edges will between two fruits will be created if someone likes both fruits. Such, peach and apple will have one edge, as Alice likes both. Same for apple and pear, which are both liked by Mary. Likewise, we can create the second network as the network of individuals, where connections between them will be their preference for the same fruit. Here, we can create a connection/edge between Steve and Mary since both of them like watermelon. . 3. Network Models . Network models can be very useful for comparing their topology to the structural properties of our network built from real data. Different network models have very distinct structural characteristics, which defines their behavior in case of information flow on the network, attacks/failures on the nodes/edges, etc, and these properties have been extensively studied and are well documented. Knowing to which network model your graph corresponds to can provide valuable insights about its potential behavior under various circumstances. . There are a miriad of network models with different topological properties. Here we will try out some of the most useful ones (that frequently occur in real complex systems). . ba = nx.barabasi_albert_graph(10, 5) nx.draw_spectral(ba, node_size=200) . Barabasi-Alber Graph. A graph of N nodes is grown by attaching new nodes each with M edges that are preferentially attached to existing nodes with high degree. . er = nx.erdos_renyi_graph(50, 0.1) nx.draw_circular(er) . complete = nx.complete_graph(5) nx.draw(complete) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2020/11/08/network_analysis_basics.html",
            "relUrl": "/python/machine-learning/2020/11/08/network_analysis_basics.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Get SMILES from PubChem using DASK",
            "content": "Dask implementation to acquire CanonicalSMILES from PubChem using the pubchem API. At the end of the notebook there is another dask based implementation of using RDKit to get InChIKey from the SMILES. While Dask is not necessary required in the case of InChIKeys it is a much more elegant implementation of dask.dataframes and map_partitions . import time import pubchempy as pcp from pubchempy import Compound, get_compounds import pandas as pd import numpy as np import re import copy . /depot/jgreeley/apps/envs/ml_torch/lib/python3.6/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError. warnings.warn(msg) . Get SMILES from Pubchem . Update:Parallelized using dask . df_100 = pd.read_csv(&#39;./DASK_SMILES/sample_chemical_names.csv&#39;, sep=&#39;,&#39;, header=0) . df_100.shape . (147, 1) . from dask.distributed import Client, progress import dask.dataframe as dd from dask import delayed, compute from dask.multiprocessing import get client = Client() client . Client . Scheduler: tcp://127.0.0.1:45859 | Dashboard: http://127.0.0.1:8787/status | . | Cluster . Workers: 4 | Cores: 8 | Memory: 39.85 GB | . | . def get_smile(cmpd_name): try: #delayed(f)(x, args=a) name = delayed(pcp.get_properties)([&#39;CanonicalSMILES&#39;], cmpd_name, &#39;name&#39;) time.sleep(5) smile = name[0][&#39;CanonicalSMILES&#39;] except: smile = &#39;X&#39; print(cta_name, smile) return smile def dask_smiles(df): df[&#39;CanonicalSMILES&#39;] = df[&#39;CTA&#39;].map(get_smile) return df #Map paritions works here -- but not with to_list() in the previous implementation . df_dask = dd.from_pandas(df_100, npartitions=10) . df_dask . Dask DataFrame Structure: CTA . npartitions=10 . 0 object | . 15 ... | . ... ... | . 135 ... | . 146 ... | . Dask Name: from_pandas, 10 tasks df_dask.visualize() %time ddf_out = df_dask.map_partitions(dask_smiles) . CPU times: user 567 ms, sys: 92.3 ms, total: 660 ms Wall time: 10 s . ddf_out.iloc[:,0] . Dask Series Structure: npartitions=10 0 object 15 ... ... 135 ... 146 ... Name: CTA, dtype: object Dask Name: getitem, 30 tasks . ddf_out.visualize() %time results = ddf_out.persist(scheduler=client).compute() . CPU times: user 9.42 s, sys: 1.27 s, total: 10.7 s Wall time: 2min 43s . type(results) . pandas.core.frame.DataFrame . results.loc[0] . CTA Cyclopropane CanonicalSMILES Delayed(&#39;getitem-e98dc8d7261c3d694a3c944735b3c... Name: 0, dtype: object . compute(results[&#39;CanonicalSMILES&#39;].iloc[0])[0] #Compute result for one entry . &#39;C1CC1&#39; . %time results[&#39;CanonicalSMILES&#39;] = [value[0] for value in results[&#39;CanonicalSMILES&#39;].map(compute)] . CPU times: user 3.73 s, sys: 443 ms, total: 4.17 s Wall time: 31.1 s . type(results) . pandas.core.frame.DataFrame . results[results[&#39;CanonicalSMILES&#39;] == &#39;X&#39;] . CTA CanonicalSMILES . results . CTA CanonicalSMILES . 0 Cyclopropane | C1CC1 | . 1 Ethylene | C=C | . 2 Methane | C | . 3 t-Butanol | CC(C)(C)O | . 4 ethane | CC | . ... ... | ... | . 142 Cyclohexane-1,3-dicarbaldehyde | C1CC(CC(C1)C=O)C=O | . 143 isobutene | CC(=C)C | . 144 propanal | CCC=O | . 145 methyl methacrylate | CC(=C)C(=O)OC | . 146 vinyl acetate | CC(=O)OC=C | . 147 rows × 2 columns . results.to_pickle(&quot;cta_smiles_table_100_less.pkl&quot;) ## Dask to get InChIKey . This implementation in my opinion is more elegant use of dask&#39;s apply command wrapper around conventional pandas apply. Also here we are defining the meta key for the variable since the code doesn&#39;t seem to recognise the type of entries we expect in the final output . More information about meta here: https://docs.dask.org/en/latest/dataframe-api.html . import rdkit from rdkit import Chem from rdkit.Chem import PandasTools from rdkit.Chem import Draw Chem.WrapLogs() lg = rdkit.RDLogger.logger() lg.setLevel(rdkit.RDLogger.CRITICAL) . def get_InChiKey(x): try: inchi_key = Chem.MolToInchiKey(Chem.MolFromSmiles(x)) except: inchi_key = &#39;X&#39; return inchi_key def dask_smiles(df): df[&#39;INCHI&#39;] = df[&#39;smiles&#39;].map(get_name) return df . results_dask = dd.from_pandas(results, npartitions=10) . inchi = results_dask[&#39;CanonicalSMILES&#39;].apply(lambda x: Chem.MolToInchiKey(Chem.MolFromSmiles(x)), meta=(&#39;inchi_key&#39;,str)) . inchi . Dask Series Structure: npartitions=10 0 object 15 ... ... 135 ... 146 ... Name: inchi_key, dtype: object Dask Name: apply, 30 tasks . inchi.visualize() inchi is a new Pandas series which has the delayed graphs for computing InChIKeys. We can compute it directly in the results dataframe as a new column. This is slightly different from the SMILES implementation above. . %time results[&#39;INCHI&#39;] = compute(inchi, scheduler = client)[0] . CPU times: user 125 ms, sys: 17.3 ms, total: 142 ms Wall time: 1.02 s . results . CTA CanonicalSMILES INCHI . 0 Cyclopropane | C1CC1 | LVZWSLJZHVFIQJ-UHFFFAOYSA-N | . 1 Ethylene | C=C | VGGSQFUCUMXWEO-UHFFFAOYSA-N | . 2 Methane | C | VNWKTOKETHGBQD-UHFFFAOYSA-N | . 3 t-Butanol | CC(C)(C)O | DKGAVHZHDRPRBM-UHFFFAOYSA-N | . 4 ethane | CC | OTMSDBZUPAUEDD-UHFFFAOYSA-N | . ... ... | ... | ... | . 142 Cyclohexane-1,3-dicarbaldehyde | C1CC(CC(C1)C=O)C=O | WHKHKMGAZGBKCK-UHFFFAOYSA-N | . 143 isobutene | CC(=C)C | VQTUBCCKSQIDNK-UHFFFAOYSA-N | . 144 propanal | CCC=O | NBBJYMSMWIIQGU-UHFFFAOYSA-N | . 145 methyl methacrylate | CC(=C)C(=O)OC | VVQNEPGJFQJSBK-UHFFFAOYSA-N | . 146 vinyl acetate | CC(=O)OC=C | XTXRWKRVRITETP-UHFFFAOYSA-N | . 147 rows × 3 columns .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/2020/09/18/SMILES_from_pubchem.html",
            "relUrl": "/python/data-analysis/2020/09/18/SMILES_from_pubchem.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Analyze Bollywood movie ratings (1950-2020)",
            "content": "import os from requests import get import numpy as np import pandas as pd from bs4 import BeautifulSoup import time as time from tqdm.notebook import tqdm . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns sns.set(style=&quot;whitegrid&quot;) %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . Load Dataset . The data set for the movie was scrapped from IMDB using BeautifulSoup. More details can be found in the supplementary python script. . df_movies = pd.read_csv(&#39;./IMDB-files/bollywood_movies_data_1950_2020_new.csv&#39;,sep=&#39;,&#39;, skipinitialspace=True) . df_movies.columns . Index([&#39;name&#39;, &#39;year&#39;, &#39;rating&#39;, &#39;metascore&#39;, &#39;num_votes&#39;], dtype=&#39;object&#39;) . df_movies.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 11876 entries, 0 to 11875 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 name 11876 non-null object 1 year 11875 non-null object 2 rating 7427 non-null float64 3 metascore 91 non-null float64 4 num_votes 7427 non-null float64 dtypes: float64(3), object(2) memory usage: 464.0+ KB . Cleaning the data . df_movies[&#39;year&#39;].unique() . array([&#39;1950&#39;, &#39;1951&#39;, &#39;I) (1951&#39;, &#39;1952&#39;, &#39;1957&#39;, &#39;II) (1952&#39;, &#39;1953&#39;, &#39;II) (1953&#39;, &#39;III) (1953&#39;, &#39;I) (1953&#39;, &#39;1954&#39;, &#39;I) (1954&#39;, &#39;III) (1954&#39;, &#39;1955&#39;, &#39;1956&#39;, &#39;II) (1957&#39;, &#39;1958&#39;, &#39;I) (1958&#39;, &#39;1959&#39;, &#39;II) (1959&#39;, &#39;1960&#39;, &#39;I) (1960&#39;, &#39;1961&#39;, &#39;1962&#39;, &#39;1963&#39;, &#39;I) (1964&#39;, &#39;1964&#39;, &#39;1965&#39;, &#39;1966&#39;, &#39;1967&#39;, &#39;1968&#39;, &#39;I) (1968&#39;, &#39;1969&#39;, &#39;I) (1969&#39;, &#39;1979&#39;, &#39;1970&#39;, &#39;II) (1970&#39;, &#39;1971&#39;, &#39;I) (1971&#39;, &#39;II) (1971&#39;, &#39;1972&#39;, &#39;II) (1972&#39;, &#39;1973&#39;, &#39;1974&#39;, &#39;II) (1974&#39;, &#39;1975&#39;, &#39;I) (1975&#39;, &#39;II) (1975&#39;, &#39;1976&#39;, &#39;1977&#39;, &#39;I) (1977&#39;, &#39;1978&#39;, &#39;II) (1978&#39;, &#39;I) (1979&#39;, &#39;II) (1979&#39;, &#39;1980&#39;, &#39;I) (1980&#39;, &#39;1981&#39;, &#39;1982&#39;, &#39;I) (1982&#39;, &#39;1983&#39;, &#39;I) (1983&#39;, &#39;II) (1983&#39;, &#39;1984&#39;, &#39;II) (1984&#39;, &#39;1985&#39;, &#39;I) (1985&#39;, &#39;1986&#39;, &#39;I) (1986&#39;, &#39;II) (1986&#39;, &#39;1987&#39;, &#39;I) (1987&#39;, &#39;1988&#39;, &#39;I) (1988&#39;, &#39;II) (1988&#39;, &#39;1989&#39;, &#39;I) (1989&#39;, &#39;1990&#39;, &#39;II) (1990&#39;, &#39;I) (1990&#39;, &#39;1991&#39;, &#39;I) (1991&#39;, &#39;1992&#39;, &#39;1993&#39;, &#39;I) (1992&#39;, &#39;II) (1992&#39;, &#39;I) (1993&#39;, &#39;II) (1993&#39;, &#39;1994&#39;, &#39;II) (1994&#39;, &#39;I) (1994&#39;, &#39;1995&#39;, &#39;1996&#39;, &#39;I) (1996&#39;, &#39;1997&#39;, &#39;I) (1997&#39;, &#39;1998&#39;, &#39;II) (1998&#39;, &#39;2005&#39;, &#39;1999&#39;, &#39;II) (1999&#39;, &#39;2000&#39;, &#39;II) (2000&#39;, &#39;I) (2000&#39;, &#39;2001&#39;, &#39;I) (2001&#39;, &#39;I) (2002&#39;, &#39;2002&#39;, &#39;2003&#39;, &#39;I) (2003&#39;, &#39;2004&#39;, &#39;2007&#39;, &#39;I) (2005&#39;, &#39;II) (2005&#39;, &#39;2006&#39;, &#39;I) (2006&#39;, &#39;II) (2006&#39;, &#39;I) (2007&#39;, &#39;III) (2007&#39;, &#39;2008&#39;, &#39;I) (2008&#39;, &#39;II) (2008&#39;, &#39;2009&#39;, &#39;I) (2009&#39;, &#39;2012&#39;, &#39;II) (2009&#39;, &#39;2010&#39;, &#39;I) (2010&#39;, &#39;II) (2010&#39;, &#39;IV) (2010&#39;, &#39;2011&#39;, &#39;I) (2011&#39;, &#39;II) (2011&#39;, &#39;IV) (2011&#39;, &#39;II) (2012&#39;, &#39;I) (2012&#39;, &#39;2013&#39;, &#39;I) (2013&#39;, &#39;II) (2013&#39;, &#39;V) (2013&#39;, &#39;2014&#39;, &#39;I) (2014&#39;, &#39;III) (2014&#39;, &#39;VIII) (2014&#39;, &#39;II) (2014&#39;, &#39;IV) (2014&#39;, &#39;2015&#39;, &#39;I) (2015&#39;, &#39;V) (2015&#39;, &#39;III) (2015&#39;, &#39;VI) (2015&#39;, &#39;II) (2015&#39;, &#39;IV) (2015&#39;, &#39;2016&#39;, &#39;I) (2016&#39;, &#39;III) (2016&#39;, &#39;XVII) (2016&#39;, &#39;IV) (2016&#39;, &#39;V) (2016&#39;, &#39;X) (2016&#39;, &#39;II) (2016&#39;, &#39;VII) (2016&#39;, &#39;VI) (2016&#39;, &#39;2017&#39;, &#39;I) (2017&#39;, &#39;II) (2017&#39;, &#39;III) (2017&#39;, &#39;IV) (2017&#39;, &#39;2018&#39;, &#39;III) (2018&#39;, &#39;I) (2018&#39;, &#39;II) (2018&#39;, &#39;2019&#39;, &#39;III) (2019&#39;, &#39;I) (2019&#39;, &#39;II) (2019&#39;, &#39;IV) (2019&#39;, &#39;2020&#39;, &#39;I) (2020&#39;, &#39;II) (2020&#39;, &#39;VI) (2020&#39;, nan], dtype=object) . Data pulled from the website has phantom characters alongside the dates. Hence this would need some cleaning from our end to ensure all the dates are in consistent format. . df_movies.shape . (11876, 5) . I am using strip to loop each date entry in the dataset and strip off any residual characters which coincide with the those mentioned in the filter . df_movies[&#39;year&#39;] = df_movies[&#39;year&#39;].astype(&#39;str&#39;) . df_movies[&#39;year&#39;]=[i.strip(&#39;IIII) XVII) ( ( TV Special TV Mov&#39;) for i in df_movies[&#39;year&#39;].tolist()] . Printing the data again to check for the date entries: . df_movies[&#39;year&#39;].unique() . array([&#39;1950&#39;, &#39;1951&#39;, &#39;1952&#39;, &#39;1957&#39;, &#39;1953&#39;, &#39;1954&#39;, &#39;1955&#39;, &#39;1956&#39;, &#39;1958&#39;, &#39;1959&#39;, &#39;1960&#39;, &#39;1961&#39;, &#39;1962&#39;, &#39;1963&#39;, &#39;1964&#39;, &#39;1965&#39;, &#39;1966&#39;, &#39;1967&#39;, &#39;1968&#39;, &#39;1969&#39;, &#39;1979&#39;, &#39;1970&#39;, &#39;1971&#39;, &#39;1972&#39;, &#39;1973&#39;, &#39;1974&#39;, &#39;1975&#39;, &#39;1976&#39;, &#39;1977&#39;, &#39;1978&#39;, &#39;1980&#39;, &#39;1981&#39;, &#39;1982&#39;, &#39;1983&#39;, &#39;1984&#39;, &#39;1985&#39;, &#39;1986&#39;, &#39;1987&#39;, &#39;1988&#39;, &#39;1989&#39;, &#39;1990&#39;, &#39;1991&#39;, &#39;1992&#39;, &#39;1993&#39;, &#39;1994&#39;, &#39;1995&#39;, &#39;1996&#39;, &#39;1997&#39;, &#39;1998&#39;, &#39;2005&#39;, &#39;1999&#39;, &#39;2000&#39;, &#39;2001&#39;, &#39;2002&#39;, &#39;2003&#39;, &#39;2004&#39;, &#39;2007&#39;, &#39;2006&#39;, &#39;2008&#39;, &#39;2009&#39;, &#39;2012&#39;, &#39;2010&#39;, &#39;2011&#39;, &#39;2013&#39;, &#39;2014&#39;, &#39;2015&#39;, &#39;2016&#39;, &#39;2017&#39;, &#39;2018&#39;, &#39;2019&#39;, &#39;2020&#39;, &#39;nan&#39;], dtype=object) . Consistency check for the dataframe shape to ensure no funny business . df_movies.shape . (11876, 5) . Filtering out movies . Since IMDb is a fairly recent rating portal there are lot of movies especially those realeased pre 1980s which have low votes. Also IMDb lists every possible movie that was released in Hindi language. To better focus on credible movies I would filter out movies with low votes . votes_filter = df_movies[&#39;num_votes&#39;] &gt; 50 #Filter out movies which have got less than 100 votes from IMDb users df_movies_filter_votes = df_movies.loc[votes_filter].reset_index(drop=True) #Reset the indices of the new dataframe and drop the old ones -- if not done a different column with old index is appended . df_movies_filter_votes.shape . (3912, 5) . Convert year data entry to pandas Datetime object for convenience . df_movies_filter_votes[&#39;year&#39;] = pd.to_datetime(df_movies_filter_votes[&#39;year&#39;],format=&#39;%Y&#39;).dt.year . Analyze annual movie releases . Defining a separate dataframe for doing per-year analysis . stat_list = [&#39;year&#39;, &#39;total_movies_year&#39;, &#39;highest_rated_movie&#39;, &#39;movie_rating&#39;,&#39;avg_num_votes&#39;, &#39;avg_movie_rating&#39;] annual_movie_stats = {keys:[] for keys in stat_list} for year_entry in df_movies_filter_votes[&#39;year&#39;].unique(): per_year_column = df_movies_filter_votes.loc[df_movies_filter_votes[&#39;year&#39;] == year_entry] try: movie_entry_with_max_ratings = df_movies_filter_votes.loc[per_year_column[&#39;rating&#39;].idxmax()] higest_movie_rating = movie_entry_with_max_ratings[&#39;rating&#39;] highest_rated_movie = movie_entry_with_max_ratings[&#39;name&#39;] avg_movie_rating = per_year_column[&#39;rating&#39;].mean() total_movies = len(per_year_column) avg_num_votes = per_year_column[&#39;num_votes&#39;].mean() except ValueError: higest_movie_rating = np.nan highest_rated_movie = np.nan total_movies = np.nan avg_movie_rating = np.nan annual_movie_stats[&#39;year&#39;].append(year_entry) annual_movie_stats[&#39;highest_rated_movie&#39;].append(highest_rated_movie) annual_movie_stats[&#39;movie_rating&#39;].append(higest_movie_rating) annual_movie_stats[&#39;avg_movie_rating&#39;].append(avg_movie_rating) annual_movie_stats[&#39;total_movies_year&#39;].append(total_movies) annual_movie_stats[&#39;avg_num_votes&#39;].append(avg_num_votes) . df_annual_movie_stats = pd.DataFrame(annual_movie_stats, columns=annual_movie_stats.keys()) . df_annual_movie_stats.sample(5) . year total_movies_year highest_rated_movie movie_rating avg_num_votes avg_movie_rating . 49 1999 | 67 | Sarfarosh | 8.1 | 2201.328358 | 5.583582 | . 3 1953 | 9 | Do Bigha Zamin | 8.4 | 293.000000 | 7.388889 | . 69 2019 | 141 | 99 Songs | 8.8 | 4041.056738 | 6.002128 | . 9 1959 | 11 | Kaagaz Ke Phool | 8.0 | 329.454545 | 7.172727 | . 34 1984 | 36 | Saaransh | 8.2 | 286.055556 | 6.427778 | . fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(30,20), sharex=True) year_list = [&quot;&#39;{}&quot;.format(str(value)[2:]) for value in df_annual_movie_stats.year.to_list()] sns.barplot(x=year_list, y=&#39;total_movies_year&#39;, color=&#39;k&#39;, alpha=0.8, data=df_annual_movie_stats, ax=ax1) ax1.set_ylabel(&#39;Average movies released&#39;) sns.scatterplot(year_list, &#39;avg_movie_rating&#39;, size=&#39;avg_num_votes&#39;, color=&#39;k&#39;, sizes=(40, 400), data=df_annual_movie_stats, ax=ax2); ax2.set_xlabel(&#39;Year&#39;) ax2.set_ylabel(&#39;Average movie rating&#39;) ax2.get_legend() for item in ax2.get_xticklabels(): item.set_rotation(45) plt.tight_layout() . /Users/pghaneka/miniconda3/envs/doodle/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . Sort the movies released as per decades . Define a new column here as per decade to condense the analysis . 10 * (df_annual_movie_stats[&#39;year&#39;]//10) . This line converts years to a decade entry . df_annual_movie_stats[&#39;decade&#39;] = 10 * (df_annual_movie_stats[&#39;year&#39;]//10) . df_annual_movie_stats_decade = df_annual_movie_stats.groupby([&#39;decade&#39;]).mean() . df_annual_movie_stats_decade.sample(5) . year total_movies_year movie_rating avg_num_votes avg_movie_rating . decade . 2000 2004.5 | 94.0 | 8.46 | 5160.443252 | 5.399690 | . 2010 2014.5 | 126.2 | 8.44 | 6305.900985 | 5.748150 | . 2020 2020.0 | 102.0 | 8.90 | 7485.009804 | 5.785294 | . 1960 1964.5 | 17.6 | 8.15 | 326.809100 | 7.104778 | . 1970 1974.5 | 30.4 | 8.18 | 820.688426 | 6.876870 | . df_annual_movie_stats_decade.index . Int64Index([1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020], dtype=&#39;int64&#39;, name=&#39;decade&#39;) . decade_list = [&quot;{}s&quot;.format(str(value)[2:]) for value in df_annual_movie_stats_decade.index.to_list()] . fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(15,10), sharex=True) sns.barplot(x=decade_list, y=&#39;total_movies_year&#39;, data=df_annual_movie_stats_decade, color=&#39;k&#39;, alpha=0.8, ax=ax1) ax1.set_ylabel(&#39;Average movies released annually&#39;) sns.scatterplot(decade_list, &#39;avg_movie_rating&#39;, size=&#39;avg_num_votes&#39;, sizes=(100, 400), data=df_annual_movie_stats_decade, ax=ax2); sns.despine() ax2.set_xlabel(&#39;Decade&#39;) ax2.set_ylabel(&#39;Average movie rating&#39;) ax2.get_legend().remove() plt.tight_layout() . /Users/pghaneka/miniconda3/envs/doodle/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/web-scrapping/2020/08/23/IMDB_bollywood.html",
            "relUrl": "/python/data-analysis/web-scrapping/2020/08/23/IMDB_bollywood.html",
            "date": " • Aug 23, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "MNIST digit classification using scikit-learn",
            "content": "This notebook is adapted from Aurélien Geron&#39;s hands-on machine learning tutorial . from __future__ import division, print_function, unicode_literals # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) . import matplotlib.pyplot as plt from matplotlib.pyplot import cm # High DPI rendering for mac %config InlineBackend.figure_format = &#39;retina&#39; plot_params = { &#39;font.size&#39; : 22, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . MNIST dataset . from sklearn.datasets import load_digits mnist = load_digits() print(mnist.data.shape) . (1797, 64) . from sklearn.datasets import fetch_openml mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings . def sort_by_target(mnist): reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1] reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1] #minist.data is a pandas DataFrame mnist_data_numpy = np.array(mnist.data.values) mnist_target_numpy = np.array(mnist.target.values) mnist_data_numpy[:60000] = mnist_data_numpy[reorder_train] mnist_target_numpy[:60000] = mnist_target_numpy[reorder_train] mnist_data_numpy[60000:] = mnist_data_numpy[reorder_test + 60000] mnist_target_numpy[60000:] = mnist_target_numpy[reorder_test + 60000] return mnist_data_numpy, mnist_target_numpy . 70,000 small images of hand-written numbers. Each image has 784 features. Those features are split in 28x28 pixels and each feature is simply that pixel gray-scale intensity. Value for each pixel ranges from 0 to 255. . X, y = sort_by_target(mnist) . random_digit=X[62123] print(&#39;The {0} entry is a photo of {1}&#39;.format(62123,y[62123])) random_digit_image=random_digit.reshape(28,28) plt.imshow(random_digit_image, cmap=cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . The 62123 entry is a photo of 2 . (-0.5, 27.5, 27.5, -0.5) . def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) . X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] . Before training we shuffle the data to ensure all cross-validation folds to be similar. Moreover some classficiation algorithms are sensitive to the order of training instances, and they perform poorly if they get many similar instances in a row. . import numpy as np index_shuffle = np.random.permutation(60000) X_train, y_train = X_train[index_shuffle], y_train[index_shuffle] . Binary classification . Here we will build a single digit classifier -- for example looking at just 2. Hence in total there will be only 2 classes -- Those which are 2 and those which are not. . y_train_2 = (y_train == 2) #True for all 2s, False for all other digits y_test_2 = (y_test == 2) . Using Stochastic Gradient Descent classifier. Known to handle large datasets very well. . from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42) sgd_clf.fit(X_train, y_train_2) . SGDClassifier(max_iter=5, random_state=42, tol=-inf) . sgd_clf.predict([random_digit]) . array([ True]) . Performance metrics . Evaluating classifiers is often significantly challenging than the case for a regressor wherein we can use RMSE or MAE. Let&#39;s look at some usual metrics used to gauge the classifier performance. . 1. Accuracy using Cross-validation . It involves splitting your training data in K-folds. Training the model on K-1 folds and testing it on the left out fold. Scikit learn has in-built method to do so: cross_val_score(). We can implement our own version as well. . from sklearn.model_selection import StratifiedKFold from sklearn.base import clone skfolds = StratifiedKFold(n_splits=3, shuffle=False) for train_index, test_index in skfolds.split(X_train,y_train_2): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_2[train_index] X_test_folds = X_train[test_index] y_test_folds = y_train_2[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred=clone_clf.predict(X_test_folds) n_correct = sum(y_pred == y_test_folds) print(n_correct/len(y_pred)) . 0.97365 0.96255 0.96165 . from sklearn.model_selection import cross_val_score cross_val_score(sgd_clf, X_train, y_train_2, cv=3, scoring=&#39;accuracy&#39;) . array([0.97365, 0.96255, 0.96165]) . Does this high accuracy tell us anything? . Is the sample space we are looking at uniform enough for this accuracy? . Maybe we have way less one-digit samples for training in the first place. . _count=0. for i in range(len(y_train)): if y_train[i] == 2: _count=_count+1. print(_count/len(y_train)*100) . 9.93 . So ~9% of the sample are actually 2. So even if we guess ALWAYS that image is not 2 we will be right 90% of the time! . The dumb classifier . To check whether classifier accuracy of ~95% is good enough so just a over-exagerration . from sklearn.base import BaseEstimator class Never2(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return(np.zeros((len(X),1),dtype=bool)) . never2 = Never2() cross_val_score(never2,X_train,y_train_2,cv=3,scoring=&#39;accuracy&#39;) . array([0.9017, 0.9001, 0.9003]) . This shows our data is skewed! . 2. Confusion Matrix . General idea is to count the number of times instances of Class A are classified as Class B. . Table that describes the performance of a classification model by grouping predictions into 4 categories. . True Positives: we correctly predicted they do have diabetes | True Negatives: we correctly predicted they don’t have diabetes | False Positives: we incorrectly predicted they do have diabetes (Type I error) | False Negatives: we incorrectly predicted they don’t have diabetes (Type II error) | . The ROWS in the matrix are the real class-labels i.e. the TRUTH values while COLUMNS are the predicted values. . from sklearn.model_selection import cross_val_predict y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3) . from sklearn.metrics import confusion_matrix confusion_matrix(y_train_2, y_train_pred) . array([[53159, 883], [ 1160, 4798]]) . Each row in the confusion matrix represent actual class, while each column represents a predicted class. Following are the terms of the confusion matrix: . First row of this matrix is the non-2 images -- (0,0) instances were correctly classified as non 2 (True Negative) | (0,1) instances were wrongly classified as 2s (False Positive) | . | Second row considers the images of 5 -- (1,0) instances were wrongly classified as non 2s (False negatives) | (1,1) instances were correctly classified as 2s (True positives) | . | An ideal classifier would be a diagonal matrix with no false positives or false negatives . Precision . It is the ratio of the total classification whether as True or Wrongly classified as True to True. That is, TP/(TP+FP) . This is looking at +ve classification and how many are really +ve and how many are wrongly shown as +ve. So Precision looks at the prediction of +ve results. . Recall . It is the ratio of total classification on the +ve samples from where they are classified correctly (TP) to wrongly classified as negative (FN). TP/(FN+TP) . So Recall looks at the prediction of the +ve samples. . This is by just comparing the +ve samples in the binary classification. To check how many of them are correctly recalled as +ve. . F1 score . Harmonic mean of recall and precision. Higher the Precision and Recall, lower are the instances of FP and FN. So we want to have higher Recall and Precision both. . F1 favors classifiers with similar recall and precision. . from sklearn.metrics import precision_score, recall_score, f1_score print(&#39;Precision score: {}&#39;.format(precision_score(y_train_2, y_train_pred))) print(&#39;Recall score: {}&#39;.format(recall_score(y_train_2, y_train_pred))) print(&#39;F1 score: {}&#39;.format(f1_score(y_train_2, y_train_pred))) . Precision score: 0.8445696180249956 Recall score: 0.8053037932192011 F1 score: 0.8244694561388435 . Recall/Precision tradeoff . Unfortunately increasing precision reduces recall and vise-versa. However sometimes one of the qualities could be desirable in a model. . Recall looks at lowering the False Negatives so culling +ve cases. That could be detrimental in catching robberies. So we need classifiers with high recall and we can fine low Precision wherein we would get False alarms. . Meanwhile, if we are censoring videos we need high Precision to ensure unsafe videos categorised as Safe ones. While we could be removing good videos by wrongly classifying them to be Unsafe (low recall). . Decision functions evaluate a decision_score we can manually set the threshold for the score to whether that will accpted or rejected for the binary case. . Increasing threshold reduces recall, but increases precision. . Why? The more Precise you want to be i.e. more True Positive than False Positives -- the higher the threshold for passing the case of accepting the data as a given class. However doing so we are strict in what we define as a ideal class and can neglect samples which are positive but are not closest to ideal. Hence we do incorrectly mark them as Negative thus increasing the case of False Negaitives and hence lowering Recall. . y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) . from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_2, y_scores) . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;, linewidth=2) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;, linewidth=2) plt.xlabel(&quot;Threshold&quot;, fontsize=16) plt.legend(loc=&quot;best&quot;, fontsize=16) plt.ylim([0, 1]) plt.figure(figsize=(8, 4)) plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.xlim([-700000, 700000]) plt.show() . plt.figure(figsize=(8, 4)) plt.plot(recalls[:-1],precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;, linewidth=2) plt.ylabel(&quot;Precision&quot;, fontsize=16) plt.xlabel(&quot;Recall&quot;, fontsize=16) . Text(0.5, 0, &#39;Recall&#39;) . If someone says let&#39;s reach 99% PRECISION, we must ALWAYS ask at what RECALL? . Manually set the Recall/Precision using threshold . y_scores = sgd_clf.decision_function([random_digit]) print(y_scores) y_pred_thresh = sgd_clf.predict([random_digit]) print(y_pred_thresh) #Setting threshold higher than the y_score threshold = y_scores + 1.0 y_pred_thresh = (y_scores &gt; threshold) print(y_pred_thresh) . [247991.40436599] [ True] [False] . y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) y_train_pred_90 = (y_scores &gt; 200000) print(&#39;Precision score: {}&#39;.format(precision_score(y_train_2, y_train_pred_90))) print(&#39;Recall score: {}&#39;.format(recall_score(y_train_2, y_train_pred_90))) print(&#39;F1 score: {}&#39;.format(f1_score(y_train_2, y_train_pred_90))) . Precision score: 0.9637028700056275 Recall score: 0.5748573346760658 F1 score: 0.720142977291842 . We have made classifier with an arbitrary Precision score: 97% However doing so we reduced the Recall. . The ROC curve . Another common tool used for binary classifiers apart from Precision/Recall. Instead of plotting precision vs recall we plot True Positive Rate (TPR) i.e. Recall against False Positive Rate (FPR). FPR is the ratio of negative instances that are incorrectly classified as positive. . ROC plots sensitivity vs 1-specificty . from sklearn.metrics import roc_curve #Decision scores for all instnces in the training set -- y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3, method=&quot;decision_function&quot;) fpr, tpr, thresholds = roc_curve(y_train_2, y_scores) def plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([0, 1, 0, 1]) plt.xlabel(&#39;False Positive Rate&#39;, fontsize=16) plt.ylabel(&#39;True Positive Rate&#39;, fontsize=16) plt.figure(figsize=(8, 6)) plot_roc_curve(fpr, tpr) plt.show() . from sklearn.metrics import roc_auc_score roc_auc_score(y_train_2, y_scores) . 0.9651158581307573 . PR curve when we care of precision -- getting False +ve and not so much of getting False -ve. We are okay with losing some +ve cases but for sure do not want to neglect any -ve ones. . Random forest classifier . from sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(n_estimators=10, random_state=42) y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_2, cv=3, method=&quot;predict_proba&quot;) . y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_2, y_scores_forest) . plt.figure(figsize=(8, 6)) plt.plot(fpr, tpr, &quot;b:&quot;, linewidth=2, label=&quot;SGD&quot;) plot_roc_curve(fpr_forest, tpr_forest, &quot;Random Forest&quot;) plt.legend(loc=&quot;lower right&quot;, fontsize=16) plt.show() . Multiclass classification . Multiclass classifiers are able to label and distinguish between more than two classes. Some algorithms such as Random Forest and Näive Bayes are capable of handling this directly. Having said that, Naive Baye&#39;s has shortcomming of considering class conditional independence and having discrete entries in the input. . OvA (One-versus-all classifiers): Herein, we would train n binary classifiers for n type of labels and see which n-th classifier has highest decision score. . | OvO (One-versus-one strategy): Binary classifier for every pair. So for n labels we will have n(n-1)/2 classifiers. . | . Error analysis . X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] index_shuffle = np.random.permutation(60000) X_train, y_train = X_train[index_shuffle], y_train[index_shuffle] sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42) sgd_clf.fit(X_train, y_train) y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3) y_scores = cross_val_predict(sgd_clf, X_train, y_train, cv=3, method=&quot;decision_function&quot;) conf_mx = confusion_matrix(y_train, y_train_pred) . plt.matshow(conf_mx, cmap=plt.cm.gray) plt.show() . Plotting heat-map for the errors in the classification . row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums #Diagonals are filled to be zero to concentrate only at the errors np.fill_diagonal(norm_conf_mx, 0) plt.matshow(norm_conf_mx, cmap=plt.cm.gray) plt.show() . ROWS in the confusion matrix are the REAL labels. COLUMNS in the confusion matrix are the PREDICTED values. It can seen that in the case of row 3 and column 5: . 5 is most of the times confused with 3 and 8 | 9 is confused with 4 and 7 | . def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = cm.binary, **options) plt.axis(&quot;off&quot;) . cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)] . plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5) plt.show() . Given above are two sets of &#39;3&#39; and &#39;5&#39; -- the boxes to the left are 3 and 5 classified as 3. Top left are the images of 3 classified as 3 while Bottom left are the images of 5 classified as 3. It can seen that some imags of 5 quite poor and the algorithm (which is linear in this case) will have difficulty predicting it. .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2020/08/12/MNIST_Scikit_learn-Classification.html",
            "relUrl": "/python/machine-learning/2020/08/12/MNIST_Scikit_learn-Classification.html",
            "date": " • Aug 12, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "S&P 500 analysis using beautifulsoup and pandas",
            "content": "To extract stock information . To extract stock information we will use yfinance module which is a convenient way to download data from Yahoo Finance. The official API for Yahoo Finance was decommissioned some time back. More details about this module can be found here. . from requests import get import numpy as np import pandas as pd from bs4 import BeautifulSoup import time as time from tqdm import tqdm import yfinance as yf from IPython.core.display import clear_output . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns sns.set(style=&quot;whitegrid&quot;) sns.color_palette(&quot;husl&quot;) %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 30, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . 1. Generate list of S&amp;P 500 companies . Parse wikipedia to generate a list . wiki_url = &#39;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies&#39; response = get(wiki_url) html_soup = BeautifulSoup(response.text, &#39;html.parser&#39;) tab = html_soup.find(&quot;table&quot;,{&quot;class&quot;:&quot;wikitable sortable&quot;}) . column_headings = [entry.text.strip() for entry in tab.findAll(&#39;th&#39;)] print(column_headings) . [&#39;Symbol&#39;, &#39;Security&#39;, &#39;SEC filings&#39;, &#39;GICS Sector&#39;, &#39;GICS Sub-Industry&#39;, &#39;Headquarters Location&#39;, &#39;Date first added&#39;, &#39;CIK&#39;, &#39;Founded&#39;] . SP_500_dict = {keys:[] for keys in column_headings} . for i, name in enumerate(SP_500_dict.keys()): print(i, name) . 0 Symbol 1 Security 2 SEC filings 3 GICS Sector 4 GICS Sub-Industry 5 Headquarters Location 6 Date first added 7 CIK 8 Founded . Populate each row entry as per company data . for row_entry in tab.findAll(&#39;tr&#39;)[1:]: row_elements = row_entry.findAll(&#39;td&#39;) for key, _elements in zip(SP_500_dict.keys(), row_elements): SP_500_dict[key].append(_elements.text.strip()) . SP_500_df = pd.DataFrame(SP_500_dict, columns=SP_500_dict.keys()) . SP_500_df . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 0 MMM | 3M Company | reports | Industrials | Industrial Conglomerates | St. Paul, Minnesota | 1976-08-09 | 0000066740 | 1902 | . 1 ABT | Abbott Laboratories | reports | Health Care | Health Care Equipment | North Chicago, Illinois | 1964-03-31 | 0000001800 | 1888 | . 2 ABBV | AbbVie Inc. | reports | Health Care | Pharmaceuticals | North Chicago, Illinois | 2012-12-31 | 0001551152 | 2013 (1888) | . 3 ABMD | Abiomed | reports | Health Care | Health Care Equipment | Danvers, Massachusetts | 2018-05-31 | 0000815094 | 1981 | . 4 ACN | Accenture | reports | Information Technology | IT Consulting &amp; Other Services | Dublin, Ireland | 2011-07-06 | 0001467373 | 1989 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 500 YUM | Yum! Brands Inc | reports | Consumer Discretionary | Restaurants | Louisville, Kentucky | 1997-10-06 | 0001041061 | 1997 | . 501 ZBRA | Zebra Technologies | reports | Information Technology | Electronic Equipment &amp; Instruments | Lincolnshire, Illinois | 2019-12-23 | 0000877212 | 1969 | . 502 ZBH | Zimmer Biomet | reports | Health Care | Health Care Equipment | Warsaw, Indiana | 2001-08-07 | 0001136869 | 1927 | . 503 ZION | Zions Bancorp | reports | Financials | Regional Banks | Salt Lake City, Utah | 2001-06-22 | 0000109380 | 1873 | . 504 ZTS | Zoetis | reports | Health Care | Pharmaceuticals | Parsippany, New Jersey | 2013-06-21 | 0001555280 | 1952 | . 505 rows × 9 columns . SP_500_df[&#39;GICS Sector&#39;].value_counts() . Information Technology 75 Industrials 74 Financials 65 Consumer Discretionary 63 Health Care 62 Consumer Staples 32 Real Estate 29 Materials 28 Utilities 28 Communication Services 26 Energy 23 Name: GICS Sector, dtype: int64 . Visualize distribution of the companies as per sectors . fig, ax = plt.subplots(1,1, figsize=(10,10)) SP_500_df[&#39;GICS Sector&#39;].value_counts().plot.pie(y=&#39;GICS Sector&#39;, autopct=&#39;%1.1f%%&#39;, fontsize=20, ax = ax, colormap=&#39;tab20&#39;) plt.axis(&#39;off&#39;) . (-1.25, 1.25, -1.25, 1.25) . SP_500_df.loc[ SP_500_df[&#39;GICS Sector&#39;] == &#39;Energy&#39;] . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 44 APA | APA Corporation | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1997-07-28 | 0000006769 | 1954 | . 59 BKR | Baker Hughes Co | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 2017-07-07 | 0001701605 | 2017 | . 80 COG | Cabot Oil &amp; Gas | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 2008-06-23 | 0000858470 | 1989 | . 101 CVX | Chevron Corp. | reports | Energy | Integrated Oil &amp; Gas | San Ramon, California | 1957-03-04 | 0000093410 | 1879 | . 121 COP | ConocoPhillips | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1957-03-04 | 0001163165 | 2002 | . 140 DVN | Devon Energy | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Oklahoma City, Oklahoma | 2000-08-30 | 0001090012 | 1971 | . 142 FANG | Diamondback Energy | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Midland, Texas | 2018-12-03 | 0001539838 | 2007 | . 169 EOG | EOG Resources | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 2000-11-02 | 0000821189 | 1999 | . 183 XOM | Exxon Mobil Corp. | reports | Energy | Integrated Oil &amp; Gas | Irving, Texas | 1957-03-04 | 0000034088 | 1999 | . 219 HAL | Halliburton Co. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 1957-03-04 | 0000045012 | 1919 | . 227 HES | Hess Corporation | reports | Energy | Integrated Oil &amp; Gas | New York, New York | 1984-05-31 | 0000004447 | 1919 | . 230 HFC | HollyFrontier Corp | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Dallas, Texas | 2018-06-18 | 0000048039 | 1947 | . 274 KMI | Kinder Morgan | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Houston, Texas | 2012-05-25 | 0001506307 | 1997 | . 298 MRO | Marathon Oil Corp. | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1991-05-01 | 0000101778 | 1887 | . 299 MPC | Marathon Petroleum | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Findlay, Ohio | 2011-07-01 | 0001510295 | 2009 (1887) | . 345 NOV | NOV Inc. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Houston, Texas | 2005-03-14 | 0001021860 | 1841 | . 352 OXY | Occidental Petroleum | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Houston, Texas | 1982-12-31 | 0000797468 | 1920 | . 355 OKE | Oneok | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Tulsa, Oklahoma | 2010-03-15 | 0001039684 | 1906 | . 372 PSX | Phillips 66 | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | Houston, Texas | 2012-05-01 | 0001534701 | 2012 (1917) | . 374 PXD | Pioneer Natural Resources | reports | Energy | Oil &amp; Gas Exploration &amp; Production | Irving, Texas | 2008-09-24 | 0001038357 | 1997 | . 411 SLB | Schlumberger Ltd. | reports | Energy | Oil &amp; Gas Equipment &amp; Services | Curaçao, Kingdom of the Netherlands | 1965-03-31 | 0000087347 | 1926 | . 466 VLO | Valero Energy | reports | Energy | Oil &amp; Gas Refining &amp; Marketing | San Antonio, Texas | | 0001035002 | 1980 | . 494 WMB | Williams Companies | reports | Energy | Oil &amp; Gas Storage &amp; Transportation | Tulsa, Oklahoma | 1975-03-31 | 0000107263 | 1908 | . We can parse these tables and search companies based on the sector . SP_500_df.loc[ SP_500_df[&#39;GICS Sector&#39;] == &#39;Information Technology&#39;] . Symbol Security SEC filings GICS Sector GICS Sub-Industry Headquarters Location Date first added CIK Founded . 4 ACN | Accenture | reports | Information Technology | IT Consulting &amp; Other Services | Dublin, Ireland | 2011-07-06 | 0001467373 | 1989 | . 6 ADBE | Adobe Inc. | reports | Information Technology | Application Software | San Jose, California | 1997-05-05 | 0000796343 | 1982 | . 7 AMD | Advanced Micro Devices | reports | Information Technology | Semiconductors | Santa Clara, California | 2017-03-20 | 0000002488 | 1969 | . 13 AKAM | Akamai Technologies | reports | Information Technology | Internet Services &amp; Infrastructure | Cambridge, Massachusetts | 2007-07-12 | 0001086222 | 1998 | . 38 APH | Amphenol Corp | reports | Information Technology | Electronic Components | Wallingford, Connecticut | 2008-09-30 | 0000820313 | 1932 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 475 V | Visa Inc. | reports | Information Technology | Data Processing &amp; Outsourced Services | San Francisco, California | 2009-12-21 | 0001403161 | 1958 | . 489 WDC | Western Digital | reports | Information Technology | Technology Hardware, Storage &amp; Peripherals | San Jose, California | 2009-07-01 | 0000106040 | 1970 | . 490 WU | Western Union Co | reports | Information Technology | Data Processing &amp; Outsourced Services | Englewood, Colorado | 2006-09-29 | 0001365135 | 1851 | . 498 XLNX | Xilinx | reports | Information Technology | Semiconductors | San Jose, California | 1999-11-08 | 0000743988 | 1984 | . 501 ZBRA | Zebra Technologies | reports | Information Technology | Electronic Equipment &amp; Instruments | Lincolnshire, Illinois | 2019-12-23 | 0000877212 | 1969 | . 75 rows × 9 columns . Get total number of Shares . We will use yfinance to extact Tickr information for each SP500 company and use pandas datareader . yf_tickr = yf.Ticker(&#39;ADBE&#39;) yf_tickr.info[&#39;sharesOutstanding&#39;] #info has good summary info for the stock . import yfinance as yf . START_DATE = &quot;2020-01-01&quot; END_DATE = &quot;2020-07-26&quot; . yf_tickr = yf.Ticker(&#39;TSLA&#39;) . _shares_outstanding = yf_tickr.info[&#39;sharesOutstanding&#39;] _previous_close = yf_tickr.info[&#39;previousClose&#39;] print(&#39;Outstanding shares: {}&#39;.format(_shares_outstanding)) print(&#39;Market Cap: {} Million USD&#39;.format((_shares_outstanding * _previous_close)/10**6)) . Outstanding shares: 959854016 Market Cap: 676447.51923584 Million USD . df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, interval=&quot;1wk&quot;, actions=False) df_tckr[&#39;Market_Cap&#39;] = df_tckr[&#39;Open&#39;] * _shares_outstanding df_tckr[&#39;YTD&#39;] = (df_tckr[&#39;Open&#39;] - df_tckr[&#39;Open&#39;][0]) * 100 / df_tckr[&#39;Open&#39;][0] . fig, ax = plt.subplots(1,1, figsize=(10,8)) df_tckr.plot(use_index=True, y=&quot;YTD&quot;,ax=ax, linewidth=4, grid=False, label=&#39;TSLA&#39;) ax.set_xlabel(&#39;Date&#39;) ax.set_ylabel(&#39;% YTD change (Weekly basis)&#39;) . Text(0, 0.5, &#39;% YTD change (Weekly basis)&#39;) . Extend this to plotting for multiple companies . import time as time def plot_market_cap(tickr_list, START_DATE, END_DATE): total_data = {} for tickr in tickr_list: total_data[tickr] = {} print(&#39;Looking at: {}&#39;.format(tickr)) yf_tickr = yf.Ticker(tickr) #try: # _shares_outstanding = yf_tickr.info[&#39;sharesOutstanding&#39;] #except(IndexError): # print(&#39;Shares outstanding not found&#39;) # _shares_outstanding = None df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, actions=False) df_tckr[&#39;YTD&#39;] = (df_tckr[&#39;Open&#39;] - df_tckr[&#39;Open&#39;][0]) * 100 / df_tckr[&#39;Open&#39;][0] total_data[tickr][&#39;hist&#39;] = df_tckr #total_data[tickr][&#39;shares&#39;] = _shares_outstanding time.sleep(np.random.randint(10)) return total_data . tickr_list = [&#39;AAPL&#39;, &#39;TSLA&#39;,&#39;FB&#39;,&#39;DAL&#39;,&#39;XOM&#39;] data = plot_market_cap(tickr_list, START_DATE, END_DATE) . Looking at: AAPL Looking at: TSLA Looking at: FB Looking at: DAL Looking at: XOM . company_name = [SP_500_df[SP_500_df[&#39;Symbol&#39;].str.contains(i)][&#39;Security&#39;].values[0] for i in tickr_list] . company_name . [&#39;Apple Inc.&#39;, &#39;Tesla, Inc.&#39;, &#39;Facebook, Inc.&#39;, &#39;Delta Air Lines Inc.&#39;, &#39;Exxon Mobil Corp.&#39;] . print(len(data[&#39;AAPL&#39;][&#39;hist&#39;][&#39;YTD&#39;])) . 142 . ytd_stat = pd.DataFrame() for tickr in tickr_list: ytd_stat[tickr] = data[tickr][&#39;hist&#39;][&#39;YTD&#39;].values ytd_stat[&#39;Date&#39;] = data[&#39;AAPL&#39;][&#39;hist&#39;].index . ytd_stat . AAPL TSLA FB DAL XOM Date . 0 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2020-01-02 | . 1 0.307187 | 3.769137 | 0.222494 | -2.426609 | 1.566061 | 2020-01-03 | . 2 -0.827016 | 3.762073 | -0.024185 | -3.292044 | 0.113891 | 2020-01-06 | . 3 1.215244 | 8.692576 | 2.935916 | -1.730873 | 0.370157 | 2020-01-07 | . 4 0.310568 | 11.590101 | 3.022975 | -2.002382 | -0.185078 | 2020-01-08 | . ... ... | ... | ... | ... | ... | ... | . 137 30.850611 | 257.835096 | 16.111244 | -53.866312 | -36.389268 | 2020-07-20 | . 138 34.589490 | 286.320361 | 19.090690 | -54.720640 | -36.477584 | 2020-07-21 | . 139 31.223803 | 276.678424 | 16.207978 | -55.181977 | -35.005460 | 2020-07-22 | . 140 31.637726 | 295.512370 | 15.903267 | -55.574967 | -36.109559 | 2020-07-23 | . 141 23.481414 | 233.571249 | 11.337365 | -54.754814 | -35.402933 | 2020-07-24 | . 142 rows × 6 columns . Final plot for returns . fig, ax = plt.subplots(1,1,figsize=(15,10)) for i, tickr in enumerate(tickr_list): ax.plot(ytd_stat[&#39;Date&#39;], ytd_stat[tickr], linewidth=5.0, label=company_name[i]) ax.set_ylabel(&#39;YTD %Return 2020&#39;) ax.set_xlabel(&#39;Date&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f9c0a4365e0&gt; .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/data-visualization/web-scrapping/2020/08/01/SP_500.html",
            "relUrl": "/python/data-analysis/data-visualization/web-scrapping/2020/08/01/SP_500.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Plotting surface in matplotlib",
            "content": "This is adapted from the following Tutorial: Link . import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; . fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) x = np.array([[1, 3], [2, 4]]) #Array format: [[a,b],[c,d]] -- a b are in row; c d are in row y = np.array([[5, 6], [7, 8]]) z = np.array([[9, 12], [10, 11]]) ax.plot_surface(x, y, z) ax.set(xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, zlabel=&#39;z&#39;) fig.tight_layout() . Meshgrid . Mesh is important to create a surface since just looking at the x, y vector by themselves what you would look at is the diagonal of the matrix formed by combination of all the possible x values with y values. For the given x and y vector, every entry in x vector can have the entire y vector as a possible point. So it is important to generate an array which captures all these possible pairing. . So using mesh-grid if x-vector is of dimensions M and y-vector is of dimensions N -- the final resulting matrix is NxM dimensions where every $n^{th}$ entry in y all the entries of x are added. Finally the ouput is given as x coordinate of that matrix and y coordinate of that matrix. . Example: . $X$ : $ begin{bmatrix} x_{1} &amp; x_{2} &amp; x_{3} end{bmatrix}$ | $Y$ : $ begin{bmatrix} y_{1} &amp; y_{2} end{bmatrix}$ | . Then resulting mesh would be: $$ X-Y-Mesh = begin{bmatrix} x_{1}y_{1} &amp; x_{2}y_{1} &amp; x_{3}y_{1} x_{1}y_{2} &amp; x_{2}y_{2} &amp; x_{3}y_{2} end{bmatrix}$$ . $$ X-path = begin{bmatrix} x_{1} &amp; x_{2} &amp; x_{3} x_{1} &amp; x_{2} &amp; x_{3} end{bmatrix}$$ . $$ X-path = begin{bmatrix} y_{1} &amp; y_{1} &amp; y_{1} y_{2} &amp; y_{2} &amp; y_{2} end{bmatrix}$$ . x_axis_range = np.arange(-2,2.1,1) y_axis_range = np.arange(-4,4.1,1) #Make the meshgrid for the x and y (x,y) = np.meshgrid(x_axis_range, y_axis_range, sparse=True) . z = x + y . fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) ax.plot_surface(x, y, z) fig.tight_layout() . Plotting this 2D function: $$ z = e^{- sqrt {x^2 + y^2}}cos(4x)cos(4y) $$ using the surface . import matplotlib.cm as cm x_axis_bound = np.linspace(-1.8,1.8,100) y_axis_bound = np.linspace(-1.8,1.8,100) (x,y) = np.meshgrid(x_axis_bound, y_axis_bound, sparse=True) def f(x,y): return np.exp(-np.sqrt( x**2 + y**2 )) * np.cos(4*x) * np.cos(4*y) Z = f(x,y) fig = plt.figure(1, clear=True) ax = fig.add_subplot(1,1,1, projection=&#39;3d&#39;) ax.plot_surface(x, y, Z, cmap=cm.hot) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) fig.tight_layout() .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-visualization/machine-learning/2020/07/18/creating_meshes.html",
            "relUrl": "/python/data-visualization/machine-learning/2020/07/18/creating_meshes.html",
            "date": " • Jul 18, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Activation functions",
            "content": "Function that activates the particular neuron or node if the value across a particular threshold. These functions add the necessary non-linearity in the ANNs. Each perceptron is, in reality (and traditionally), a logistic regression unit. When N units are stacked on top of each other we get a basic single layer perceptron which serves as the basis of Artificial neural network. . Click here for Google&#39;s ML glossary definition . There are different types of activation function and each has its benefits and faults. One of the consideration is the ease in evaluation of the gradient. It should be easy but also help in the final learning process by translating the necessary abstraction and non-linearity across the network. Some of the activation functions are primarily used to model the output of the ANN. Traditionally for a classification task, we would use a sigmoid activation function for a binary classification to predict a binary output (yes/no). In the case of multi-class classification that activation is replaced by softmax activation to estimate the &#39;probability&#39; across different classes. . Some of the traditionally used Activation functions: . Sigmoid activaton function | tanh (hyperbolic tangent) activaton function | ReLU activaton function | Leaky ReLU activaton function | Softplus function | Softmax function | . import numpy as np import matplotlib.pyplot as plt %config InlineBackend.figure_format = &#39;retina&#39; import seaborn as sns sns.set_palette(&quot;deep&quot;) . ## Baseline reference . z = np.linspace(-10,10,100) . Sigmoid activation function . def sigmoid(z): return 1/(1+np.exp(-z)) # derivative of Sigmoid Function def dsigmoid(a): return a*(1-a) # returns a derivative od sigmoid function if a=sigmoid then a&#39;=a(1-a) . plt.plot(z, sigmoid(z), label = r&#39;$sigmoid$&#39;) plt.plot(z, dsigmoid(sigmoid(z)), label = r&#39;$ frac{ partial (sigmoid)}{ partial z}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.show() . import torch x = torch.tensor(z, requires_grad=True) print(x.requires_grad) b = torch.sigmoid(x) . True . x . tensor([-10.0000, -9.7980, -9.5960, -9.3939, -9.1919, -8.9899, -8.7879, -8.5859, -8.3838, -8.1818, -7.9798, -7.7778, -7.5758, -7.3737, -7.1717, -6.9697, -6.7677, -6.5657, -6.3636, -6.1616, -5.9596, -5.7576, -5.5556, -5.3535, -5.1515, -4.9495, -4.7475, -4.5455, -4.3434, -4.1414, -3.9394, -3.7374, -3.5354, -3.3333, -3.1313, -2.9293, -2.7273, -2.5253, -2.3232, -2.1212, -1.9192, -1.7172, -1.5152, -1.3131, -1.1111, -0.9091, -0.7071, -0.5051, -0.3030, -0.1010, 0.1010, 0.3030, 0.5051, 0.7071, 0.9091, 1.1111, 1.3131, 1.5152, 1.7172, 1.9192, 2.1212, 2.3232, 2.5253, 2.7273, 2.9293, 3.1313, 3.3333, 3.5354, 3.7374, 3.9394, 4.1414, 4.3434, 4.5455, 4.7475, 4.9495, 5.1515, 5.3535, 5.5556, 5.7576, 5.9596, 6.1616, 6.3636, 6.5657, 6.7677, 6.9697, 7.1717, 7.3737, 7.5758, 7.7778, 7.9798, 8.1818, 8.3838, 8.5859, 8.7879, 8.9899, 9.1919, 9.3939, 9.5960, 9.7980, 10.0000], dtype=torch.float64, requires_grad=True) . b.backward(torch.ones(x.shape)) . x.grad . tensor([4.5396e-05, 5.5558e-05, 6.7994e-05, 8.3213e-05, 1.0184e-04, 1.2463e-04, 1.5252e-04, 1.8666e-04, 2.2843e-04, 2.7954e-04, 3.4207e-04, 4.1859e-04, 5.1221e-04, 6.2673e-04, 7.6682e-04, 9.3817e-04, 1.1477e-03, 1.4039e-03, 1.7172e-03, 2.1000e-03, 2.5677e-03, 3.1389e-03, 3.8362e-03, 4.6869e-03, 5.7241e-03, 6.9876e-03, 8.5250e-03, 1.0394e-02, 1.2661e-02, 1.5407e-02, 1.8724e-02, 2.2721e-02, 2.7521e-02, 3.3259e-02, 4.0084e-02, 4.8151e-02, 5.7615e-02, 6.8615e-02, 8.1257e-02, 9.5592e-02, 1.1158e-01, 1.2906e-01, 1.4771e-01, 1.6703e-01, 1.8633e-01, 2.0471e-01, 2.2118e-01, 2.3471e-01, 2.4435e-01, 2.4936e-01, 2.4936e-01, 2.4435e-01, 2.3471e-01, 2.2118e-01, 2.0471e-01, 1.8633e-01, 1.6703e-01, 1.4771e-01, 1.2906e-01, 1.1158e-01, 9.5592e-02, 8.1257e-02, 6.8615e-02, 5.7615e-02, 4.8151e-02, 4.0084e-02, 3.3259e-02, 2.7521e-02, 2.2721e-02, 1.8724e-02, 1.5407e-02, 1.2661e-02, 1.0394e-02, 8.5250e-03, 6.9876e-03, 5.7241e-03, 4.6869e-03, 3.8362e-03, 3.1389e-03, 2.5677e-03, 2.1000e-03, 1.7172e-03, 1.4039e-03, 1.1477e-03, 9.3817e-04, 7.6682e-04, 6.2673e-04, 5.1221e-04, 4.1859e-04, 3.4207e-04, 2.7954e-04, 2.2843e-04, 1.8666e-04, 1.5252e-04, 1.2463e-04, 1.0184e-04, 8.3213e-05, 6.7994e-05, 5.5558e-05, 4.5396e-05], dtype=torch.float64) . plt.plot(x.data.numpy(), b.data.numpy(), label = r&#39;$sigmoid$&#39;) plt.plot(x.data.numpy(), x.grad.data.numpy(), label = r&#39;$ frac{ partial (sigmoid)}{ partial z}$&#39;) plt.legend(fontsize = 12) . &lt;matplotlib.legend.Legend at 0x7f8b5f3ece48&gt; . np.unique(np.round((x.grad.data.numpy() - dsigmoid(sigmoid(z))),4)) . array([0.]) . Hyperbolic tangent activation function . def tanh(z): return np.tanh(z) # derivative of tanh def dtanh(a): return 1-np.power(a,2) . plt.plot(z, tanh(z),&#39;b&#39;, label = &#39;tanh&#39;) plt.plot(z, dtanh(tanh(z)),&#39;r&#39;, label=r&#39;$ frac{dtanh}{dz}$&#39;) plt.legend(fontsize = 12) plt.show() . ReLU (Rectified Linear Unit) Activation function . def ReLU(z): return np.maximum(0,z) # derivative of ReLu def dReLU(a): return 1*(a&gt;0) . plt.plot(z, ReLU(z),&#39;b&#39;, label =&#39;ReLU&#39;) plt.plot(z, dReLU(ReLU(z)),&#39;r&#39;, label=r&#39;$ frac{dReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.ylim(0,4) plt.xlim(-4,4) plt.show() . Leaky ReLU Activation function . def LeakyReLU(z): return np.maximum(0.01*z,z) # derivative of ReLu def dLeakyReLU(a): return 0.01*(a&gt;0) . plt.plot(z, LeakyReLU(z),&#39;b&#39;, label = &#39;LeakyReLU&#39;) plt.plot(z, dLeakyReLU(LeakyReLU(z)),&#39;r&#39;, label=r&#39;$ frac{dLeakyReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.ylim(0,4) plt.xlim(-4,4) plt.show() . Comparison of derivative for activation functions . plt.plot(z, dsigmoid(sigmoid(z)),label = r&#39;$ frac{dsigmoid}{dz}$&#39; ) plt.plot(z, dtanh(tanh(z)), label = r&#39;$ frac{dtanh}{dz}$&#39;) plt.plot(z, dReLU(ReLU(z)), label=r&#39;$ frac{dReLU}{dz}$&#39;) plt.plot(z, dLeakyReLU(LeakyReLU(z)), label=r&#39;$ frac{dLeakyReLU}{dz}$&#39;) plt.legend(fontsize = 12) plt.xlabel(&#39;z&#39;) plt.title(&#39;Derivatives of activation functions&#39;) plt.show() .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/pytorch/2020/04/22/activation_functions.html",
            "relUrl": "/python/machine-learning/pytorch/2020/04/22/activation_functions.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Implement Support Vector Machines in scikit-learn",
            "content": "This tutorial is borrowed from Jake VanderPlas&#39;s example of SVM in his notebook: Python Data Science Handbook . Motivation for Support Vector Machines . We want to find a line/curve (in 2D) or a manifold (in n-D) that divides the class from each other. This is a type of Discriminative Classification | Consider a simple case of classification task, in which the two classes of points are well separated | . import os import numpy as np import matplotlib.pyplot as plt from scipy import stats random_state = 42 . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns; sns.set() %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 30, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=200, centers=2, random_state=random_state, cluster_std=1.5) . print(&#39;X is a {} array with x-y coordinates of the cluster points n&#39;.format(np.shape(X))) print(X[:3]) print(&#39; n&#39;) print(&#39;y is a {} array with a classification of the points to which cluster they belong to n&#39;.format(np.shape(y))) print(y[:3]) print(&#39; n&#39;) plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap=&#39;autumn&#39;); . X is a (200, 2) array with x-y coordinates of the cluster points [[2.24823735 1.07410715] [5.12395668 0.73232327] [4.6766441 2.72016712]] y is a (200,) array with a classification of the points to which cluster they belong to [1 1 1] . For two-dimensional data, as observed in this case, a linear discriminative classifier would attempt to draw a straight line separating the two data-sets and thereby creating a model for (binary) classification. For the 2D data like the shown above, this task could be done by hand. But there is more than one line that can divide this data in two halves! . x_fit = np.linspace(min(X[:,0]),max(X[:,0])) fig, ax = plt.subplots(1,1, figsize=(5,5)) ax.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) # Random point in the 2D plain ax.plot([-2],[4],&#39;x&#39;,color=&#39;blue&#39;, markeredgewidth=2, markersize=10) # Plot various 2D planes separating the two &#39;blobs&#39; for m,b in [(3.5,5),(2,5),(0.9,5)]: plt.plot(x_fit, x_fit*m+b, &#39;-k&#39;) plt.xlim(min(X[:,0]),max(X[:,0])) plt.ylim(min(X[:,1]),max(X[:,1])) . (-0.9549620153430207, 13.094539878082749) . What&#39;s a better methodology to determine the cutting plane? Something like k-nearest neighbor clustering wherein you find the plane with best separation from the two clusters based on some distance metric. However, k-nearest neighbors is based on non-parametric method -- needing to be estimated everytime a new data point is introduced. . What if I want something which is learned and then used as a function every other time a new datum is to be classified. This is where support vector machines (SVM) are useful. . Support Vector Machines: . Rather than simply drawing a zero-width line between classes, we can draw round each line a margin of some width, up to the nearest point. . x_fit = np.linspace(min(X[:,0]),max(X[:,0])) plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) plt.plot([-2],[4],&#39;x&#39;,color=&#39;blue&#39;, markeredgewidth=2, markersize=10) for m, b, d in [(3.5,5,0.33),(2,5,0.55),(0.9,5,0.8)]: y_fit = x_fit*m + b plt.plot(x_fit, y_fit, &#39;-k&#39;) plt.fill_between(x_fit, y_fit-d, y_fit+d, edgecolor=&#39;none&#39;, color=&#39;#AAAAAA&#39;, alpha=0.4) plt.xlim(min(X[:,0]),max(X[:,0])) plt.ylim(min(X[:,1]),max(X[:,1])) . (-0.9549620153430207, 13.094539878082749) . In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator. . Fitting a support vector machine model . Using Scikit-learn&#39;s SVM module to train a classifier on the above data. We will use a linear-kernel and set C parameters to a very large value. . from sklearn.svm import SVC model = SVC(kernel=&#39;linear&#39;,C=1E10) model.fit(X,y) . SVC(C=10000000000.0, kernel=&#39;linear&#39;) . To better appreciate the SVM classification logic, we use a convenience function to visualize the decision boundary as made by the SVM module. Code is adopted from Jake&#39;s tutorial. . def plot_svc_decision_function(model, ax=None, plot_support=True, list_vectors=False): &quot;&quot;&quot;Plot the decision function for a 2D SVC&quot;&quot;&quot; if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() # create grid to evaluate model x = np.linspace(xlim[0], xlim[1], 30) y = np.linspace(ylim[0], ylim[1], 30) Y, X = np.meshgrid(y, x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) # plot decision boundary and margins ax.contour(X, Y, P, colors=&#39;k&#39;, levels=[-1, 0, 1], alpha=0.5, linestyles=[&#39;--&#39;, &#39;-&#39;, &#39;--&#39;]) # plot support vectors if plot_support: ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;,linestyle=&#39;--&#39;); if list_vectors: print(model.support_vectors_) ax.set_xlim(xlim) ax.set_ylim(ylim) . plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(model) . This is the dividing line that maximizes the margin between the two sets of points. . Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure. . These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. . In Scikit-Learn, the identity of these points are stored in the supportvectors attribute of the classifier. . model.support_vectors_ . array([[-0.40500616, 6.91150953], [ 2.65952903, 4.72035783], [ 2.07017704, 4.00397825]]) . A key to this classifier&#39;s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! . Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin. . We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset: . def plot_svm(N=10, ax=None): X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.60) X = X[:N] y = y[:N] model = SVC(kernel=&#39;linear&#39;, C=1E10) model.fit(X, y) ax = ax or plt.gca() ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plot_svc_decision_function(model, ax) fig, ax = plt.subplots(1, 2, figsize=(16, 6)) fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1) for axi, N in zip(ax, [60, 120]): plot_svm(N, axi) axi.set_title(&#39;N = {0}&#39;.format(N)) . In spite of increasing the training points, once the margins and the corresponding support vectors are identified the model does not change. This is one of the strengths of this algorithm . from ipywidgets import interact, fixed interact(plot_svm, N=[10, 50, 100, 150, 200], ax=fixed(None)); . Beyond linear kernels: Kernel SVM . Kernels are helpful in projecting data into higher dimensional feature space. This can be useful in simplest case to fit non-linear data using linear regression models. Similarly in the case of SVM: Projecting the data into higher dimensions through either polynomial or gaussian kernels we can fit non-linear relationships to a linear classifier . Let&#39;s look at a data-set which is not linearly separated: . from sklearn.datasets import make_circles X, y = make_circles(200, factor=0.1, noise=0.1) clf = SVC(kernel=&#39;linear&#39;).fit(X,y) plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(clf, plot_support=False) . There is not straight forward way to separate this data however we can project the data into higher dimensions based on its properties in the current dimensional space and get more information about its spread. One way of doing so is computing a radial basis function centered at the middle lump . r = np.exp(-(np.sum((X)**2,axis=1))) from mpl_toolkits import mplot3d def plot_3D(elev=30, azim=30, X=X, y=y): ax = plt.subplot(projection=&#39;3d&#39;) ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap=&#39;autumn&#39;) ax.view_init(elev=elev, azim=azim) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_zlabel(&#39;r&#39;) interact(plot_3D, elev=[-90, -45, -30, 30, 45, 60, 90], azip=(-180, 180), X=fixed(X), y=fixed(y)); . Projecting the data in an additonal dimensions we can see can having a plane at r=0.7 could give us good separation. . Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. . In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use. . One strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a kernel transformation, as it is based on a similarity relationship (or kernel) between each pair of points. . A potential problem with this strategy—projecting N points into N dimensions—is that it might become very computationally intensive as N grows large. However, because of a neat little procedure known as the kernel trick, a fit on kernel-transformed data can be done implicitly—that is, without ever building the full N-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful. . In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the kernel model hyperparameter: . clf = SVC(kernel=&#39;rbf&#39;, C=1E6) clf.fit(X, y) . SVC(C=1000000.0) . plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(clf) plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=300, lw=1, facecolors=&#39;none&#39;); . Softer margins . X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.2) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;); . To handle this case, the SVM implementation has a bit of a fudge-factor which &quot;softens&quot; the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as C. . For very large C, the margin is hard, and points cannot lie in it. For smaller C, the margin is softer, and can grow to encompass some points. . The plot shown below gives a visual picture of how a changing C parameter affects the final fit, via the softening of the margin: . X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8) fig, ax = plt.subplots(1, 3, figsize=(16, 6)) for axi, C in zip(ax, [1E10, 10.0, 0.1]): model = SVC(kernel=&#39;linear&#39;, C=C).fit(X, y) axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;) plot_svc_decision_function(model, axi) axi.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;,linestyle=&#39;--&#39;) axi.set_title(&#39;C = {0:.1f}&#39;.format(C), size=14) . Example: Facial Recognition . As an example of support vector machines in action, let&#39;s take a look at the facial recognition problem. . We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. . A fetcher for the dataset is built into Scikit-Learn: . from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people(min_faces_per_person=60) print(faces.target_names) print(faces.images.shape) . [&#39;Ariel Sharon&#39; &#39;Colin Powell&#39; &#39;Donald Rumsfeld&#39; &#39;George W Bush&#39; &#39;Gerhard Schroeder&#39; &#39;Hugo Chavez&#39; &#39;Junichiro Koizumi&#39; &#39;Tony Blair&#39;] (1348, 62, 47) . fig, ax = plt.subplots(3,5,figsize=(20,20)) for i,axi in enumerate(ax.flat): axi.imshow(faces.images[i],cmap=&#39;bone&#39;) axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]]) . Each image contains [62×47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features. . Here we will use a principal component analysis to extract 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline: . from sklearn.svm import SVC from sklearn.decomposition import PCA from sklearn.pipeline import make_pipeline pca = PCA(n_components=150, whiten=True, svd_solver=&#39;randomized&#39;, random_state=42) svc = SVC(kernel=&#39;rbf&#39;, class_weight=&#39;balanced&#39;) model = make_pipeline(pca,svc) . from sklearn.model_selection import train_test_split, GridSearchCV Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42) . Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model: . param_grid = {&#39;svc__C&#39;: [0.001, 0.1, 1, 5, 10, 50], &#39;svc__gamma&#39;: [0.0001, 0.0005, 0.001, 0.005]} grid = GridSearchCV(model, param_grid, cv=5) %time grid.fit(Xtrain, ytrain) print(grid.best_params_) . CPU times: user 2min 26s, sys: 35.1 s, total: 3min 1s Wall time: 33 s {&#39;svc__C&#39;: 10, &#39;svc__gamma&#39;: 0.001} . model = grid.best_estimator_ yfit = model.predict(Xtest) . fig, ax = plt.subplots(4, 6,figsize=(20,20)) for i, axi in enumerate(ax.flat): axi.imshow(Xtest[i].reshape(62, 47), cmap=&#39;bone&#39;) axi.set(xticks=[], yticks=[]) axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color=&#39;black&#39; if yfit[i] == ytest[i] else &#39;red&#39;) fig.suptitle(&#39;Predicted Names; Incorrect Labels in Red&#39;); . from sklearn.metrics import classification_report print(classification_report(ytest, yfit, target_names=faces.target_names)) . precision recall f1-score support Ariel Sharon 0.65 0.73 0.69 15 Colin Powell 0.80 0.87 0.83 68 Donald Rumsfeld 0.74 0.84 0.79 31 George W Bush 0.92 0.83 0.88 126 Gerhard Schroeder 0.86 0.83 0.84 23 Hugo Chavez 0.93 0.70 0.80 20 Junichiro Koizumi 0.92 1.00 0.96 12 Tony Blair 0.85 0.95 0.90 42 accuracy 0.85 337 macro avg 0.83 0.84 0.84 337 weighted avg 0.86 0.85 0.85 337 . from sklearn.metrics import confusion_matrix mat = confusion_matrix(ytest, yfit) fig, ax = plt.subplots(1,1, figsize=(10,10)) sns.heatmap(mat.T, square=True, annot=True, fmt=&#39;d&#39;, cbar=False, xticklabels=faces.target_names, yticklabels=faces.target_names, ax=ax) plt.xlabel(&#39;true label&#39;) plt.ylabel(&#39;predicted label&#39;); .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2020/02/19/SVM_example.html",
            "relUrl": "/python/machine-learning/2020/02/19/SVM_example.html",
            "date": " • Feb 19, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "http://pgg1610.github.io/blog_fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Vectorisation in python using numpy",
            "content": "import numpy as np import time a = np.random.randint(10E6,size=(50,1000)) print(np.shape(a)) w = np.random.randint(100,size=(50,1)) print(np.shape(w)) . (50, 1000) (50, 1) . t_start = time.time() z = np.dot(w.T,a).T t_stop = time.time() print(&#39;Time take: {} ms&#39;.format(1000*(t_stop-t_start))) #Non vectorized version z_for = [] t_start = time.time() for j in range(np.shape(a)[1]): _count = 0.0 for i in range(np.shape(a)[0]): _count+=w[i,0]*a[i,j] z_for.append(_count) t_stop = time.time() print(&#39;Time take for for-loop: {} ms&#39;.format(1000*(t_stop-t_start))) #Check the output print(&#39;Check sum: {}&#39;.format(np.sum(np.asarray(z_for).reshape(np.shape(z))-z))) . Time take: 0.3979206085205078 ms Time take for for-loop: 33.74624252319336 ms Check sum: 0.0 . #If I want to have expoenential of different values in the array a = np.random.randint(10,size=(10,2)) #With for loops: import math exp_a = np.zeros(np.shape(a)) for j in range(np.shape(a)[1]): for i in range(np.shape(a)[0]): exp_a[i,j] = math.exp(a[i,j]) . exp_a_numpy = np.exp(a) #Vector already setup -- element-wise exponential #Other vectorized functions: # np.log(x) # np.abs(x) # np.maximum(x,0) -- computes element-wise maximum comparing to 0 # x**2 for numpy array # 1/x for numpy array . exp_a_numpy - exp_a . array([[0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.]]) . food_cal = np.array([[56.0,0.0,4.4,68.0], [1.2, 104, 52, 8.], [1.8, 135.,99., 0.9]]) . carb = np.array([food_cal[0,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) protein = np.array([food_cal[1,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) fat = np.array([food_cal[2,i]/np.sum(food_cal[:,i])*100 for i in range(4)]) cal = np.array([carb,protein,fat]) print(cal) . [[94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] . cal = food_cal.sum(axis=0) #AXIS = 0 is sum vertically -- along column #AXIS = 1 is sum horizontally -- along row print(cal) . [ 59. 239. 155.4 76.9] . #Here the cal is BROADCASTING from 1,4 to 4,4 percentage = 100*food_cal/cal.reshape(1,4) print(percentage) . [[94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] . #Example 1 A = np.linspace(1,5,5) print(A.shape) B = A+10. print(A, B, B.shape) # Here 10. was broadcasted into 5x1 vector . (5,) [1. 2. 3. 4. 5.] [11. 12. 13. 14. 15.] (5,) . A = np.array([[1,2,3], [4,5,6]]) print(A.shape) B = np.array([100,200,300]) print(B.shape) C = A + B print(C.shape) print(A,B) print(C) # Here B was broadcasted from (3,) to 2x3! . (2, 3) (3,) (2, 3) [[1 2 3] [4 5 6]] [100 200 300] [[101 202 303] [104 205 306]] . General principle . (m,n) matrix with (+, -, *, /) with (1,n) or (m,1) lead of copying it to (m,n) before conducting computing. . Good practices and tips . import numpy as np a = np.random.randn(5) print(a) . [ 0.68281763 -1.3579685 0.99577659 0.31269709 0.595569 ] . print(a.shape) . (5,) . Here a is a array of rank 1. It is neither a row or a column vector. So this has some non-intuitive effects . print(a.T) . [ 0.68281763 -1.3579685 0.99577659 0.31269709 0.595569 ] . print(np.dot(a,a.T)) . 3.7543713020122427 . So it is recommended for consistency to NOT use data-structures have rank 1 like the one above but instead instantiate the array as the fixed array of known size . ALWAYS COMMIT TO MAKING DEFINED ROW AND COLUMN VECTORS . a1 = np.random.randn(5,1) print(a1) print(a1.shape) . [[-0.7474656 ] [-0.75790159] [ 0.30984002] [ 0.18874051] [-0.80470167]] (5, 1) . print(a1.T) . [[-0.7474656 -0.75790159 0.30984002 0.18874051 -0.80470167]] . Here there are two Square Brackets compared to the previous transport of a suggesting in the case of a1 it is well-defined 1x5 row vector . print(np.dot(a1,a1.T)) #Outer product . [[ 0.55870482 0.56650536 -0.23159476 -0.14107704 0.60148682] [ 0.56650536 0.57441482 -0.23482825 -0.14304673 0.60988468] [-0.23159476 -0.23482825 0.09600084 0.05847936 -0.24932878] [-0.14107704 -0.14304673 0.05847936 0.03562298 -0.1518798 ] [ 0.60148682 0.60988468 -0.24932878 -0.1518798 0.64754478]] . assert(a1.shape==(5,1)) #Assertion statement to check the known size a = a.reshape((5,1)) print(a.shape) . (5, 1) . A = np.random.randn(4,3) . print(A) . [[ 0.22469294 0.78832742 -1.13148285] [-0.04070683 -0.74061401 -1.59838506] [ 0.12821164 0.72892812 0.4912876 ] [ 0.09323584 1.66090848 1.87905216]] . np.sum(A,axis=1,keepdims=True).shape . (4, 1) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/machine-learning/2019/10/11/Vectorisation_and_TF_example.html",
            "relUrl": "/python/machine-learning/2019/10/11/Vectorisation_and_TF_example.html",
            "date": " • Oct 11, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "Lambda, Filter, and Map functions in Python",
            "content": "Lambda . Lambda is an important function/operator to create anonymous in-line functions in pyhton. . Basic syntax: . lamda arguments: expression . def func(x,y): return(x+y) func(2,3) . 5 . add=lambda x,y:x+y add(2,3) . 5 . lambda functions can be used in place of a iterator when sorting -- this allows for selecting the column or the varible according to which sorting has to be done . import numpy as np np.random.seed(42) a1=np.random.choice(10,5,replace=False) file_list=[] for i in a1: file_list.append(&#39;{0}-{1}&#39;.format(&#39;Filename&#39;,i)) . print(file_list) . [&#39;Filename-8&#39;, &#39;Filename-1&#39;, &#39;Filename-5&#39;, &#39;Filename-0&#39;, &#39;Filename-7&#39;] . file_list=sorted(file_list, key=lambda x:x.split(&#39;-&#39;)[-1]) . file_list . [&#39;Filename-0&#39;, &#39;Filename-1&#39;, &#39;Filename-5&#39;, &#39;Filename-7&#39;, &#39;Filename-8&#39;] . Map . When you want to have multiple outputs for the functions but do not want to write a for all explicitly you can use map function for pseeding things up . def square(x): return(x**2) print(a1) ans=[square(i) for i in a1] print(ans) . [8 1 5 0 7] [64, 1, 25, 0, 49] . map(square,a1) . &lt;map at 0x7f85058e5eb8&gt; . list(map(square,a1)) . [64, 1, 25, 0, 49] . Combining the two: . a=np.random.choice(10,5,replace=False) b=np.random.choice(50,5,replace=False) result=map(lambda x,y:x*y,a,b) print(a) print(b) print(np.asarray(list(result))) . [0 1 8 5 3] [36 16 4 9 45] [ 0 16 32 45 135] .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/2019/10/04/Lambda_Map.html",
            "relUrl": "/python/2019/10/04/Lambda_Map.html",
            "date": " • Oct 4, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "End-to-end Machine Learning Project",
            "content": "This project is adapted from Aurelien Geron&#39;s ML book (Github link) . The aim to predict median house values in Californian districts, given a number of features from these districts. . Main steps we will go through: . Formulate the problem | Get the data | Discover and visualize data / Data exploration to gain insight | Prep data for ML algorithm testing | Select model and train it | Fine-tuning the model | Step 1: Formulate the problem . Prediction of district&#39;s median housing price given all other metrics. A supervised learning task is where we are given &#39;labelled&#39; data for training purpose. Regression model to predict a continuous variable i.e. district median housing price. Given multiple features, this is a multi-class regression type problem. Univariate regression since a single output is estimated. . Step 2: Get the data . import os import pandas as pd import numpy as np . import matplotlib.pyplot as plt from matplotlib.pyplot import cm import seaborn as sns sns.set(style=&quot;whitegrid&quot;) sns.color_palette(&quot;husl&quot;) %config InlineBackend.figure_format = &#39;retina&#39; %config InlineBackend.print_figure_kwargs={&#39;facecolor&#39; : &quot;w&quot;} plot_params = { &#39;font.size&#39; : 30, &#39;axes.titlesize&#39; : 24, &#39;axes.labelsize&#39; : 20, &#39;axes.labelweight&#39; : &#39;bold&#39;, &#39;lines.linewidth&#39; : 3, &#39;lines.markersize&#39; : 10, &#39;xtick.labelsize&#39; : 16, &#39;ytick.labelsize&#39; : 16, } plt.rcParams.update(plot_params) . housing = pd.read_csv(&#39;./data/housing.csv&#39;) . housing.sample(7) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 10769 -117.89 | 33.66 | 33.0 | 3595.0 | 785.0 | 1621.0 | 732.0 | 4.1372 | 265200.0 | &lt;1H OCEAN | . 4165 -118.20 | 34.11 | 36.0 | 1441.0 | 534.0 | 1809.0 | 500.0 | 2.1793 | 185700.0 | &lt;1H OCEAN | . 3685 -118.37 | 34.21 | 36.0 | 2080.0 | 455.0 | 1939.0 | 484.0 | 4.2875 | 176600.0 | &lt;1H OCEAN | . 8040 -118.15 | 33.84 | 37.0 | 1508.0 | 252.0 | 635.0 | 241.0 | 3.7500 | 221300.0 | &lt;1H OCEAN | . 11836 -120.98 | 39.08 | 20.0 | 4570.0 | 906.0 | 2125.0 | 815.0 | 3.0403 | 148000.0 | INLAND | . 1525 -122.07 | 37.89 | 28.0 | 3410.0 | 746.0 | 1428.0 | 670.0 | 4.3864 | 266800.0 | NEAR BAY | . 18180 -122.03 | 37.37 | 9.0 | 2966.0 | 770.0 | 1430.0 | 740.0 | 3.0047 | 256000.0 | &lt;1H OCEAN | . Each row presents one district. Each of these districts has 10 attributes (features). . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . One thing to notice in this dataset is the number of total_bedroom entries is different from other entries. This suggests there are some missing entries or null in the dataset. . housing[housing.total_bedrooms.isnull()] . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 290 -122.16 | 37.77 | 47.0 | 1256.0 | NaN | 570.0 | 218.0 | 4.3750 | 161900.0 | NEAR BAY | . 341 -122.17 | 37.75 | 38.0 | 992.0 | NaN | 732.0 | 259.0 | 1.6196 | 85100.0 | NEAR BAY | . 538 -122.28 | 37.78 | 29.0 | 5154.0 | NaN | 3741.0 | 1273.0 | 2.5762 | 173400.0 | NEAR BAY | . 563 -122.24 | 37.75 | 45.0 | 891.0 | NaN | 384.0 | 146.0 | 4.9489 | 247100.0 | NEAR BAY | . 696 -122.10 | 37.69 | 41.0 | 746.0 | NaN | 387.0 | 161.0 | 3.9063 | 178400.0 | NEAR BAY | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20267 -119.19 | 34.20 | 18.0 | 3620.0 | NaN | 3171.0 | 779.0 | 3.3409 | 220500.0 | NEAR OCEAN | . 20268 -119.18 | 34.19 | 19.0 | 2393.0 | NaN | 1938.0 | 762.0 | 1.6953 | 167400.0 | NEAR OCEAN | . 20372 -118.88 | 34.17 | 15.0 | 4260.0 | NaN | 1701.0 | 669.0 | 5.1033 | 410700.0 | &lt;1H OCEAN | . 20460 -118.75 | 34.29 | 17.0 | 5512.0 | NaN | 2734.0 | 814.0 | 6.6073 | 258100.0 | &lt;1H OCEAN | . 20484 -118.72 | 34.28 | 17.0 | 3051.0 | NaN | 1705.0 | 495.0 | 5.7376 | 218600.0 | &lt;1H OCEAN | . 207 rows × 10 columns . For categorical entries (here, ocean_proximity entries) we can find out the entries and their number using the value_counts(). We can do this for any entry we wish but makes more sense for categorical entries. . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 . housing.describe().round(2) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . count 20640.00 | 20640.00 | 20640.00 | 20640.00 | 20433.00 | 20640.00 | 20640.00 | 20640.00 | 20640.00 | . mean -119.57 | 35.63 | 28.64 | 2635.76 | 537.87 | 1425.48 | 499.54 | 3.87 | 206855.82 | . std 2.00 | 2.14 | 12.59 | 2181.62 | 421.39 | 1132.46 | 382.33 | 1.90 | 115395.62 | . min -124.35 | 32.54 | 1.00 | 2.00 | 1.00 | 3.00 | 1.00 | 0.50 | 14999.00 | . 25% -121.80 | 33.93 | 18.00 | 1447.75 | 296.00 | 787.00 | 280.00 | 2.56 | 119600.00 | . 50% -118.49 | 34.26 | 29.00 | 2127.00 | 435.00 | 1166.00 | 409.00 | 3.53 | 179700.00 | . 75% -118.01 | 37.71 | 37.00 | 3148.00 | 647.00 | 1725.00 | 605.00 | 4.74 | 264725.00 | . max -114.31 | 41.95 | 52.00 | 39320.00 | 6445.00 | 35682.00 | 6082.00 | 15.00 | 500001.00 | . Describe is powerful subroutine since that allows us to check the stat summary of numerical attributes . The 25%-50%-75% entries for each column show corresponding percentiles. It indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of observation have median income below 2.56, 50% observations have median income below 3.53, and 75% observations have median income below 4.74. 25% --&gt; 1st Quartile, 50% --&gt; Median, 75% --&gt; 3rd Quartile . housing.hist(bins=50,figsize=(20,20)); . Few observations from the Histogram plots, again remember each row is an entry for an ENTIRE district: . NOTICE:From the dataset&#39;s source disclaimer: The housing_median_value, housing_median_age, median_income_value are capped at an arbitrary value. . From latitute and longitude plots there seems to be lots of district in four particular locations (34,37 -- latitude) and (-120,-118 -- longitude). We cannot comment on the exact location but only one on these pairs giving most data. | We see a tighter distribution for total_rooms, total_bedrooms, and population but spread for house_value and an intresting spike at its end. | Small spike at the end of median_income plot suggests presence of small group of affluent families but interestingly that spike does not correlate with the spike in the house_value (More high-end property entries than more &quot;rich&quot; people in a district) | Finally, the dataset is tail-heavy that is they extend further to the right from the median which might make modeling using some ML algorithm a bit chanellenging. Few entries should be scaled such that the distribution is more normal. . Create a test-set . This ensures that this is the data on which training, testing occurs and we do not try overfitting to account for all the variance in the data. Typical 20% of data-points are randomly chosen. . def split_train_test(data,test_ratio): shuffled_indices=np.random.permutation(len(data)) test_set_size=int(len(data)*test_ratio) test_indices=shuffled_indices[:test_set_size] train_indices=shuffled_indices[test_set_size:] return(data.iloc[train_indices],data.iloc[test_indices]) . #shuffled indices risking the possibility of the algo seeing the entire dataset! np.random.seed(42) #Random seed set to 42 for no particular reason #but just cause its the answer to the Ultimate Question of Life, The Universe, and Everything train_set, test_set = split_train_test(housing, 0.2) print(len(train_set), &quot;train +&quot;, len(test_set), &quot;test&quot;) . 16512 train + 4128 test . Better way is to have an instance identifier (like id) for each entry to distingusih each entry and see if its sampled or not. . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set] . The dataset currently doesnt have inherent id. We could use the row index as id. Or we could use an ad-hoc unique identifier as an interim id. . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) #HOUSING DATA WITH ID AS COMBO OF LAT AND LONG. housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) #SCIKIT-LEARN IMPLEMENTATION #from sklearn.model_selection import train_test_split #train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) . test_set.head() . index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 59 59 | -122.29 | 37.82 | 2.0 | 158.0 | 43.0 | 94.0 | 57.0 | 2.5625 | 60000.0 | NEAR BAY | -122252.18 | . 60 60 | -122.29 | 37.83 | 52.0 | 1121.0 | 211.0 | 554.0 | 187.0 | 3.3929 | 75700.0 | NEAR BAY | -122252.17 | . 61 61 | -122.29 | 37.82 | 49.0 | 135.0 | 29.0 | 86.0 | 23.0 | 6.1183 | 75000.0 | NEAR BAY | -122252.18 | . 62 62 | -122.29 | 37.81 | 50.0 | 760.0 | 190.0 | 377.0 | 122.0 | 0.9011 | 86100.0 | NEAR BAY | -122252.19 | . 67 67 | -122.29 | 37.80 | 52.0 | 1027.0 | 244.0 | 492.0 | 147.0 | 2.6094 | 81300.0 | NEAR BAY | -122252.20 | . However the sampling we have considered here or the one used in Scikit-learn is random sampling by default. This is fine for large dataset however for smaller dataset it is utmost important that the sampled data is representative of the main population data or else we will introduce sampling bias. . This is an important bias that could be introduced without prior knowledge and could be overlooked at multiple occassion leading to wrong conclusions. To ensure the sampled dataset is representative of the population set we use stratified sampling (pseudo-random sampling). To make the stratified sampling tractable we first divide the main data into multiple &#39;stratas&#39; based on an variable which we feel is an feature that should be replicated in our test set. The sample is divided into strata and right number of instances are chosen from each strata. We must not have too many stratas and the each strate must have appropriate number of instances. . For the case of property pricing in the district, median_income variable is chosen as the variable whose distribution in the main population and the randomly chosen test sample is same. This attribute is an important attribute to predict the final median housing price. So we can think of converting the continuous variable of median_variable into categorical variable -- that is stratas. . Stratified sampling using median income . housing[&quot;median_income&quot;].hist() . &lt;AxesSubplot:&gt; . From the median_income histogram it is seen that most of the entries are clustered in the range of 2-5 (arbitrary units). We can then use this information to make stratas around these instances. Cut routine in the pandas is used for this purpose. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], #bins around 2-5 income bracket labels=[1, 2, 3, 4, 5]) housing[&quot;income_cat&quot;].value_counts() . 3 7236 2 6581 4 3639 5 2362 1 822 Name: income_cat, dtype: int64 . housing[&quot;income_cat&quot;].hist() . &lt;AxesSubplot:&gt; . Now with the population categorised into various median income groups we can use stratified sampling routine (as implemented in scikit-learn) to make our test-set. As an additional proof let&#39;s compare this to a randomly sampled test_case. We will redo the random sampling we did prviously but with the new population with categorised median_income. . from sklearn.model_selection import StratifiedShuffleSplit, train_test_split split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] #Using random sampling rand_train_set, rand_test_set = train_test_split(housing, test_size=0.2, random_state=42) . Let&#39;s check the distribution of the income_cat variable in the strat_test, random_test, and the main sample. . housing[&quot;income_cat&quot;].value_counts()/len(housing[&quot;income_cat&quot;]) . 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 . rand_test_set[&quot;income_cat&quot;].value_counts()/len(rand_test_set[&quot;income_cat&quot;]) . 3 0.358527 2 0.324370 4 0.167393 5 0.109496 1 0.040213 Name: income_cat, dtype: float64 . strat_test_set[&quot;income_cat&quot;].value_counts()/len(strat_test_set[&quot;income_cat&quot;]) . 3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64 . def income_cat_proportions(data): return data[&quot;income_cat&quot;].value_counts() / len(data) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(housing), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(rand_test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 . compare_props . Overall Stratified Random Rand. %error Strat. %error . 1 0.039826 | 0.039729 | 0.040213 | 0.973236 | -0.243309 | . 2 0.318847 | 0.318798 | 0.324370 | 1.732260 | -0.015195 | . 3 0.350581 | 0.350533 | 0.358527 | 2.266446 | -0.013820 | . 4 0.176308 | 0.176357 | 0.167393 | -5.056334 | 0.027480 | . 5 0.114438 | 0.114583 | 0.109496 | -4.318374 | 0.127011 | . Now, we can remove the income_cat column . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Preliminary visualization of the data . Let&#39;s now dive a bit deeper into the data visualization and analysis. Before we do so, copy the strat_train_set as that would be the data-set we would be playing around and make sure the main data-set is not touched. . housing=strat_train_set.copy() housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 16512 entries, 17606 to 15775 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 16512 non-null float64 1 latitude 16512 non-null float64 2 housing_median_age 16512 non-null float64 3 total_rooms 16512 non-null float64 4 total_bedrooms 16354 non-null float64 5 population 16512 non-null float64 6 households 16512 non-null float64 7 median_income 16512 non-null float64 8 median_house_value 16512 non-null float64 9 ocean_proximity 16512 non-null object dtypes: float64(9), object(1) memory usage: 1.4+ MB . Geographical visualization . Let&#39;s now plot the data entries in the housing data as per the latitude and longitude. . housing.plot(kind=&#39;scatter&#39;,x=&#39;longitude&#39;,y=&#39;latitude&#39;); . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points. . &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . This look&#39;s like California however, we cannot infer anything more out of this. Let&#39;s play around a little bit more... . Playing with the alpha value in the plotting routine allows us to see the frequency of THAT datapoint in the plot | housing.plot(kind=&#39;scatter&#39;,x=&#39;longitude&#39;,y=&#39;latitude&#39;,alpha=0.1); . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points. . &lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt; . From here, we can see the high density of listings in the Bay area and LA also around Sacramento and Fresco. . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(8,5), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, sharex=False) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82c1692f70&gt; . This is more interesting! We have now plotted the data with more information. Each data-point has two additional set of info apart of frequency of occurence. First being the color of the point is the median_house_value entry (option c). The radius of the data-point is the population of that district (option s). It can be seen that the housing prices are very much related to the location. The ones closer to the bay area are more expensive but need not be densely populated. . Looking for simple correlations . In addition to looking at the plot of housing price, we can check for simpler correaltions. Pearson&#39;s correlation matrix is something which is in-built in pandas and can be directly used. It checks for correlation between every pair of feature provided in the data-set. It estimates the covariance of the two features and estimates whether the correlation is inverse, direct, or none. . corr_matrix=housing.corr() corr_matrix.style.background_gradient(cmap=&#39;coolwarm&#39;).set_precision(2) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . longitude 1.00 | -0.92 | -0.11 | 0.05 | 0.08 | 0.11 | 0.06 | -0.02 | -0.05 | . latitude -0.92 | 1.00 | 0.01 | -0.04 | -0.07 | -0.12 | -0.08 | -0.08 | -0.14 | . housing_median_age -0.11 | 0.01 | 1.00 | -0.36 | -0.33 | -0.30 | -0.31 | -0.11 | 0.11 | . total_rooms 0.05 | -0.04 | -0.36 | 1.00 | 0.93 | 0.86 | 0.92 | 0.20 | 0.14 | . total_bedrooms 0.08 | -0.07 | -0.33 | 0.93 | 1.00 | 0.88 | 0.98 | -0.01 | 0.05 | . population 0.11 | -0.12 | -0.30 | 0.86 | 0.88 | 1.00 | 0.90 | 0.00 | -0.03 | . households 0.06 | -0.08 | -0.31 | 0.92 | 0.98 | 0.90 | 1.00 | 0.01 | 0.06 | . median_income -0.02 | -0.08 | -0.11 | 0.20 | -0.01 | 0.00 | 0.01 | 1.00 | 0.69 | . median_house_value -0.05 | -0.14 | 0.11 | 0.14 | 0.05 | -0.03 | 0.06 | 0.69 | 1.00 | . corr_matrix[&#39;median_house_value&#39;].sort_values(ascending=True) . latitude -0.142724 longitude -0.047432 population -0.026920 total_bedrooms 0.047689 households 0.064506 housing_median_age 0.114110 total_rooms 0.135097 median_income 0.687160 median_house_value 1.000000 Name: median_house_value, dtype: float64 . The correlation matrix suggests the amount of correlation between a pair of variables. When close to 1 it means a strong +ve correlation whereas, -1 means an inverse correlation. Looking at the correlation between median_house_values and other variables, we can see that there&#39;s some correlation with median_income (0.68 -- so +ve), and with the latitude (-0.14 -- so an inverse relation). . Another to check this relation is to plot scatter plots for each pair of variables in the dataset. Below we plot this for few potential/interesting variables . from pandas.plotting import scatter_matrix attributes = [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;] scatter_axes = scatter_matrix(housing[attributes], figsize=(12, 8)); #y labels [plt.setp(item.yaxis.get_label(), &#39;size&#39;, 10) for item in scatter_axes.ravel()]; #x labels [plt.setp(item.xaxis.get_label(), &#39;size&#39;, 10) for item in scatter_axes.ravel()]; . The diagonal entries show the histogram for each variable. We saw this previously for some variables. The most promising variable from this analysis seems to be the median_income. . housing.plot(kind=&quot;scatter&quot;, x=&quot;median_income&quot;, y=&quot;median_house_value&quot;, alpha=0.1); . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points. . Plotting it shows the stronger correlation with the target variable i.e. median_house_value however we can see horizontal lines (especially at USD 500k, 450k 350k) these could be due to some stratifying done in the dataset implicitly. It would be better to remove those to ensure our model does not spuriously fit for those since they are some of the quirks in the data. . Experimenting with attributes . Before we began proposing models for the data. We can play around with the variables and try different combinations of them to see if we get better trends. Let&#39;s look at a few. First, the total_room and/or total_bedroom variable could be changed to average_bedroom_per_house to better for bedrooms rather than looking for total bedroom in that district we would be looking at avg_bedroom per district and similarly we would do it for rooms. . housing[&#39;avg_bedroom&#39;]=housing[&#39;total_bedrooms&#39;]/housing[&#39;households&#39;] #Average room per house-holds in the district housing[&#39;avg_room&#39;]=housing[&#39;total_rooms&#39;]/housing[&#39;households&#39;] #Average bedrooms per rooms in a given district housing[&#39;bedroom_per_room&#39;]=housing[&#39;total_bedrooms&#39;]/housing[&#39;total_rooms&#39;] #Average population per household in a given district housing[&#39;population_per_household&#39;]=housing[&#39;population&#39;]/housing[&#39;households&#39;] #Average room per population in a given district housing[&#39;room_per_popoulation&#39;]=housing[&#39;total_rooms&#39;]/housing[&#39;population&#39;] #Average room per population in a given district housing[&#39;room_per_popoulation&#39;]=housing[&#39;total_rooms&#39;]/housing[&#39;population&#39;] . corr_matrix=housing.corr() corr_matrix.style.background_gradient(cmap=&#39;coolwarm&#39;).set_precision(2) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value avg_bedroom avg_room bedroom_per_room population_per_household room_per_popoulation . longitude 1.00 | -0.92 | -0.11 | 0.05 | 0.08 | 0.11 | 0.06 | -0.02 | -0.05 | 0.01 | -0.03 | 0.10 | -0.00 | -0.07 | . latitude -0.92 | 1.00 | 0.01 | -0.04 | -0.07 | -0.12 | -0.08 | -0.08 | -0.14 | 0.07 | 0.11 | -0.12 | 0.01 | 0.14 | . housing_median_age -0.11 | 0.01 | 1.00 | -0.36 | -0.33 | -0.30 | -0.31 | -0.11 | 0.11 | -0.08 | -0.15 | 0.14 | 0.02 | -0.10 | . total_rooms 0.05 | -0.04 | -0.36 | 1.00 | 0.93 | 0.86 | 0.92 | 0.20 | 0.14 | 0.03 | 0.13 | -0.19 | -0.02 | 0.12 | . total_bedrooms 0.08 | -0.07 | -0.33 | 0.93 | 1.00 | 0.88 | 0.98 | -0.01 | 0.05 | 0.04 | 0.00 | 0.09 | -0.03 | 0.05 | . population 0.11 | -0.12 | -0.30 | 0.86 | 0.88 | 1.00 | 0.90 | 0.00 | -0.03 | -0.07 | -0.07 | 0.04 | 0.08 | -0.14 | . households 0.06 | -0.08 | -0.31 | 0.92 | 0.98 | 0.90 | 1.00 | 0.01 | 0.06 | -0.06 | -0.08 | 0.07 | -0.03 | -0.04 | . median_income -0.02 | -0.08 | -0.11 | 0.20 | -0.01 | 0.00 | 0.01 | 1.00 | 0.69 | -0.06 | 0.31 | -0.62 | 0.02 | 0.23 | . median_house_value -0.05 | -0.14 | 0.11 | 0.14 | 0.05 | -0.03 | 0.06 | 0.69 | 1.00 | -0.04 | 0.15 | -0.26 | -0.02 | 0.20 | . avg_bedroom 0.01 | 0.07 | -0.08 | 0.03 | 0.04 | -0.07 | -0.06 | -0.06 | -0.04 | 1.00 | 0.86 | 0.05 | -0.01 | 0.84 | . avg_room -0.03 | 0.11 | -0.15 | 0.13 | 0.00 | -0.07 | -0.08 | 0.31 | 0.15 | 0.86 | 1.00 | -0.40 | -0.01 | 0.90 | . bedroom_per_room 0.10 | -0.12 | 0.14 | -0.19 | 0.09 | 0.04 | 0.07 | -0.62 | -0.26 | 0.05 | -0.40 | 1.00 | 0.00 | -0.26 | . population_per_household -0.00 | 0.01 | 0.02 | -0.02 | -0.03 | 0.08 | -0.03 | 0.02 | -0.02 | -0.01 | -0.01 | 0.00 | 1.00 | -0.05 | . room_per_popoulation -0.07 | 0.14 | -0.10 | 0.12 | 0.05 | -0.14 | -0.04 | 0.23 | 0.20 | 0.84 | 0.90 | -0.26 | -0.05 | 1.00 | . corr_matrix[&#39;median_house_value&#39;].sort_values(ascending=True) . bedroom_per_room -0.259984 latitude -0.142724 longitude -0.047432 avg_bedroom -0.043343 population -0.026920 population_per_household -0.021985 total_bedrooms 0.047689 households 0.064506 housing_median_age 0.114110 total_rooms 0.135097 avg_room 0.146285 room_per_popoulation 0.199429 median_income 0.687160 median_house_value 1.000000 Name: median_house_value, dtype: float64 . This is interesting! We see that bedroom_per_room is another potential descriptor with negative corelation moreover we get room_per_population and avg_room to be decent new descriptors for the median_house_value. Not bad for a simple math manipulation to better represent the data. This is a crucial step and where domain knowledge and intuition would come handy. . Data cleaning and prepping . Separate the predictors and the target values | Write functions to conduct various data transformations ensuring consistency and ease | Make sure the data is devoid of any NaN values since that would raise warning and errors. We have three strategies we can implement here: a. Get rid of those points (districts) entirely b. Get rid of whole attribute c. Set missing values to either zero or one of the averages (mean, median, or mode) | In our case, total bedrooms had some missing values. . # Option a: housing.dropna(subset=[&quot;total_bedrooms&quot;]) # Option b: housing.drop(&quot;total_bedrooms&quot;, axis=1) # Option c: median = housing[&quot;total_bedrooms&quot;].median() housing[&quot;total_bedrooms&quot;].fillna(median, inplace=True) # option 3 . Before we do any of this let&#39;s first separate the predictor and target_values . housing = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) # drop labels for training set housing_labels = strat_train_set[&quot;median_house_value&quot;].copy() . sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head() sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | NaN | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | NaN | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | NaN | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | NaN | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | NaN | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . sample_incomplete_rows.dropna(subset=[&quot;total_bedrooms&quot;]) # option 1 . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . sample_incomplete_rows.drop(&quot;total_bedrooms&quot;, axis=1) #option 2 . longitude latitude housing_median_age total_rooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . median = housing[&quot;total_bedrooms&quot;].median() sample_incomplete_rows[&quot;total_bedrooms&quot;].fillna(median, inplace=True) # option 3 sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 433.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 433.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 433.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 433.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 433.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . Scikit-learn imputer class . This is a handy class to take of missing values. First, we create an instance of that class with specifying what is to be replaced and what strategy is used. Before doing so, we need to make srue the entire data-set has ONLY numerical entries and Imputer will evaluate the given average for all the dataset and store it in the statistics_ instance . What the imputer will do is, . Evaluate an specified type of average. | For a given numerical data-set look for NaN or Null entires in a given attribute and replace it with the computed avearge for that attribute | try: from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+ except ImportError: from sklearn.preprocessing import Imputer as SimpleImputer imputer = SimpleImputer(strategy=&quot;median&quot;) #We define the strategy here housing_num = housing.drop(&#39;ocean_proximity&#39;, axis=1) # alternatively: housing_num = housing.select_dtypes(include=[np.number]) . imputer.fit(housing_num) . SimpleImputer(strategy=&#39;median&#39;) . imputer.statistics_ . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . housing_num.median().values . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . We can now use this as a training variables set for our model . X = imputer.transform(housing_num) . We convert the Pandas dataframe entries to a numpy array which is transformed with appropriately replacing the missing entries with median. . print(type(X), type(housing_num)) . &lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; . print(np.shape(X), housing_num.shape) &#39;&#39;&#39; If we need the data-frame back housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index) &#39;&#39;&#39; . (16512, 8) (16512, 8) . &#39; nIf we need the data-frame back nhousing_tr = pd.DataFrame(X, columns=housing_num.columns, n index=housing.index) n&#39; . Handling Text and Categorical Attribute . Now let&#39;s preprocess the categorical input feature, ocean_proximity: . housing_cat = housing[[&#39;ocean_proximity&#39;]] type(housing_cat) . pandas.core.frame.DataFrame . Converting the categorical entries to integers . try: from sklearn.preprocessing import OrdinalEncoder except ImportError: from future_encoders import OrdinalEncoder # Scikit-Learn &lt; 0.20 ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) housing_cat_encoded[:10] . array([[0.], [0.], [4.], [1.], [0.], [1.], [0.], [1.], [0.], [0.]]) . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 7276 INLAND 5263 NEAR OCEAN 2124 NEAR BAY 1847 ISLAND 2 Name: ocean_proximity, dtype: int64 . Now, housing_cat_encoded has converted the categorical entries to purely numerical values for each category . ordinal_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . Now, this is helpful with the string categories getting converted to numerical categories. However, there is still an small issue. The categorical numbering may introduce some bias in the final model. ML algorithms will assume that two nearby values are more similar than two distant values. . In the above case, &#39;&lt;1H OCEAN&#39; and &#39;INLAND&#39;have category values as 0 and 1 however &#39;&lt;1H OCEAN&#39; is more closer to &#39;NEAR OCEAN&#39; with category value 4. . To fix this issue, one solution is to create one binary attribute per category. This is called One-hot encoding as ONLY one of the attribute in the vector is 1 (hot) and others are 0 (cold). . Scikit-learn provides a OneHotEncoder encoder to convert integer categorical values to one-hot vectors. . try: from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn &lt; 0.20 from sklearn.preprocessing import OneHotEncoder except ImportError: from future_encoders import OneHotEncoder # Scikit-Learn &lt; 0.20 cat_encoder = OneHotEncoder() #1-Hot encoded vector for the housing training data-set housing_cat_1hot = cat_encoder.fit_transform(housing_cat) type(housing_cat_1hot) . scipy.sparse.csr.csr_matrix . A sparse array is declared in this case which has the position of the non-zero value and not necessarily the entire numpy matrix. This is helpful in the cases where there are too many categories and also many datapoints. For examples, if we have 4 categories and 1000 datapoints the final one-hot matrix would be 1000x4 size. Most of that would be full of 0s, with only one 1 per row for a particular category. . The housing_cat_1hot can be converted to numpy array by using the housing_cat_1hot.toarray() . Alternatively, you can set sparse=False when creating the OneHotEncoder . cat_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . Let&#39;s create a custom transformer to add extra attributes: . housing.columns . Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;ocean_proximity&#39;], dtype=&#39;object&#39;) . from sklearn.base import BaseEstimator, TransformerMixin # get the right column indices: safer than hard-coding indices rooms_ix, bedrooms_ix, population_ix, household_ix = [ list(housing.columns).index(col) for col in (&quot;total_rooms&quot;, &quot;total_bedrooms&quot;, &quot;population&quot;, &quot;households&quot;)] #Here we convert the housing.columns to list and #then find the index for the entry which matches the string in the loop class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, household_ix] population_per_household = X[:, population_ix] / X[:, household_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True) housing_extra_attribs = attr_adder.transform(housing.values) . From the above class, there&#39;s only one hyperparameter in the class. add_bedrooms_per_room is the only option and is set True by default. Let&#39;s check the new feature space by converting it to a Pandas Dataframe . housing_extra_attribs = pd.DataFrame( housing_extra_attribs, columns=list(housing.columns)+[&quot;bedrooms_per_room&quot;,&quot;rooms_per_household&quot;, &quot;population_per_household&quot;], index=housing.index) housing_extra_attribs.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity bedrooms_per_room rooms_per_household population_per_household . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | &lt;1H OCEAN | 4.625369 | 2.094395 | 0.223852 | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | &lt;1H OCEAN | 6.00885 | 2.707965 | 0.159057 | . 14650 -117.2 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | NEAR OCEAN | 4.225108 | 2.025974 | 0.241291 | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | INLAND | 5.232295 | 4.135977 | 0.200866 | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | &lt;1H OCEAN | 4.50581 | 3.047847 | 0.231341 | . Feature scaling . Feaature scaling is an important transformation needed to be applied to the data. With some exceptions, ML algorithms dont perform well when the input numerical entries have very different scales. Eg: One variable has range 0-1 but other variable has range 1-1000. This is the case in our data-base where the total number of rooms range from 6 to 39,320 while the objective variable i.e. median income only ranges from 0-15. Two common ways of scaling: . Min-max scaling (also called Normalization) Values are shifted such that they are normalized. They are rescaled in the range of 0-1. We do this by subtracting the min value and dividing by the range in the data begin{equation} x_{i} = frac{X_{i}-min}{max-min} end{equation} . | Standardization This is when the mean of the dataset is subtracted from each entry so that the data has mean as 0 and then divided by the standard deviation so that the resulting distribution has a unit variance. Unlike min-max scaling, standardization does not bound to a particular range like 0-1. However, standardisation is much less affected by outliers. If a particular values is extremely high or low that could affect the other inputs in the case of min-max scaling. However that effect is reduced in the case of standardization given it does not directly account for the range in the scaling but the mean and variance. begin{equation} x_{i} = frac{X_{i}- mu}{ sigma} end{equation} . NOTE: It is important that these scaling operations are performed on the training data only and not on the full dataset . | Transformation pipelines . Pipeline class in scikit-learn can help with sequences of transformations. Now let&#39;s build a pipeline for preprocessing the numerical attributes (note that we could use CombinedAttributesAdder() instead of FunctionTransformer(...) if we preferred): . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), #Fill in missing values using the median of each entry (&#39;attribs_adder&#39;, CombinedAttributesAdder(add_bedrooms_per_room=True)), #Add additional columns entrys (&#39;std_scaler&#39;, StandardScaler()), #Feature scaling -- using standardisation here ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . housing_num_tr . array([[-1.15604281, 0.77194962, 0.74333089, ..., -0.31205452, -0.08649871, 0.15531753], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0.21768338, -0.03353391, -0.83628902], [ 1.18684903, -1.34218285, 0.18664186, ..., -0.46531516, -0.09240499, 0.4222004 ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0.3469342 , -0.03055414, -0.52177644], [ 0.78221312, -0.85106801, 0.18664186, ..., 0.02499488, 0.06150916, -0.30340741], [-1.43579109, 0.99645926, 1.85670895, ..., -0.22852947, -0.09586294, 0.10180567]]) . This Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last step/estimator must be the trasnformers (like feature scaling). . try: from sklearn.compose import ColumnTransformer except ImportError: from future_encoders import ColumnTransformer # Scikit-Learn &lt; 0.20 . Now let&#39;s join all these components into a big pipeline that will preprocess both the numerical and the categorical features (again, we could use CombinedAttributesAdder() instead of FunctionTransformer(...) if we preferred): . num_attribs = list(housing_num) cat_attribs = [&quot;ocean_proximity&quot;] full_pipeline = ColumnTransformer([ (&quot;num&quot;, num_pipeline, num_attribs), (&quot;cat&quot;, OneHotEncoder(), cat_attribs), ]) housing_prepared = full_pipeline.fit_transform(housing) . housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . housing_prepared.shape . (16512, 16) . Step 3: Select and train a model . Finally. . Training and evaluation on the training set . Given the prioir transformations, the features are scaled, categories are converted to one-hot vectors, and the missing variables are taken account of. Let&#39;s train a Linear Regression model first . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) . LinearRegression() . trial_input = housing.iloc[:5] #First 5 entries trial_label = housing_labels.iloc[:5] #First 5 labels corresponding to the entries prep_trial_input = full_pipeline.transform(trial_input) #Transforming the entries to suit the trained input print(&#39;Predictions:&#39;,lin_reg.predict(prep_trial_input)) . Predictions: [210644.60459286 317768.80697211 210956.43331178 59218.98886849 189747.55849879] . print(&#39;Labels:&#39;,list(trial_label)) . Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] . from sklearn.metrics import mean_squared_error housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) lin_rmse . 68628.19819848923 . housing_labels.describe() . count 16512.000000 mean 206990.920724 std 115703.014830 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64 . As seen the RMSE is 68628 which is better than nothing but still it is quite high when the range of the median_house_values range from 15000 to 500000 . from sklearn.metrics import mean_absolute_error lin_mae = mean_absolute_error(housing_labels, housing_predictions) lin_mae . 49439.89599001897 . Here the model is underfitting the data since the RMSE is so high. . When this happens either the features do not provide enough information to make good predictions or the model is not powerful enough. . Let&#39;s try using a more complex model, DecisionTreeRegressor which is capable of finding non-linear relationships in the data . Decision Tree Regressor . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(random_state=42) tree_reg.fit(housing_prepared, housing_labels) . DecisionTreeRegressor(random_state=42) . housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse . 0.0 . LOL! . This model is badly overfitting the data! Let&#39;s proof this hypothesis using cross-validation schemes. We dont want to touch the test set, JUST WORK WITH THE TRAINING SET. Only touch the test set when the model we are using is good enough. We will use the part of the training set for training and other part for model validation. . Fine-tune the model . Cross-validation . Cross-validation is a method of getting reliable estimate of model performance using only the training data 10-fold cross-validation breaking training data in 10 equal parts creating 10 miniature test/train splits. Out of the 10 folds, train data on 9 and test on 10th. Do this 10 times each time holding out different fold. . Scikit-learn cross-validation feature expects a utility function (greater the better) rather than a cost function (lower the better), so to score the functions we use opposite of MSE, which is why we again compute -scores before calculating the square root . from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) tree_rmse_scores = np.sqrt(-scores) . def display_scores(scores): print(&quot;Scores:&quot;, scores) print(&quot;Mean:&quot;, scores.mean()) print(&quot;Standard deviation:&quot;, scores.std()) display_scores(tree_rmse_scores) . Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027] Mean: 71407.68766037929 Standard deviation: 2439.4345041191004 . Cross-validation not only allows us to get an estimate of the performance of the model but also the measure of how precise this estimate is (i.e. standard deviation). The Decision tree has high std-dev. This information could not be obtained with just one validation set. However, caveat is that cross-validation comes at the cost of training the model several times, so it is not always possible. . Let&#39;s compute the same score for LinearRegresson model. . lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores) . Scores: [66782.73843989 66960.118071 70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067] Mean: 69052.46136345083 Standard deviation: 2731.674001798342 . Here it can be seen that DecisionTree model performs much worse than the LinearRegression model. . Random Forrest Regressor . Random forrest works by employing many decision trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning and it is often great way to push ML algorithms even further. . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=10, random_state=42) forest_reg.fit(housing_prepared, housing_labels) . RandomForestRegressor(n_estimators=10, random_state=42) . Note:we specify n_estimators=10 to avoid a warning about the fact that the default value is going to change to 100 in Scikit-Learn 0.22. . housing_predictions = forest_reg.predict(housing_prepared) forest_mse = mean_squared_error(housing_labels, housing_predictions) forest_rmse = np.sqrt(forest_mse) forest_rmse . 21933.31414779769 . from sklearn.model_selection import cross_val_score forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores) display_scores(forest_rmse_scores) . Scores: [51646.44545909 48940.60114882 53050.86323649 54408.98730149 50922.14870785 56482.50703987 51864.52025526 49760.85037653 55434.21627933 53326.10093303] Mean: 52583.72407377466 Standard deviation: 2298.353351147122 . Random forest regressor looks better than DecisionTree and LinearRegression. The RMSE is still quite high for production quality code and could be due to overfitting. We can try other algorithms before spending time on a particular algorithm tweaking the hyperparameters. The goal is to shortlist 2-3 methods that are promising then fine-tune the model. Before we move ahead we can take a look at one more ML algorithm which is commonly employed for such supervised learning cases: Support Vector Regression . from sklearn.svm import SVR svm_reg = SVR(kernel=&quot;linear&quot;) svm_reg.fit(housing_prepared, housing_labels) housing_predictions = svm_reg.predict(housing_prepared) svm_mse = mean_squared_error(housing_labels, housing_predictions) svm_rmse = np.sqrt(svm_mse) svm_rmse . 111094.6308539982 . Step 4: Fine-tune the model . Once we settle for an algorithm we can fine-tune them efficiently using some of the in-built scikit-learn routines. . Grid Search . GridSearchCV is a faster way of tweaking the hyper-parameters for a given algorithm. It needs the hyper-parameters you want to experiment with, what values to try out, and it will evaluate possible combination of hyperparameters values using cross-validation. We can do that step for RandomForrestRegressor which we found to have lowesst RMSE of the three methods we tried . from sklearn.model_selection import GridSearchCV param_grid = [ # try 12 (3×4) combinations of hyperparameters {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, # then try 6 (2×3) combinations with bootstrap set as False {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor(random_state=42) # train across 5 folds, that&#39;s a total of (12+6)*5=90 rounds of training grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) grid_search.fit(housing_prepared, housing_labels) . GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), param_grid=[{&#39;max_features&#39;: [2, 4, 6, 8], &#39;n_estimators&#39;: [3, 10, 30]}, {&#39;bootstrap&#39;: [False], &#39;max_features&#39;: [2, 3, 4], &#39;n_estimators&#39;: [3, 10]}], return_train_score=True, scoring=&#39;neg_mean_squared_error&#39;) . The param_grid tells Scikit-learn to: . First evaluate 3x4 combinations of n_estimators and max_features with bootstrap True which is the default. | Then with bootstrap set as False we look for 2x3 combinations of n_estimators and max_featurs for the random forest | Finally, both these models are trained 5 times for the cross validation purposes in a 5-fold cross-validation fashion. | Total of (12+6)x5=90 round of training are conducted. Finally when it is done we get the best model parameters which give lowest RMSE. . grid_search.best_params_ . {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} . grid_search.best_estimator_ . RandomForestRegressor(max_features=8, n_estimators=30, random_state=42) . cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(np.sqrt(-mean_score), params) . 63669.11631261028 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55627.099719926795 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 53384.57275149205 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60965.950449450494 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52741.04704299915 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50377.40461678399 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58663.93866579625 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52006.19873526564 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 50146.51167415009 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 57869.25276169646 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 51711.127883959234 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 49682.273345071546 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 62895.06951262424 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54658.176157539405 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 59470.40652318466 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52724.9822587892 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 57490.5691951261 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 51009.495668875716 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} . pd.DataFrame(grid_search.cv_results_) . mean_fit_time std_fit_time mean_score_time std_score_time param_max_features param_n_estimators param_bootstrap params split0_test_score split1_test_score ... mean_test_score std_test_score rank_test_score split0_train_score split1_train_score split2_train_score split3_train_score split4_train_score mean_train_score std_train_score . 0 0.053401 | 0.002499 | 0.003312 | 0.000173 | 2 | 3 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} | -3.837622e+09 | -4.147108e+09 | ... | -4.053756e+09 | 1.519591e+08 | 18 | -1.064113e+09 | -1.105142e+09 | -1.116550e+09 | -1.112342e+09 | -1.129650e+09 | -1.105559e+09 | 2.220402e+07 | . 1 0.182488 | 0.002683 | 0.010123 | 0.001727 | 2 | 10 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} | -3.047771e+09 | -3.254861e+09 | ... | -3.094374e+09 | 1.327062e+08 | 11 | -5.927175e+08 | -5.870952e+08 | -5.776964e+08 | -5.716332e+08 | -5.802501e+08 | -5.818785e+08 | 7.345821e+06 | . 2 0.503301 | 0.010947 | 0.024532 | 0.000476 | 2 | 30 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} | -2.689185e+09 | -3.021086e+09 | ... | -2.849913e+09 | 1.626875e+08 | 9 | -4.381089e+08 | -4.391272e+08 | -4.371702e+08 | -4.376955e+08 | -4.452654e+08 | -4.394734e+08 | 2.966320e+06 | . 3 0.081569 | 0.001284 | 0.003247 | 0.000154 | 4 | 3 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} | -3.730181e+09 | -3.786886e+09 | ... | -3.716847e+09 | 1.631510e+08 | 16 | -9.865163e+08 | -1.012565e+09 | -9.169425e+08 | -1.037400e+09 | -9.707739e+08 | -9.848396e+08 | 4.084607e+07 | . 4 0.269809 | 0.005145 | 0.008730 | 0.000133 | 4 | 10 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} | -2.666283e+09 | -2.784511e+09 | ... | -2.781618e+09 | 1.268607e+08 | 8 | -5.097115e+08 | -5.162820e+08 | -4.962893e+08 | -5.436192e+08 | -5.160297e+08 | -5.163863e+08 | 1.542862e+07 | . 5 0.800619 | 0.007793 | 0.024065 | 0.000543 | 4 | 30 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} | -2.387153e+09 | -2.588448e+09 | ... | -2.537883e+09 | 1.214614e+08 | 3 | -3.838835e+08 | -3.880268e+08 | -3.790867e+08 | -4.040957e+08 | -3.845520e+08 | -3.879289e+08 | 8.571233e+06 | . 6 0.113473 | 0.004140 | 0.003330 | 0.000178 | 6 | 3 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} | -3.119657e+09 | -3.586319e+09 | ... | -3.441458e+09 | 1.893056e+08 | 14 | -9.245343e+08 | -8.886939e+08 | -9.353135e+08 | -9.009801e+08 | -8.624664e+08 | -9.023976e+08 | 2.591445e+07 | . 7 0.378872 | 0.014358 | 0.009405 | 0.001258 | 6 | 10 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} | -2.549663e+09 | -2.782039e+09 | ... | -2.704645e+09 | 1.471569e+08 | 6 | -4.980344e+08 | -5.045869e+08 | -4.994664e+08 | -4.990325e+08 | -5.055542e+08 | -5.013349e+08 | 3.100456e+06 | . 8 1.222785 | 0.030201 | 0.027815 | 0.001110 | 6 | 30 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} | -2.370010e+09 | -2.583638e+09 | ... | -2.514673e+09 | 1.285080e+08 | 2 | -3.838538e+08 | -3.804711e+08 | -3.805218e+08 | -3.856095e+08 | -3.901917e+08 | -3.841296e+08 | 3.617057e+06 | . 9 0.146751 | 0.002996 | 0.003320 | 0.000174 | 8 | 3 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} | -3.353504e+09 | -3.348552e+09 | ... | -3.348850e+09 | 1.241939e+08 | 13 | -9.228123e+08 | -8.553031e+08 | -8.603321e+08 | -8.881964e+08 | -9.151287e+08 | -8.883545e+08 | 2.750227e+07 | . 10 0.510138 | 0.016372 | 0.009396 | 0.000904 | 8 | 10 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} | -2.571970e+09 | -2.718994e+09 | ... | -2.674041e+09 | 1.392777e+08 | 5 | -4.932416e+08 | -4.815238e+08 | -4.730979e+08 | -5.155367e+08 | -4.985555e+08 | -4.923911e+08 | 1.459294e+07 | . 11 1.518905 | 0.056501 | 0.025125 | 0.001086 | 8 | 30 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} | -2.357390e+09 | -2.546640e+09 | ... | -2.468328e+09 | 1.091662e+08 | 1 | -3.841658e+08 | -3.744500e+08 | -3.773239e+08 | -3.882250e+08 | -3.810005e+08 | -3.810330e+08 | 4.871017e+06 | . 12 0.077979 | 0.001837 | 0.003983 | 0.000390 | 2 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -3.785816e+09 | -4.166012e+09 | ... | -3.955790e+09 | 1.900964e+08 | 17 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 13 0.259085 | 0.003267 | 0.010411 | 0.000311 | 2 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -2.810721e+09 | -3.107789e+09 | ... | -2.987516e+09 | 1.539234e+08 | 10 | -6.056477e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -2.967449e+00 | -6.056027e-01 | 1.181156e+00 | . 14 0.104454 | 0.004167 | 0.003841 | 0.000315 | 3 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -3.618324e+09 | -3.441527e+09 | ... | -3.536729e+09 | 7.795057e+07 | 15 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -6.072840e+01 | -1.214568e+01 | 2.429136e+01 | . 15 0.345978 | 0.004147 | 0.010372 | 0.000444 | 3 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -2.757999e+09 | -2.851737e+09 | ... | -2.779924e+09 | 6.286720e+07 | 7 | -2.089484e+01 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -5.465556e+00 | -5.272080e+00 | 8.093117e+00 | . 16 0.135484 | 0.015839 | 0.004140 | 0.000585 | 4 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -3.134040e+09 | -3.559375e+09 | ... | -3.305166e+09 | 1.879165e+08 | 12 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 17 0.453605 | 0.013427 | 0.010725 | 0.000358 | 4 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -2.525578e+09 | -2.710011e+09 | ... | -2.601969e+09 | 1.088048e+08 | 4 | -0.000000e+00 | -1.514119e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -3.028238e-03 | 6.056477e-03 | . 18 rows × 23 columns . Randomised search . Grid search approach is fine when we are exploring relatively few combinations. But when hyperparameter space is large it is often preferrable to use RandomizedSearchCV instead. Here instead of doing all the possible combinationes of hyperparameters, it evaluates a given number of random combinations by selecting a random value for each hyper parameter at every iteration. . Ensemble search . Combine models that perform best. The group or &#39;ensemble&#39; will often perform better than the best individual model just like RandomForest peforms better than Decision Trees especially if we have individual models make different types of errors. . Step 5: Analyze the Best Models and their Errors . RandomForestRegressor can indicate the relative importance of each attribute for making the accurate predictions. . feature_importances = grid_search.best_estimator_.feature_importances_ feature_importances . array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]) . extra_attribs = [&quot;rooms_per_hhold&quot;, &quot;pop_per_hhold&quot;, &quot;bedrooms_per_room&quot;] cat_encoder = full_pipeline.named_transformers_[&quot;cat&quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True) . [(0.36615898061813423, &#39;median_income&#39;), (0.16478099356159054, &#39;INLAND&#39;), (0.10879295677551575, &#39;pop_per_hhold&#39;), (0.07334423551601243, &#39;longitude&#39;), (0.06290907048262032, &#39;latitude&#39;), (0.056419179181954014, &#39;rooms_per_hhold&#39;), (0.053351077347675815, &#39;bedrooms_per_room&#39;), (0.04114379847872964, &#39;housing_median_age&#39;), (0.014874280890402769, &#39;population&#39;), (0.014672685420543239, &#39;total_rooms&#39;), (0.014257599323407808, &#39;households&#39;), (0.014106483453584104, &#39;total_bedrooms&#39;), (0.010311488326303788, &#39;&lt;1H OCEAN&#39;), (0.0028564746373201584, &#39;NEAR OCEAN&#39;), (0.0019604155994780706, &#39;NEAR BAY&#39;), (6.0280386727366e-05, &#39;ISLAND&#39;)] . Step 6: Evaluate the model on the Test Set . final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(&quot;median_house_value&quot;, axis=1) y_test = strat_test_set[&quot;median_house_value&quot;].copy() X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) . fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(y_test, final_predictions, s=100) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.plot(lims, lims, &#39;k--&#39;, linewidth=2.0, alpha=0.75, zorder=0) ax.set_aspect(&#39;equal&#39;) ax.set_xlim(lims) ax.set_ylim(lims) ax.set_xlabel(&#39;ML Prediction&#39;) ax.set_ylabel(&#39;Actual Value&#39;) . Text(0, 0.5, &#39;Actual Value&#39;) . final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) print(final_rmse) . 47730.22690385927 . We can compute a 95% confidence interval for the test RMSE: . from scipy import stats confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 mean = squared_errors.mean() m = len(squared_errors) np.sqrt(stats.t.interval(confidence, m - 1, loc=np.mean(squared_errors), scale=stats.sem(squared_errors))) . array([45685.10470776, 49691.25001878]) . Alternatively, we could use a z-scores rather than t-scores: . zscore = stats.norm.ppf((1 + confidence) / 2) zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m) np.sqrt(mean - zmargin), np.sqrt(mean + zmargin) . (45685.717918136594, 49690.68623889426) .",
            "url": "http://pgg1610.github.io/blog_fastpages/python/data-analysis/machine-learning/2019/08/02/end2endML_housing.html",
            "relUrl": "/python/data-analysis/machine-learning/2019/08/02/end2endML_housing.html",
            "date": " • Aug 2, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hello, I am Pushkar Ghanekar. . This blog is a compilation of tinkerings, learnings, and helpful tips I have accumulated in the area of data science and machine learning, with some flavor of chemical science sprinkled sporadically. The posts are meant to be a reference for my future self and whoever wishes to start dabbling in the data science and ML space. . This is my first-ever venture in the world of blogging and is definitely a work in progress. Would love hear what you think! . You can check out my personal website here. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "http://pgg1610.github.io/blog_fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://pgg1610.github.io/blog_fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}