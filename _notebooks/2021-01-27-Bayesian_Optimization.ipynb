{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimisation implementation\n",
    "> Walkthrough on implementing a modular bayesian optimization routine\n",
    "\n",
    "- toc:true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Pushkar G. Ghanekar\n",
    "- categories: [python, machine-learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f$ (objective function) is cheap to evaluate we can sample various points and built a potential surface however, if the $f$ is expensive -- like in case of first-principles electronic structure calculations, it is important to minimize the number of $f$ calls and number of samples drawn from this evaluation. In that case, if an exact functional form for f is not available (that is, f behaves as a “black box”), what can we do? \n",
    "\n",
    "Bayesian optimization proceeds by maintaining a probabilistic belief about f and designing a so called **_acquisition function_** to determine where to evaluate the function next. Bayesian optimization is particularly well-suited to global optimization problems where f is an expensive black-box function. The idea is the find \"global\" minimum with least number of steps. Incorporating prior beliefs about the underlying process and update the prior with samples draw from the model to better estimate the posterior. Model used for approximating the objective function is called the **_surrogate model_**. \n",
    "\n",
    "* Notebook explaining the idea behind bayesian optimization alongside a small example showing its use. This notebook was adapted from Martin Krasser's [blogpost](http://krasserm.github.io/2018/03/21/bayesian-optimization/)\n",
    "\n",
    "* Good introductory write-up on Bayesian optimization [here](https://distill.pub/2020/bayesian-optimization/)\n",
    "\n",
    "* Nice lecture explaining the working of Gaussian Processes [here](https://www.youtube.com/watch?v=92-98SYOdlY&t=4827s)\n",
    "\n",
    "### Surrogate model \n",
    "\n",
    "A popular surrogate model applied for Bayesian optimization, although strictly not required, are Gaussian Processes (GPs). These are used to define a prior beliefs about the objective function. The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement. Herein, we could substitute this for a ANNs or other surrogate models. \n",
    "\n",
    "### Acquisition functions \n",
    "Used to propose sampling points in the search space. Trade-off between exploitation vs exploration. Exploitation == sampling where objective function value is high; exploration == where uncertainty is high. Both correspond to high `acquisition function` value. The goal is the maximize the acquisition value to determine next sampling point. \n",
    "\n",
    "Popular acquisition functions: \n",
    "* Maximum probability of improvement    \n",
    "\n",
    ">PI involves sampling for points which improve on the current best objective function value. The point in the sample space with the highest probability of improvement, based on the value predicted by the surrogate function, is chosen as the next point for evaluating through the expensive method. However in this searching scheme we look only at the probability improvement and not the extent of improvement. This might lead it to get stuck in a local minima. Instead we can turn to the __Expected value__ of improvement wherein we consider the extent of improvement as well.\n",
    "\n",
    "* Expected improvement: \n",
    "\n",
    "* Upper confidence bound (UCB)\n",
    "\n",
    "### Optimization strategy \n",
    "\n",
    "Following strategy is followed when optimizing using Bayesian optimization: \n",
    "<ul> \n",
    "    <li>Find the next sampling point $\\mathbf{x}_t$ by optimizing the acquisition function over a surrogate model (in this case a GP) fit over a distribution $\\mathcal{D}_{1:t-1}$</li>\n",
    "    <li>Evaluate $f$ at $f(x_{t})$ i.e. sample $f(x_{t})$ from the $f$</li>\n",
    "    <li>Add the new point to the prior of the GP now $\\mathcal{D}_{1:t} = ( \\mathcal{D}_{1:t-1}, (x_{t},f(x_{t})) )$</li>\n",
    "</ul>\n",
    "\n",
    "### Expected improvement\n",
    "\n",
    "Expected improvement is defined as: \n",
    "$$\\mathrm{EI}(\\mathbf{x}) = \\max((f(\\mathbf{x}) - f(\\mathbf{x}^+), 0))\\tag{1}$$\n",
    "\n",
    "where $f(\\mathbf{x}^+)$ is the value of the best sample so far and $\\mathbf{x}^+$ is the location of that sample i.e. $\\mathbf{x}^+ = \\mathrm{argmax}_{\\mathbf{x}_i \\in \\mathbf{x}_{1:t}} f(\\mathbf{x}_i)$. The expected improvement can be evaluated analytically under the GP model<sup>[3]</sup>\n",
    "\n",
    "Expected improvement can be evaluated analytically for a GP model. \n",
    "\n",
    "\n",
    "### Implementation with Numpy and Scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Plot matplotlib plots with white background: \n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process import kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bayesian_optimization import plotting_utils, acquisition, objectives, opti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_params = {\n",
    "'font.size' : 22,\n",
    "'axes.titlesize' : 24,\n",
    "'axes.labelsize' : 20,\n",
    "'axes.labelweight' : 'bold',\n",
    "'xtick.labelsize' : 16,\n",
    "'ytick.labelsize' : 16,\n",
    "}\n",
    " \n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Import the acquisition functions implemented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI = acquisition.ExpectedImprovement(delta = 0.01)\n",
    "LCB = acquisition.LowerConfidenceBound(sigma = 1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A One-Dimensional Example\n",
    "\n",
    "### ### Egg-carton objective function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = objectives.egg_carton\n",
    "(low, high) = (0.0, 10.0)\n",
    "domain = np.array([[low], [high]])\n",
    "x_pts = np.linspace(low, high, 1000).reshape(-1, 1)\n",
    "y_pts = objective(x_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample_points = 10\n",
    "noise_ = 0.1\n",
    "generator = np.random.default_rng(42)\n",
    "x_sample = generator.uniform(low, high, size = (num_sample_points, 1))\n",
    "y_sample = objective(x_sample, noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,3))\n",
    "ax.plot(x_pts, y_pts, 'k-', linewidth=2.0, label='Ground truth')\n",
    "ax.plot(x_sample, y_sample, 'ro', label='Noisy sampled points')\n",
    "\n",
    "ax.set_xlabel(r\"$x$\", fontsize = 20)\n",
    "ax.set_ylabel(r\"$f(x)$\", fontsize = 20)\n",
    "ax.set_title(\"Initial setup\", fontsize = 20)\n",
    "ax.legend(fontsize = 15)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a GPR model (surrogate function) to the sampled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = kernels.ConstantKernel()\n",
    "matern = kernels.Matern(nu = 2.5)\n",
    "rbf = kernels.RBF()\n",
    "\n",
    "gpr_model = GPR(kernel = constant*rbf, alpha = 1e-3, n_restarts_optimizer = 20, normalize_y = False, random_state = 42)\n",
    "\n",
    "gpr_model.fit(x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_pred, stddev_pred) = gpr_model.predict(x_pts, return_std = True)\n",
    "gpr_model.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Initial Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Few Iterations and Assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkwargs = {\"num_sample\": 50,\n",
    "           \"num_improve\": 5,\n",
    "           \"generator\": generator}\n",
    "\n",
    "res_ec, _ = opti.bayesian_optimization(objective, gpr_model, LCB, domain, max_iter=10, noise=noise_, prop_kwargs = pkwargs)\n",
    "gpr_model.fit(res_ec[\"X\"], res_ec[\"y\"]) # Incorporate final point into plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB, num_sample_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Few More Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ec, _ = opti.bayesian_optimization(objective, gpr_model, LCB, domain, noise=noise_, prop_kwargs = pkwargs)\n",
    "gpr_model.fit(res_ec[\"X\"], res_ec[\"y\"]) # Incorporate final point into plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig_ec, ax_ec) = plotting_utils.illustrate_1d_gpr(objective, gpr_model, x_pts, EI, LCB, num_sample_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total the noisy estimation of the ground-truth is conducted on 30 additional points. It is evident from the plot that most of those points are near the x = (4,6) since that is the minimum value region for the function.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
